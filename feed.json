{
    "version": "https://jsonfeed.org/version/1",
    "title": "金娇娇",
    "subtitle": "去留无意，漫随天外云卷云舒",
    "icon": "https://jinjiaojiao.top/images/favicon.ico",
    "description": "金同学的个人博客",
    "home_page_url": "https://jinjiaojiao.top",
    "items": [
        {
            "id": "https://jinjiaojiao.top/2024/02/24/%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90/",
            "url": "https://jinjiaojiao.top/2024/02/24/%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90/",
            "title": "竞品分析",
            "date_published": "2024-02-24T05:48:37.000Z",
            "content_html": "<p>本文章分为 4 个部分<br />\n第 1 部分 什么是竞品分析<br />\n第 2 部分 为什么要做竞品分析<br />\n第 3 部分 怎么做竞品分析<br />\n第 4 部分 竞品分析案例</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65d986be9f345e8d03158c39.jpg\" alt=\"\" /></p>\n<h1 id=\"什么是竞品分析\"><a class=\"anchor\" href=\"#什么是竞品分析\">#</a> 什么是竞品分析</h1>\n<h2 id=\"竞品\"><a class=\"anchor\" href=\"#竞品\">#</a> 竞品</h2>\n<p>竞争对手的产品</p>\n<p>竞品分析</p>\n<p>对现有或者潜在竞争对手的产品进行分析</p>\n<p><strong>Part1 什么是竞品分析？</strong> 先来说说什么是竞品？ 竞品就是竞争产品，竞争对手的产品。 比如：微信的竞品是 QQ，钉钉，陌陌；抖音的竞品是快手，微视…. 竞品分析：顾名思义，竞品分析就是是对现有或者潜在的竞争对手的产品进行比较分析。</p>\n<h1 id=\"为什么要做竞品分析\"><a class=\"anchor\" href=\"#为什么要做竞品分析\">#</a> 为什么要做竞品分析</h1>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-254265c6dcad097f1973246efec88e74_1440w.webp\" alt=\"img\" /></p>\n<p><strong>Part2 为什么要做竞品分析？</strong></p>\n<p><strong>当我们对一个行业进行研究过后，觉得有前景，准备去做或者已经在做</strong>，这个时候我们就需要进行竞品分析了。 准备开始做我们需要关注这些</p>\n<ul>\n<li>看看市面上有没有和自己产品类似的竞品</li>\n<li>快速了解这块领域当前处于什么阶段？蓝海？红海？</li>\n<li>快速了解竞品的产品模式，用户群体，运营模式，是否可以借鉴与改进</li>\n</ul>\n<p>如果已经在做了，那么我们做竞品分析更关注这些</p>\n<ul>\n<li>了解自己与竞品的<strong>区别</strong>、差距，自身优势与劣势，产品的市场份额及竞争力，有助于及时调整产品 / 运营策略</li>\n<li>了解客户评价，反馈，进一步了解用户需求，引导产品及服务改良或创新</li>\n<li>关注竞品的最新动向，有助于发现新的增长点，有助于及时调整产品 / 运营策略</li>\n</ul>\n<h1 id=\"怎么做竞品分析\"><a class=\"anchor\" href=\"#怎么做竞品分析\">#</a> 怎么做竞品分析</h1>\n<h2 id=\"part3-怎么做竞品分析\"><a class=\"anchor\" href=\"#part3-怎么做竞品分析\">#</a> <strong>Part3 怎么做竞品分析</strong></h2>\n<ol>\n<li><strong>明确分析目的</strong></li>\n<li><strong>寻找竞品，选择竞品</strong></li>\n<li><strong>定义分析维度</strong></li>\n<li><strong>竞品数据收集</strong></li>\n<li><strong>分析总结</strong></li>\n</ol>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-dc8c196594ef536c384479fc3b541256_1440w.webp\" alt=\"img\" /></p>\n<p><strong>竞品分析步骤 1 – 明确分析目的</strong><br />\n我们做竞品分析之前首先的确定分析目的，分析的目的不同，所关注的点和所用的方法也不同。<br />\n先看看我们的产品是正准备做还是说已经做了出来？<br />\n如果是正准备做，那么我们的分析目的可能是<br />\n 1. 了解竞品的用户群体中与我们的目标群体重合的部分，从竞品数据中了解用户的需求<br />\n 2. 了解竞品的产品模式，用户群体，运营模式等，看看我们的产品直接借（chao）鉴（xi）改造成功的竞品还是创新另做<br />\n如果我们的产品已经做出来了，那么我们的分析目的可能是<br />\n 1. 了解我们与竞品的区别，差距，自身优势与劣势，产品的市场份额及竞争力。<br />\n2. 关注竞品的最新动向，有助于及时调整产品 / 运营策略，有助于发现新的增长点。</p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-85c33e6657acb096dd7fe4ba7c4842f3_1440w.webp\" alt=\"img\" /></p>\n<p><strong>竞品分析步骤 2 – 寻找竞品，选择竞品</strong><br />\n当我们明确了分析目的之后就是寻找竞品与选择竞品。</p>\n<p>** 如何寻找竞品？** 我们可以通过以下渠道来寻找</p>\n<ul>\n<li>App Store 分类，应用商城分类</li>\n<li>行业分析报告</li>\n<li>行业媒体</li>\n<li>网上的竞品分析</li>\n<li>各类搜索引擎关键词搜索</li>\n</ul>\n<p><strong>找出竞品之后，又如何选择竞品？</strong><br />\n竞品有很多，我们需要对这些竞品进行分级</p>\n<ul>\n<li><strong>直接竞品</strong> – 产品功能相似，目标人群重合，解决方案与技术无明显差异。\n<ul>\n<li><strong>重要竞品</strong> – 做得比我们好的竞品</li>\n<li><strong>核心竞品</strong> – 做得比我们好，并且非常有竞争力的竞品</li>\n<li><strong>一般竞品</strong> – 不如我的竞品</li>\n</ul>\n</li>\n<li><strong>间接竞品</strong> – 产品功能形态和解决方案不同，但用户群高度重合，解决的用户需求相同，目前不构成直接的利益竞争，但未来有很大的竞争风险。</li>\n</ul>\n<p>在确定分级关系之后，在竞品分析过程中，你就需要决定在核心竞品，重要竞品，一般竞品中，对哪些模块需要花多大力度去研究和分析。一般而言需要分析的是主要是重点竞品和核心竞品。</p>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-4b4a695ee68fecf7ebff8ca86ae7d7b8_1440w.webp\" alt=\"img\" /></p>\n<p><strong>竞品分析步骤 3 – 定义分析维度</strong><br />\n选择完竞品之后，我们需要定义分析维度。如果此时你不知道怎么定义，不妨先体验一款竞品，然后采取它的信息维度作为分析维度，然后再在后面的竞品数据收集时增加或者删减维度。下面给一个比较通用的分析维度模板</p>\n<ul>\n<li>竞品基本信息\n<ul>\n<li>竞品所属公司，公司愿景</li>\n<li>竞品的核心成员</li>\n<li>竞品发展历程</li>\n<li>竞品的运营数据</li>\n</ul>\n</li>\n<li>竞品的产品定位</li>\n<li>竞品的目标用户</li>\n<li>息架构</li>\n<li>主要功能</li>\n<li>视觉与交互</li>\n<li>运营推广策略</li>\n</ul>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-3b4eff1603870d700208c9a39131a262_1440w.webp\" alt=\"img\" /></p>\n<p><strong>竞品分析步骤 4 – 数据收集</strong><br />\n分析竞品，少不了的一个环节就是数据收集<br />\n收集竞品数据，我们可以从以下途径来进行收集，收集下来之后放在一起便于比较分析，收集的数据可以用截图，excel 表格，文档等形式记录。</p>\n<ol>\n<li><strong>官方渠道</strong>：公司官网，财报，公司数据披露，招聘信息</li>\n<li><strong>行业研究</strong>：行业分析报告，艾瑞咨询，易观智库，企鹅智酷，</li>\n<li><strong>数据平台</strong>：CNNIC，DCCI 互联网数据中心 ，百度指数，七麦数据，App Annie 等</li>\n<li><strong>媒体咨询</strong>：行业媒体、论坛</li>\n<li><strong>相关人员</strong>：调查核心用户，公司员工</li>\n<li><strong>亲身体验</strong>：使用对方的产品，咨询客服，技术问答</li>\n</ol>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-b68c461468bfd5eb1c9558c17cf143ec_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-942b624adcd9cff31b477890779b44cc_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic2.zhimg.com/80/v2-02ac2df1ecfb0d3eb85b79c883cdf7f9_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-1ed80b0597e6312e7f70c197b3c0d163_1440w.webp\" alt=\"img\" /></p>\n<p><strong>选择竞品分析方法</strong></p>\n<ul>\n<li><strong>SWOT 分析法</strong> – 针对索要分析的竞品，从 “优势、劣势、机会、威胁” 四个维度进行比较和梳理</li>\n<li><strong>YES/NO 法</strong> – 适用于功能层面，将功能点全盘列出，具有该功能点的产品标记为 yes，没有的标记 no，通过对比可以清晰的了解竞品间功能点的差异</li>\n<li><strong>评分法</strong> – 这个方法在用户研究中常会用到，通常适用于定量研究的问券调查中，即给出 1-5 分的区间，根据产品中的某一个方面或某点进行打分</li>\n<li><strong>分析描述法</strong> – 指将不同的产品特性以比较的形式描述出来</li>\n</ul>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-2b6f4b5d6e473dda7b97b68c9395c4d2_1440w.webp\" alt=\"img\" /></p>\n<p><strong>竞品分析步骤 5 – 分析总结，产出竞品分析报告</strong><br />\n针对分析目的，运用上述收集的数据，选择合适的分析方法，分析总结，形成竞品分析报告，分析报告通常为文档形式，或者 PPT 的形式。</p>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-da0d9e0cc874073515d0d601a5b7c7de_1440w.webp\" alt=\"img\" /></p>\n<h2 id=\"part4-竞品分析案例-现金贷竞品分析-前面说了那么多方法论现在附一个案例现金贷竞品分析以便更容易理解消化\"><a class=\"anchor\" href=\"#part4-竞品分析案例-现金贷竞品分析-前面说了那么多方法论现在附一个案例现金贷竞品分析以便更容易理解消化\">#</a> <strong>Part4 竞品分析案例 – 现金贷竞品分析</strong> 前面说了那么多方法论，现在附一个案例《现金贷竞品分析》以便更容易理解消化。</h2>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-6db3b22eb4f4bee6497910a5d8af6ef4_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic2.zhimg.com/80/v2-3ef8c3629ee2e441e0b1d5570d4b6939_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-0e69bc391832c86b2fbbc0758c5db3ab_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-849daf2b07bb891bd370ff7253f7a0ae_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-5a232ac67bed8e4a905881ddef89961b_1440w.webp\" alt=\"img\" /></p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-770b2b49d16b591742c5aa3ab9fd9847_1440w.webp\" alt=\"img\" /></p>\n<p><strong>4.4 数据收集</strong><br />\n数据的收集我采用了官网查询，公司年报，使用产品，产品帮助手册，询问产品客服，行业媒体披露，网络检索，以及其他竞品分析文章中的数据等等。<br />\n最后将收集的数据整理分析比较后以 ppt 的形势产出了竞品分析报告。</p>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-67ecbc50c1d138ef5ba85de5592da578_1440w.webp\" alt=\"img\" /></p>\n<p>现金贷竞品分析.ppt</p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-17a8eab5ec0f47005c85d73083ad185f_1440w.webp\" alt=\"img\" /></p>\n<p>竞品分析.ppt</p>\n<p><img data-src=\"https://pic3.zhimg.com/80/v2-5656ebdbf0ffcac672d76069197a2c02_1440w.webp\" alt=\"img\" /></p>\n<p>现金贷行业分析.ppt</p>\n<p>参考文档：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NzU2NTcxMw==\">如何做好竞品分析？ – 每个产品经理的必经之路 - 知乎 (zhihu.com)</span></p>\n",
            "tags": [
                "产品经理",
                "产品经理"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/24/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/",
            "url": "https://jinjiaojiao.top/2024/02/24/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/",
            "title": "需求分析",
            "date_published": "2024-02-24T05:48:14.000Z",
            "content_html": "<p>11111</p>\n",
            "tags": [
                "产品经理",
                "产品经理"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/24/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9/",
            "url": "https://jinjiaojiao.top/2024/02/24/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9/",
            "title": "产品经理工作内容",
            "date_published": "2024-02-24T05:47:59.000Z",
            "content_html": "<h1 id=\"产品经理的工作流程\"><a class=\"anchor\" href=\"#产品经理的工作流程\">#</a> 产品经理的工作流程</h1>\n<p><img data-src=\"https://pic.imgdb.cn/item/65d9848c9f345e8d030ec17d.jpg\" alt=\"image-20240224130410821\" /></p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc98069f345e8d03f67080.jpg\" alt=\"\" /></p>\n<h2 id=\"产品定义\"><a class=\"anchor\" href=\"#产品定义\">#</a> <strong>产品定义：</strong></h2>\n<p>产品在设计前，需要做两份准备工作：详细的<strong>调研</strong>和输出<strong>需求文档（prd）</strong>。</p>\n<h3 id=\"调研\"><a class=\"anchor\" href=\"#调研\">#</a> 调研：</h3>\n<p>1. 目标用户调研</p>\n<p>2. 市场调研</p>\n<p>3. 竞品分析</p>\n<h4 id=\"目标用户调研\"><a class=\"anchor\" href=\"#目标用户调研\">#</a> 目标用户调研</h4>\n<p>需求都是从用户中来，到用户中去的，很多时候用户也不知道自己想要什么，那 PM 还要听用户的吗？答案当然是要听，但有技巧地听，并进行分析。</p>\n<p>用户调研的两种情景：</p>\n<p>【产品还未设计】，处于从 0 到 1 的过程，在设计前收集资料来针对性的设计产品功能和布局。</p>\n<p>【产品已经上线】，处于待优化的状态，收集用户反馈来优化产品功能。</p>\n<h5 id=\"用户调研的目的\"><a class=\"anchor\" href=\"#用户调研的目的\">#</a> <strong>用户调研的目的</strong>：</h5>\n<p>了解用户对产品的<strong>使用过程</strong></p>\n<p>了解目标用户群的<strong>使用场景和过程</strong></p>\n<p>总结用户的<strong>问题和流程</strong></p>\n<p>提出最合理的<strong>解决方案</strong></p>\n<h4 id=\"市场调研\"><a class=\"anchor\" href=\"#市场调研\">#</a> 市场调研</h4>\n<p>市场调查方法可分为两大类：第一类按选择<strong>调查对象</strong>来划分，有<strong>全面普查</strong>、<a href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3D%E9%87%8D%E7%82%B9%E8%B0%83%E6%9F%A5%26tn%3DSE_PcZhidaonwhc_ngpagmjz%26rsv_dl%3Dgh_pc_zhidao\"><strong>重点调查</strong></a>、<strong>随机抽样</strong>、<a href=\"https://links.jianshu.com/go?to=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3D%E9%9D%9E%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7%26tn%3DSE_PcZhidaonwhc_ngpagmjz%26rsv_dl%3Dgh_pc_zhidao\"><strong>非随机抽样</strong></a>等；第二类是按调查对象所采用的具体方法来划分，有<strong>访问法</strong>、<strong>观察法</strong>、<strong>实验法</strong>。</p>\n<p>市场调研的方法：</p>\n<h5 id=\"需求文档prd\"><a class=\"anchor\" href=\"#需求文档prd\">#</a> 需求文档（Prd）：</h5>\n<p>一份完整的需求文档的结构如下图所示：</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc98f79f345e8d03f9a76d.jpg\" alt=\"\" /></p>\n<p><img data-src=\"https://upload-images.jianshu.io/upload_images/15808187-a14fe1bf5e0f0c8e?imageMogr2/auto-orient/strip%7CimageView2/2/w/710/format/webp\" alt=\"\" /></p>\n<p><strong>1.3 竞品分析</strong></p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc997c9f345e8d03fbe4f7.jpg\" alt=\"\" /></p>\n<p><img data-src=\"https://upload-images.jianshu.io/upload_images/15808187-9a81fd3f62af28fe?imageMogr2/auto-orient/strip%7CimageView2/2/w/363/format/web\" alt=\"img\" /></p>\n<p><strong>产品设计：</strong></p>\n<p>产品设计分为<strong>原型设计</strong>和<strong>视觉设计</strong>。</p>\n<p><strong>原型设计</strong>：</p>\n<p>首先要缕清业务流程，根据用户调研和市场调研分析得来的结论，来设计界面布局和整体架构。</p>\n<p>所需工具及产出：</p>\n<p>原型 —— <code>Axure RP</code> 、 <code>墨刀</code> 、 <code>figma</code> 、 <code>Mockup</code>  等（苹果电脑推荐 sketch、Mockplus）</p>\n<p>流程图 —— <code>visio</code></p>\n<p>脑图 —— <code>mindmanager</code> 、 <code>Xmind</code>  等</p>\n<p>用例图 —— <code>processOn</code> 、 <code>visio</code></p>\n<p>视觉设计（ <code>UI</code> 、交互）：</p>\n<p>视觉设计是把原型细化、美化。包括<strong> UI</strong> 和交互。色彩搭配、图片、Logo、图标等等。交互设计所用软件与原型设计所用软件大多相似，一般原型设计软件都带有交互功能。只是交互设计更注重于产品的美感和交互形式。</p>\n<p><strong>产品研发：</strong></p>\n<p>在产品研发阶段，产品经理需要跟进研发进度，沟通研发人员、测试人员、UI 设计人员等。管理需求，及时修改方案。</p>\n<p><strong>测试</strong></p>\n<p>测试是从开始研发到产品上线前一直贯穿的。而产品经理需要进行可行性测试、用户体验测试、用例测试。</p>\n<p><strong>发布</strong></p>\n<p>在产品完成后，需要进行发布准备：销售培训（培训产品的功能、使用流程、卖点）、产品定价、运营策略、推广方案、用户教育（撰写产品使用手册）。</p>\n<p><strong>改进</strong></p>\n<p>在产品上线后，产品经理还有个漫长的工作：产品改进 / 产品迭代。需要收集用户反馈、产品使用的相关数据、竞品分析…… 进行数据分析，从而进行功能改进，产品迭代。</p>\n<p><strong>6.1 通过用户反馈发现问题</strong></p>\n<p>当看到用户反馈的内容的时候，关注的点，基本是：<strong>自身产品的问题</strong>、<strong>竞品的问题、可能的机会点</strong>。</p>\n<p>那我们可以通过哪些<strong>渠道</strong>来收集用户反馈？</p>\n<p>公开渠道：App Store 等应用市场、微博、贴吧</p>\n<p>半公开渠道：微信朋友圈</p>\n<p>内部渠道：用户投诉，电话录音、客服咨询</p>\n<p><strong>6.1.1 针对用户反馈不同渠道的处理策略：</strong></p>\n<p>公开渠道：对于公开渠道，可以采取搜索 + 关键字订阅 + 使用监测工具的策略；</p>\n<p>半公开渠道：微信朋友圈可以通过 “搜一搜” 功能，搜索关键字的方法；</p>\n<p>内部渠道：这就需要整合内部用户反馈渠道，包括邮件、QQ、留言等；需要定期与一线的同事进行沟通，或者适当地当一天客服。</p>\n<p>具体的用户调研渠道有哪些，进入这些渠道中有大量的用户反馈，又该着重看什么？</p>\n<p>举个例子：</p>\n<p><strong>应用商店评论</strong></p>\n<p>主流的应用商店与常用的工具包括：</p>\n<p>IOS：App Store，安卓：360 手机助手、安卓市场、百度手机助手、小米应用商店、安智市场、豌豆荚等；常用的工具有 APPAnnie、应用雷达、ASO114、酷传等。</p>\n<p>对于应用商店，应该着重监控以下四点：</p>\n<p><strong>低分差评</strong>：重点看低分 1-3 分；</p>\n<p><strong>有效评论</strong>：重点看有实际描述的评论</p>\n<p><strong>异常行为</strong>：比如水军刷榜、恶意评价</p>\n<p><strong>竞品变化</strong>：监控竞争对手的应用变化</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc99d29f345e8d03fd475c.jpg\" alt=\"\" /></p>\n<p><strong>主流的社交平台与常用工具</strong></p>\n<p>微博、贴吧、知乎、人人网、雪球的等，工具是关键字 + 收藏夹、微博企业版、百度、Google 等。</p>\n<p>查看贴吧内容有个小技巧，在关键词后添加 site:tieba.baidu.com，相当于筛选了站点的内容。</p>\n<p>通过用户咨询、投诉发现问题</p>\n<p>内容来源：客服后台、录音、意见建议、用户反馈、邮件等。</p>\n<p><strong>6.1.2 通过用户调研发现问题</strong></p>\n<p>用户调研的流程：</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc9a159f345e8d03fe668d.jpg\" alt=\"\" /></p>\n<p>明确调研的背景和目的：</p>\n<p>背景：什么情况下发起的调研？是否必须通过用户调研来解决？</p>\n<p>目的：希望通过用户调研得到的结果是什么？</p>\n<p>调研的目的忌大而全，调研的方向越聚焦，越有价值；忌假大空，针对行业用户的调研，针对满意度的调研，价值都不大。</p>\n<p><img data-src=\"https://upload-images.jianshu.io/upload_images/15808187-ca471eea21c477ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/812/format/webp\" alt=\"img\" /></p>\n<p><strong>选择目标用户</strong></p>\n<p>基于背景和目的，先挑出大量符合行为的用户</p>\n<p>选定部分目标用户，针对性分析（用户画像）</p>\n<p>选择合适的用户（时间、地点、感兴趣程度等）</p>\n<p>邀约用户：直接说明目的，并告知可能发生的情况</p>\n<p>数量：一般不超过 5 个</p>\n<p><strong>分析用户和问题</strong></p>\n<p>这里是事先猜测目标用户可能面临的<strong>问题</strong>是什么。</p>\n<p>分析调研对象可能碰到的问题和解决方案</p>\n<p>猜测用户的需求并提出解决方案</p>\n<p>把解决方案变成可执行的 demo（纸面、原型等）</p>\n<p><strong>准备任务和访谈提纲，并演习</strong></p>\n<p>按照用户调研预设时间的<strong> 2 倍</strong>去准备问题，标注必须要回答和用户操作的关键问题，准备用户必须操作的任务，把问题串起来，并找同事预演一遍，最后总结和调整。</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65dc9a419f345e8d03fef7da.jpg\" alt=\"\" /></p>\n<p><strong>调研现场</strong></p>\n<p>根据不同情况，布置调研现场。先缓和用户情绪，不要着急一下子进入到访谈 / 调研中，其次了解背景信息与自己的猜想是否匹配（用户画像），尽可能模拟用户真实的环境，尽可能记录用户的操作过程：<strong>录屏、录音、笔记</strong>。</p>\n<p><strong>调研结束总结</strong></p>\n<p>整理单个用户的调研过程，是否要调整调研对象，汇总本轮调研用户的过程和结论。</p>\n<p>参照文档：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC8zYTI5YTJkNmNjMGI=\">产品经理（PM）的工作流程 - 简书 (jianshu.com)</span></p>\n",
            "tags": [
                "产品经理",
                "产品经理"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/",
            "url": "https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/",
            "title": "学习d2l深度学习day2——回归",
            "date_published": "2024-02-13T18:46:58.000Z",
            "content_html": "<h1 id=\"线性回归\"><a class=\"anchor\" href=\"#线性回归\">#</a> 线性回归</h1>\n<p><em><strong>回归</strong></em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p>\n<p>在机器学习领域中的大多数任务通常都与<em><strong>预测</strong></em>（prediction）有关。</p>\n<p>当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。</p>\n<p>但不是所有的<em>预测</em>都是回归问题。</p>\n<p>分类问题的目标是预测数据属于一组类别中的哪一个。</p>\n<h2 id=\"线性回归的基本元素\"><a class=\"anchor\" href=\"#线性回归的基本元素\">#</a> 线性回归的基本元素</h2>\n<p>线性回归基于几个简单的假设： 首先，假设自变量 x 和因变量 y 之间的关系是线性的， 即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</p>\n<p>为了解释<em>线性回归</em>，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em><strong>训练数据集</strong></em>（training data set） 或<em><strong>训练集</strong></em>（training set）。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>（sample）， 也可以称为<em><strong>数据点</strong></em>（data point）或<em><strong>数据样本</strong></em>（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为<em><strong>标签</strong></em>（label）或<em><strong>目标</strong></em>（target）。 预测所依据的自变量（面积和房龄）称为<em><strong>特征</strong></em>（feature）或<em><strong>协变量</strong></em>（covariate）。</p>\n<p>通常，我们使用 n 来表示数据集中的样本数。 对索引为 i 的样本，其输入表示为</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi mathvariant=\"bold\">X</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msup><mrow><mo fence=\"true\">[</mo><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo fence=\"true\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}^{(i)} = \\left[ x_1^{(i)}, x_2^{(i)} \\right]^T\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.938em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbf\">X</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.938em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.031251em;vertical-align:-0.65002em;\"></span><span class=\"minner\"><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">[</span></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.266308em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.266308em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">]</span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3812309999999999em;\"><span style=\"top:-3.6029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>， 其对应的标签是 y（i）。</p>\n<h1 id=\"softmax回归\"><a class=\"anchor\" href=\"#softmax回归\">#</a> Softmax 回归</h1>\n<p>Softmax 实质上是分类</p>\n<h2 id=\"回归与分类\"><a class=\"anchor\" href=\"#回归与分类\">#</a> 回归与分类</h2>\n<h3 id=\"回归\"><a class=\"anchor\" href=\"#回归\">#</a> 回归</h3>\n<ul>\n<li>\n<p><strong>单</strong>连续数值输出</p>\n</li>\n<li>\n<p>自然区间 R</p>\n</li>\n<li>\n<p>跟真实值的区别作为损失</p>\n</li>\n</ul>\n<h3 id=\"分类\"><a class=\"anchor\" href=\"#分类\">#</a> 分类</h3>\n<ul>\n<li>通常<strong>多个输出</strong></li>\n<li>输出 i 是预测为第 i 类的置信度</li>\n</ul>\n<h3 id=\"从回归到多类分类均方损失\"><a class=\"anchor\" href=\"#从回归到多类分类均方损失\">#</a> 从回归到多类分类 —— 均方损失</h3>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218211324212.png\" alt=\"image-20240218211324212\" /></p>\n<p>最大化 Oi 置信度</p>\n<p>需要更置信的识别正确类（大余量）</p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218211654235.png\" alt=\"image-20240218211654235\" /></p>\n<p>远远大于其他</p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218211732178.png\" alt=\"image-20240218211732178\" /></p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218211813295.png\" alt=\"image-20240218211813295\" /></p>\n<p>导数是我们 softmax 模型分配的概率如（0.1，0.2，0.7）与实际发生的情况（0，0，1）（由独热标签向量表示）之间的差异</p>\n<p><strong>交叉熵损失函数：</strong></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">y</mi><mo separator=\"true\">,</mo><mover accent=\"true\"><mi mathvariant=\"bold\">y</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>y</mi><mi>j</mi></msub><mi>log</mi><mo>⁡</mo><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>j</mi></msub><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70788em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.1122820000000004em;vertical-align:-1.4137769999999998em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6985050000000006em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.347113em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4137769999999998em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\">.</span></span></span></span></span></p>\n<p>y 是一个长度为 q 的独热编码向量</p>\n<ul>\n<li>Softmax 回归是一个多类分类模型</li>\n<li>・使用 Softmax 操作子得到每个类的预测置信度</li>\n<li>・使用交叉熵来来衡量预测和标号的区别</li>\n</ul>\n<h2 id=\"损失函数\"><a class=\"anchor\" href=\"#损失函数\">#</a> 损失函数</h2>\n<h4 id=\"l2-loss\"><a class=\"anchor\" href=\"#l2-loss\">#</a> L2 Loss</h4>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218215459096.png\" alt=\"image-20240218215459096\" /></p>\n<h4 id=\"l1-loss\"><a class=\"anchor\" href=\"#l1-loss\">#</a> L1 Loss</h4>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218215513903.png\" alt=\"image-20240218215513903\" /></p>\n<p>特性：预测值与真实值无论多远，梯度都是常数，较稳定，0 点处不可导，但是 0 点处不平滑</p>\n<p>蓝色：y'</p>\n<p>绿色：似然函数（高斯分布）</p>\n<p>黄色：梯度（穿过原点）</p>\n<h4 id=\"huber-s-robust-loss\"><a class=\"anchor\" href=\"#huber-s-robust-loss\">#</a> Huber' s Robust Loss</h4>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218215531852.png\" alt=\"image-20240218215531852\" /></p>\n<p>结合两个</p>\n<h2 id=\"信息论基础\"><a class=\"anchor\" href=\"#信息论基础\">#</a> 信息论基础</h2>\n<p><em>信息论</em>（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</p>\n<h3 id=\"熵\"><a class=\"anchor\" href=\"#熵\">#</a> 熵</h3>\n<p>信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布 P 的<em>熵</em>（entropy）。可以通过以下方程得到：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>H</mi><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><mo>−</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">H[P] = \\sum_j - P(j) \\log P(j).\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.463782em;vertical-align:-1.413777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.050005em;\"><span style=\"top:-1.8723309999999997em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.0500049999999996em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.413777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">−</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span><span class=\"mclose\">)</span><span class=\"mord\">.</span></span></span></span></span></p>\n<h3 id=\"vscode终端进入运行jupyter\"><a class=\"anchor\" href=\"#vscode终端进入运行jupyter\">#</a> vscode 终端进入运行 jupyter</h3>\n<p>1. 下载</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install jupyter</span><br></pre></td></tr></table></figure></p>\n<p>2. 进入</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"图像分类数据集\"><a class=\"anchor\" href=\"#图像分类数据集\">#</a> 图像分类数据集</h2>\n<p>MNIST 数据集 (<a href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90\">LeCun <em>et al.</em>, 1998</a>) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的 Fashion-MNIST 数据集 (<a href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189\">Xiao <em>et al.</em>, 2017</a>)。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms <span class=\"comment\">#数据操作</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.use_svg_display()</span><br></pre></td></tr></table></figure></p>\n<p>svg 清晰度高一些</p>\n<h3 id=\"读取数据集\"><a class=\"anchor\" href=\"#读取数据集\">#</a> 读取数据集</h3>\n<p>我们可以通过框架中的内置函数将 Fashion-MNIST 数据集下载并读取到内存中。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，</span></span><br><span class=\"line\"><span class=\"comment\"># 并除以255使得所有像素的数值均在0～1之间</span></span><br><span class=\"line\">trans = transforms.ToTensor()</span><br><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class=\"line\">    root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">True</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#训练数据集</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class=\"line\">    root=<span class=\"string\">&quot;../data&quot;</span>, train=<span class=\"literal\">False</span>, transform=trans, download=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure></p>\n<p>train=True 训练数据集</p>\n<p>transform=trans 拿到的是 tensor 而不是一堆图片</p>\n<p>download=True 默认在网上下载，如果已经存在 data 中就不用指定 download 了</p>\n<p>Fashion-MNIST 由 10 个类别的图像组成， 每个类别由<em>训练数据集</em>（train dataset）中的 6000 张图像 和<em>测试数据集</em>（test dataset）中的 1000 张图像组成。 因此，训练集和测试集分别包含 60000 和 10000 张图像。 测试数据集不会用于训练，只用于评估模型性能。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">len</span>(mnist_train), <span class=\"built_in\">len</span>(mnist_test)</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(60000, 10000)</span><br></pre></td></tr></table></figure></p>\n<p>每个输入图像的高度和宽度均为 28 像素。 数据集由灰度图像组成，其通道数为 1。 为了简洁起见，本书将高度ℎ像素、宽度 w 像素图像的形状记为ℎ×w 或（ℎ,w）。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mnist_train[<span class=\"number\">0</span>][<span class=\"number\">0</span>].shape</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([1, 28, 28])</span><br></pre></td></tr></table></figure></p>\n<p>Fashion-MNIST 中包含的 10 个类别，分别为 t-shirt（T 恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和 ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_fashion_mnist_labels</span>(<span class=\"params\">labels</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class=\"line\">    text_labels = [<span class=\"string\">&#x27;t-shirt&#x27;</span>, <span class=\"string\">&#x27;trouser&#x27;</span>, <span class=\"string\">&#x27;pullover&#x27;</span>, <span class=\"string\">&#x27;dress&#x27;</span>, <span class=\"string\">&#x27;coat&#x27;</span>,</span><br><span class=\"line\">                   <span class=\"string\">&#x27;sandal&#x27;</span>, <span class=\"string\">&#x27;shirt&#x27;</span>, <span class=\"string\">&#x27;sneaker&#x27;</span>, <span class=\"string\">&#x27;bag&#x27;</span>, <span class=\"string\">&#x27;ankle boot&#x27;</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [text_labels[<span class=\"built_in\">int</span>(i)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> labels]</span><br></pre></td></tr></table></figure></p>\n<p>我们现在可以创建一个函数来可视化这些样本。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_images</span>(<span class=\"params\">imgs, num_rows, num_cols, titles=<span class=\"literal\">None</span>, scale=<span class=\"number\">1.5</span></span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class=\"line\">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class=\"line\">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class=\"line\">    axes = axes.flatten()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (ax, img) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(axes, imgs)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> torch.is_tensor(img):</span><br><span class=\"line\">            <span class=\"comment\"># 图片张量</span></span><br><span class=\"line\">            ax.imshow(img.numpy())</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># PIL图片</span></span><br><span class=\"line\">            ax.imshow(img)</span><br><span class=\"line\">        ax.axes.get_xaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        ax.axes.get_yaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> titles:</span><br><span class=\"line\">            ax.set_title(titles[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> axes</span><br></pre></td></tr></table></figure></p>\n<p>以下是训练数据集中前几个样本的图像及其相应的标签。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X, y = <span class=\"built_in\">next</span>(<span class=\"built_in\">iter</span>(data.DataLoader(mnist_train, batch_size=<span class=\"number\">18</span>)))</span><br><span class=\"line\">show_images(X.reshape(<span class=\"number\">18</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>), <span class=\"number\">2</span>, <span class=\"number\">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240218232124716.png\" alt=\"image-20240218232124716\" /></p>\n<h2 id=\"读取小批量\"><a class=\"anchor\" href=\"#读取小批量\">#</a> 读取小批量</h2>\n<p>为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为 <code>batch_size</code> 。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_dataloader_workers</span>():  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                             num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure></p>\n<p>训练集要随机，测试集都可以</p>\n<p>我们看一下<strong>读取</strong>训练数据所需的时间。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">timer = d2l.Timer()</span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">    <span class=\"keyword\">continue</span></span><br><span class=\"line\"><span class=\"string\">f&#x27;<span class=\"subst\">&#123;timer.stop():<span class=\"number\">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x27;3.37 sec&#x27;</span><br></pre></td></tr></table></figure></p>\n<p>保证读取速度比训练速度快，尽量快很多</p>\n<h3 id=\"整合所有组件\"><a class=\"anchor\" href=\"#整合所有组件\">#</a> 整合所有组件</h3>\n<p>现在我们定义 <code>load_data_fashion_mnist</code>  函数，用于获取和读取 Fashion-MNIST 数据集。 这个函数返回<strong>训练集和验证集的数据迭代器</strong>。 此外，这个函数还接受一个可选参数 <code>resize</code> ，用来将图像大小调整为另一种形状。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data_fashion_mnist</span>(<span class=\"params\">batch_size, resize=<span class=\"literal\">None</span></span>):   <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class=\"line\">    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()</span><br><span class=\"line\">    <span class=\"comment\"># 将所有数字除以255，使所有像素值介于0和1之间，在最后添加一个批处理维度，</span></span><br><span class=\"line\">    <span class=\"comment\"># 并将标签转换为int32。</span></span><br><span class=\"line\">    process = <span class=\"keyword\">lambda</span> X, y: (tf.expand_dims(X, axis=<span class=\"number\">3</span>) / <span class=\"number\">255</span>,</span><br><span class=\"line\">                            tf.cast(y, dtype=<span class=\"string\">&#x27;int32&#x27;</span>))</span><br><span class=\"line\">    resize_fn = <span class=\"keyword\">lambda</span> X, y: (</span><br><span class=\"line\">        tf.image.resize_with_pad(X, resize, resize) <span class=\"keyword\">if</span> resize <span class=\"keyword\">else</span> X, y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (</span><br><span class=\"line\">        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(</span><br><span class=\"line\">            batch_size).shuffle(<span class=\"built_in\">len</span>(mnist_train[<span class=\"number\">0</span>])).<span class=\"built_in\">map</span>(resize_fn),</span><br><span class=\"line\">        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(</span><br><span class=\"line\">            batch_size).<span class=\"built_in\">map</span>(resize_fn))</span><br></pre></td></tr></table></figure></p>\n",
            "tags": [
                "深度学习",
                "跟着李沐学深度学习"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/",
            "url": "https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/",
            "title": "学习d2l深度学习day1——介绍与数据处理",
            "date_published": "2024-02-13T17:39:00.000Z",
            "content_html": "<p><strong><em>深入理解深度学习的方法</em>：</strong></p>\n<p>亲自实现，从 0 开始编写可实现运行的程序，一边看源码，一边思考</p>\n<p><strong>课程信息：</strong></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jb3Vyc2VzLmQybC5haS96aC12Mi8=\">课程安排 - 动手学深度学习课程 (d2l.ai)</span></p>\n<p>认识一下 <code>Colab</code></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81Mjc2NjMxNjM=\">Colab 使用教程（超级详细版）及 Colab Pro/Pro + 评测 - 知乎 (zhihu.com)</span></p>\n<h1 id=\"深度学习应用\"><a class=\"anchor\" href=\"#深度学习应用\">#</a> 深度学习应用</h1>\n<h3 id=\"图像分类\"><a class=\"anchor\" href=\"#图像分类\">#</a> 图像分类</h3>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5pbWFnZS1uZXQub3JnLw==\">http://www.image-net.org/</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9xei5jb20vMTAzNDk3Mi90aGUtZGF0YS10aGF0LWNoYW5nZWQtdGhlLWRpcmVjdGlvbi1vZi1haS1yZXNlYXJjaC1hbmQtcG9zc2libHktdGhlLXdvcmxk\">ImageNet: the data that spawned the current AI boom (qz.com)</span></p>\n<h3 id=\"物体检测和分割\"><a class=\"anchor\" href=\"#物体检测和分割\">#</a> 物体检测和分割</h3>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbabb49f345e8d036e0632.png\" alt=\"image-20240213204425211\" /></p>\n<p>分割指的是某个像素点属于哪个物体</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL21hdHRlcnBvcnQvTWFza19SQ05O\">matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow (github.com)</span></p>\n<h3 id=\"样式迁移\"><a class=\"anchor\" href=\"#样式迁移\">#</a> 样式迁移</h3>\n<p>风格变换</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL3poYW5naGFuZzE5ODkvTVhOZXQtR2x1b24tU3R5bGUtVHJhbnNmZXIv\">zhanghang1989/MXNet-Gluon-Style-Transfer: Neural Style and MSG-Net (github.com)</span></p>\n<h3 id=\"人脸合成\"><a class=\"anchor\" href=\"#人脸合成\">#</a> 人脸合成</h3>\n<p><code>Karras et al, ICLR 2018</code></p>\n<h3 id=\"文字生成图片\"><a class=\"anchor\" href=\"#文字生成图片\">#</a> 文字生成图片</h3>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9vcGVuYWkuY29tL2Jsb2cvZGFsbC1lLw==\">https://openai.com/blog/dall-e/</span></p>\n<h3 id=\"完整的故事\"><a class=\"anchor\" href=\"#完整的故事\">#</a> 完整的故事</h3>\n<p>领域专家（实现产品应用）</p>\n<p>数据科学家（data-&gt;model）</p>\n<p>AI 专家（提升模型精度和性能）</p>\n<h2 id=\"安装\"><a class=\"anchor\" href=\"#安装\">#</a> 安装</h2>\n<h3 id=\"步骤\"><a class=\"anchor\" href=\"#步骤\">#</a> 步骤</h3>\n<h4 id=\"登录\"><a class=\"anchor\" href=\"#登录\">#</a> 登录</h4>\n<p>仅参考，李沐老师亚马逊平台 ubuntu 系统</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh ubuntu@100.20.65.33 </span><br></pre></td></tr></table></figure></p>\n<h4 id=\"升级服务器系统\"><a class=\"anchor\" href=\"#升级服务器系统\">#</a> 升级服务器系统</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"装一些gcc这类编译开发环境\"><a class=\"anchor\" href=\"#装一些gcc这类编译开发环境\">#</a> 装一些 GCC 这类编译开发环境</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install build-essential</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"安装python\"><a class=\"anchor\" href=\"#安装python\">#</a> 安装 python</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install python3.8</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"安装miniconda\"><a class=\"anchor\" href=\"#安装miniconda\">#</a> 安装 miniconda</h4>\n<p>打开官网</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmFuYWNvbmRhLmNvbS9mcmVlL21pbmljb25kYS8=\">Miniconda — Anaconda documentation</span></p>\n<p>复制所需要的下载连接</p>\n<p>这里 Linux 安装到服务器</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"启动\"><a class=\"anchor\" href=\"#启动\">#</a> 启动</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></p>\n<p>一直回车到 yes</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash</span><br></pre></td></tr></table></figure></p>\n<p>进入 conda 环境：刚开始最基础的 base 环境</p>\n<h4 id=\"创建一个新环境\"><a class=\"anchor\" href=\"#创建一个新环境\">#</a> 创建一个新环境</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n lm</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"激活\"><a class=\"anchor\" href=\"#激活\">#</a> 激活</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate lm</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"安装记事本\"><a class=\"anchor\" href=\"#安装记事本\">#</a> 安装记事本</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install jupyter d2l torch torchvision</span><br></pre></td></tr></table></figure></p>\n<p>（国内慢提前安装个源）</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbabe29f345e8d036e7eb7.png\" alt=\"image-20240213211231841\" /></p>\n<h4 id=\"复制链接\"><a class=\"anchor\" href=\"#复制链接\">#</a> 复制链接</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://zh-v2.d2l.ai/d2l-zh.zip</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"安装zip\"><a class=\"anchor\" href=\"#安装zip\">#</a> 安装 zip</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install zip</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"查看\"><a class=\"anchor\" href=\"#查看\">#</a> 查看</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ls</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"解压文件\"><a class=\"anchor\" href=\"#解压文件\">#</a> 解压文件</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unzip d2l-zh.zip</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/d2l-ai/d2l-zh-pytorch-slides.git</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure></p>\n<p><strong>本地 prompt 操作！！</strong></p>\n<p>需要把远程机器的端口映射运行在本地</p>\n<p>m 神操作</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh -L8888:localhost:8888 ubuntu@100.20.65.33</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh -L8888:localhost:8888 thjin@yuanshen.moe</span><br></pre></td></tr></table></figure></p>\n<p>再 vscode 点击 8888 进入</p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240213213900135.png\" alt=\"image-20240213213900135\" /></p>\n<p>这个场景</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbabfa9f345e8d036eba41.png\" alt=\"image-20240213214121778\" /></p>\n<p>继续 prompt</p>\n<p><strong>别忘了启动对应的环境</strong></p>\n<h4 id=\"下载插件\"><a class=\"anchor\" href=\"#下载插件\">#</a> 下载插件</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install rise</span><br></pre></td></tr></table></figure></p>\n<p>下载之后 jupyter 就可以直接用了</p>\n<h3 id=\"补充\"><a class=\"anchor\" href=\"#补充\">#</a> 补充</h3>\n<ul>\n<li>\n<p>删除单个文件（所有系统）：</p>\n<p>Code</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1rm filename.ext</span><br></pre></td></tr></table></figure></p>\n<p>或（Windows）</p>\n<p>Code</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1del filename.ext</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>删除空文件夹（所有系统）：</p>\n<p>Code</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1rmdir foldername</span><br></pre></td></tr></table></figure></p>\n<p>或（Windows）</p>\n<p>Code</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1rmdir /S /Q foldername  # 使用/S/Q参数强制删除非空目录及其内容</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>删除非空文件夹及其中的所有内容（所有系统）：</p>\n<p>Code</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1rm -rf foldername</span><br></pre></td></tr></table></figure></p>\n<p>注意：在使用 <code>rm -rf</code>  时要格外小心，因为它会立即、不可逆地删除指定的文件夹及其包含的所有内容。</p>\n</li>\n</ul>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbac129f345e8d036ef549.png\" alt=\"image-20240213202207539\" /></p>\n<h3 id=\"数据操作\"><a class=\"anchor\" href=\"#数据操作\">#</a> 数据操作</h3>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9jb3Vyc2VzLmQybC5haS96aC12Mi8=\">课程安排 - 动手学深度学习课程 (d2l.ai)</span></p>\n<h5 id=\"导入\"><a class=\"anchor\" href=\"#导入\">#</a> 导入</h5>\n<p>首先，我们导入  <code>torch</code> 。请注意，虽然它被称为 PyTorch，但我们应该导入  <code>torch</code>  而不是  <code>pytorch</code></p>\n<p>In [1]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"张量表示由一个数值组成的数组这个数组可能有多个维度\"><a class=\"anchor\" href=\"#张量表示由一个数值组成的数组这个数组可能有多个维度\">#</a> <strong>张量表示由一个数值组成的数组，这个数组可能有多个维度</strong></h5>\n<p>In [2]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(12)</span><br><span class=\"line\">x</span><br></pre></td></tr></table></figure></p>\n<p>Out[2]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"可以通过张量的-shape-属性来访问张量的形状-和张量中元素的总数\"><a class=\"anchor\" href=\"#可以通过张量的-shape-属性来访问张量的形状-和张量中元素的总数\">#</a> <strong>可以通过张量的  <code>shape</code>  属性来访问张量的<em>形状</em> 和张量中元素的总数</strong></h5>\n<p>In [3]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.shape</span><br></pre></td></tr></table></figure></p>\n<p>Out[3]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([12])</span><br></pre></td></tr></table></figure></p>\n<p>In [4]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.numel()  #numel张量大小</span><br></pre></td></tr></table></figure></p>\n<p>Out[4]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">12</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"要改变一个张量的形状而不改变元素数量和元素值可以调用-reshape-函数\"><a class=\"anchor\" href=\"#要改变一个张量的形状而不改变元素数量和元素值可以调用-reshape-函数\">#</a> <strong>要改变一个张量的形状而不改变元素数量和元素值，可以调用  <code>reshape</code>  函数</strong></h5>\n<p>In [5]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = x.reshape(3, 4)</span><br><span class=\"line\">X</span><br></pre></td></tr></table></figure></p>\n<p>Out[5]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0,  1,  2,  3],</span><br><span class=\"line\">        [ 4,  5,  6,  7],</span><br><span class=\"line\">        [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"使用全0-全1-其他常量或者从特定分布中随机采样的数字\"><a class=\"anchor\" href=\"#使用全0-全1-其他常量或者从特定分布中随机采样的数字\">#</a> <strong>使用全 0、全 1、其他常量或者从特定分布中随机采样的数字</strong></h5>\n<p>In [6]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.zeros((2, 3, 4))</span><br></pre></td></tr></table></figure></p>\n<p>Out[6]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[[0., 0., 0., 0.],</span><br><span class=\"line\">         [0., 0., 0., 0.],</span><br><span class=\"line\">         [0., 0., 0., 0.]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[0., 0., 0., 0.],</span><br><span class=\"line\">         [0., 0., 0., 0.],</span><br><span class=\"line\">         [0., 0., 0., 0.]]])</span><br></pre></td></tr></table></figure></p>\n<p>In [7]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.ones((2, 3, 4))</span><br></pre></td></tr></table></figure></p>\n<p>Out[7]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[[1., 1., 1., 1.],</span><br><span class=\"line\">         [1., 1., 1., 1.],</span><br><span class=\"line\">         [1., 1., 1., 1.]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[1., 1., 1., 1.],</span><br><span class=\"line\">         [1., 1., 1., 1.],</span><br><span class=\"line\">         [1., 1., 1., 1.]]])</span><br></pre></td></tr></table></figure></p>\n<p>In [8]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.randn(3, 4)</span><br></pre></td></tr></table></figure></p>\n<p>Out[8]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.2104,  1.4439, -1.3455, -0.8273],</span><br><span class=\"line\">        [ 0.8009,  0.3585, -0.2690,  1.6183],</span><br><span class=\"line\">        [-0.4611,  1.5744, -0.4882, -0.5317]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"通过提供包含数值的-python-列表或嵌套列表来为所需张量中的每个元素赋予确定值\"><a class=\"anchor\" href=\"#通过提供包含数值的-python-列表或嵌套列表来为所需张量中的每个元素赋予确定值\">#</a> <strong>通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值</strong></h5>\n<p>In [9]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class=\"line\">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]).shape</span><br></pre></td></tr></table></figure></p>\n<p>Out[9]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[2, 1, 4, 3],</span><br><span class=\"line\">        [1, 2, 3, 4],</span><br><span class=\"line\">        [4, 3, 2, 1]])</span><br><span class=\"line\">torch.Size([1,3,4])</span><br></pre></td></tr></table></figure></p>\n<p><strong>常见的标准算术运算符（ <code>+</code> 、 <code>-</code> 、 <code>*</code> 、 <code>/</code>  和  <code>**</code> ）都可以被升级为按元素运算</strong></p>\n<p>In [10]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([1.0, 2, 4, 8])</span><br><span class=\"line\">y = torch.tensor([2, 2, 2, 2])</span><br><span class=\"line\">x + y, x - y, x * y, x / y, x**y</span><br></pre></td></tr></table></figure></p>\n<p>Out[10]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([ 3.,  4.,  6., 10.]),</span><br><span class=\"line\"> tensor([-1.,  0.,  2.,  6.]),</span><br><span class=\"line\"> tensor([ 2.,  4.,  8., 16.]),</span><br><span class=\"line\"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span><br><span class=\"line\"> tensor([ 1.,  4., 16., 64.]))</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"按按元素方式应用更多的计算\"><a class=\"anchor\" href=\"#按按元素方式应用更多的计算\">#</a> 按按元素方式应用更多的计算</h5>\n<p>In [11]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.exp(x) #每个数的指数</span><br></pre></td></tr></table></figure></p>\n<p>Out[11]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"我们也可以把多个张量-连结concatenate-在一起\"><a class=\"anchor\" href=\"#我们也可以把多个张量-连结concatenate-在一起\">#</a> 我们也可以把多个张量 <em>连结</em>（concatenate） 在一起</h5>\n<p>In [12]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(12, dtype=torch.float32).reshape((3, 4))</span><br><span class=\"line\">Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class=\"line\">torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)</span><br><span class=\"line\"># dim=0 按行拼接  dim=1 按列拼接</span><br></pre></td></tr></table></figure></p>\n<p>Out[12]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class=\"line\">         [ 4.,  5.,  6.,  7.],</span><br><span class=\"line\">         [ 8.,  9., 10., 11.],</span><br><span class=\"line\">         [ 2.,  1.,  4.,  3.],</span><br><span class=\"line\">         [ 1.,  2.,  3.,  4.],</span><br><span class=\"line\">         [ 4.,  3.,  2.,  1.]]),</span><br><span class=\"line\"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class=\"line\">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class=\"line\">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"数据预处理通过-逻辑运算符-构建二元张量\"><a class=\"anchor\" href=\"#数据预处理通过-逻辑运算符-构建二元张量\">#</a> 数据预处理通过 <em>逻辑运算符</em> 构建二元张量</h5>\n<p>In [13]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X == Y</span><br></pre></td></tr></table></figure></p>\n<p>Out[13]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[False,  True, False,  True],</span><br><span class=\"line\">        [False, False, False, False],</span><br><span class=\"line\">        [False, False, False, False]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"求和\"><a class=\"anchor\" href=\"#求和\">#</a> 求和</h5>\n<p>对张量中的所有元素进行求和会产生一个只有一个元素的张量</p>\n<p>In [14]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.sum()</span><br></pre></td></tr></table></figure></p>\n<p>Out[14]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor(66.)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"广播体制\"><a class=\"anchor\" href=\"#广播体制\">#</a> 广播体制</h5>\n<p>即使形状不同，我们仍然可以通过调用 <em>广播机制</em> （broadcasting mechanism） 来执行按元素操作</p>\n<p>In [15]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.arange(3).reshape((3, 1)) #3行1列</span><br><span class=\"line\">b = torch.arange(2).reshape((1, 2))</span><br><span class=\"line\">a, b</span><br></pre></td></tr></table></figure></p>\n<p>Out[15]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([[0],</span><br><span class=\"line\">         [1],</span><br><span class=\"line\">         [2]]),</span><br><span class=\"line\"> tensor([[0, 1]]))</span><br></pre></td></tr></table></figure></p>\n<p>In [16]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a + b  #广播机制 需要均有1维</span><br></pre></td></tr></table></figure></p>\n<p>Out[16]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0, 1],</span><br><span class=\"line\">        [1, 2],</span><br><span class=\"line\">        [2, 3]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"可以用-1-选择最后一个元素可以用-13-选择第二个和第三个元素\"><a class=\"anchor\" href=\"#可以用-1-选择最后一个元素可以用-13-选择第二个和第三个元素\">#</a> 可以用  <code>[-1]</code>  选择最后一个元素，可以用  <code>[1:3]</code>  选择第二个和第三个元素</h5>\n<p>In [17]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[-1] #最后一行</span><br><span class=\"line\">, X[1:3]</span><br></pre></td></tr></table></figure></p>\n<p>Out[17]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([ 8.,  9., 10., 11.]),</span><br><span class=\"line\"> tensor([[ 4.,  5.,  6.,  7.],</span><br><span class=\"line\">         [ 8.,  9., 10., 11.]]))</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"除读取外我们还可以通过指定索引来将元素写入矩阵\"><a class=\"anchor\" href=\"#除读取外我们还可以通过指定索引来将元素写入矩阵\">#</a> 除读取外，我们还可以通过指定索引来将元素写入矩阵</h5>\n<p>In [18]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[1, 2] = 9</span><br><span class=\"line\">#相当于 x[0:1,0:2]</span><br><span class=\"line\">X</span><br></pre></td></tr></table></figure></p>\n<p>Out[18]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class=\"line\">        [ 4.,  5.,  9.,  7.],</span><br><span class=\"line\">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"为多个元素赋值相同的值我们只需要索引所有元素然后为它们赋值\"><a class=\"anchor\" href=\"#为多个元素赋值相同的值我们只需要索引所有元素然后为它们赋值\">#</a> 为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</h5>\n<p>In [19]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X[0:2, :] = 12</span><br><span class=\"line\">X</span><br></pre></td></tr></table></figure></p>\n<p>Out[19]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[12., 12., 12., 12.],</span><br><span class=\"line\">        [12., 12., 12., 12.],</span><br><span class=\"line\">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"内存问题\"><a class=\"anchor\" href=\"#内存问题\">#</a> 内存问题</h5>\n<p>运行一些操作可能会导致为新结果分配内存</p>\n<p>python 引用语义</p>\n<p>id 相当于 c 中的指针</p>\n<p>In [20]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = id(Y)</span><br><span class=\"line\">Y = Y + X #产生了一个新的Y</span><br><span class=\"line\">id(Y) == before</span><br></pre></td></tr></table></figure></p>\n<p>Out[20]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">False</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"执行原地操作\"><a class=\"anchor\" href=\"#执行原地操作\">#</a> 执行原地操作</h5>\n<p>In [21]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = torch.zeros_like(Y)</span><br><span class=\"line\">print(&#x27;id(Z):&#x27;, id(Z))</span><br><span class=\"line\">Z[:] = X + Y</span><br><span class=\"line\">print(&#x27;id(Z):&#x27;, id(Z))</span><br><span class=\"line\">id(Z): 140452400950336</span><br><span class=\"line\">id(Z): 140452400950336</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"如果在后续计算中没有重复使用-x我们也可以使用-x-x-y-或-x-y-来减少操作的内存开销\"><a class=\"anchor\" href=\"#如果在后续计算中没有重复使用-x我们也可以使用-x-x-y-或-x-y-来减少操作的内存开销\">#</a> 如果在后续计算中没有重复使用  <code>X</code> ，我们也可以使用  <code>X[:] = X + Y</code>  或  <code>X += Y</code>  来减少操作的内存开销</h5>\n<p>In [22]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">before = id(X)</span><br><span class=\"line\">X += Y</span><br><span class=\"line\">id(X) == before</span><br></pre></td></tr></table></figure></p>\n<p>Out[22]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">True</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"转换为-numpy-张量\"><a class=\"anchor\" href=\"#转换为-numpy-张量\">#</a> 转换为  <code>NumPy</code>  张量</h5>\n<p>In [23]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = X.numpy()</span><br><span class=\"line\">B = torch.tensor(A)</span><br><span class=\"line\">type(A), type(B)</span><br></pre></td></tr></table></figure></p>\n<p>Out[23]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure></p>\n<p>numpy 外部库，需要 import</p>\n<h5 id=\"将大小为1的张量转换为-python-标量\"><a class=\"anchor\" href=\"#将大小为1的张量转换为-python-标量\">#</a> 将大小为 1 的张量转换为 Python 标量</h5>\n<p>In [24]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.tensor([3.5])</span><br><span class=\"line\">a, a.item(), float(a), int(a)</span><br></pre></td></tr></table></figure></p>\n<p>Out[24]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([3.5000]), 3.5, 3.5, 3)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"数据预处理\"><a class=\"anchor\" href=\"#数据预处理\">#</a> 数据预处理</h2>\n<h5 id=\"创建一个人工数据集并存储在csv逗号分隔值文件\"><a class=\"anchor\" href=\"#创建一个人工数据集并存储在csv逗号分隔值文件\">#</a> 创建一个人工数据集，并存储在 csv（逗号分隔值）文件</h5>\n<p>In [1]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)</span><br><span class=\"line\">data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)</span><br><span class=\"line\">with open(data_file, &#x27;w&#x27;) as f:</span><br><span class=\"line\">    f.write(&#x27;NumRooms,Alley,Price\\n&#x27;)</span><br><span class=\"line\">    f.write(&#x27;NA,Pave,127500\\n&#x27;)</span><br><span class=\"line\">    f.write(&#x27;2,NA,106000\\n&#x27;)</span><br><span class=\"line\">    f.write(&#x27;4,NA,178100\\n&#x27;)</span><br><span class=\"line\">    f.write(&#x27;NA,NA,140000\\n&#x27;)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"从创建的csv文件中加载原始数据集\"><a class=\"anchor\" href=\"#从创建的csv文件中加载原始数据集\">#</a> 从创建的 csv 文件中加载原始数据集</h5>\n<p>In [2]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(data_file)</span><br><span class=\"line\">print(data)</span><br><span class=\"line\">   NumRooms Alley   Price</span><br><span class=\"line\">0       NaN  Pave  127500</span><br><span class=\"line\">1       2.0   NaN  106000</span><br><span class=\"line\">2       4.0   NaN  178100</span><br><span class=\"line\">3       NaN   NaN  140000</span><br></pre></td></tr></table></figure></p>\n<p>直接输出 data 会好看一些</p>\n<p>一般 csv 与 pandas 一起</p>\n<h5 id=\"为了处理缺失的数据典型的方法包括插值和删除-这里我们将考虑插值\"><a class=\"anchor\" href=\"#为了处理缺失的数据典型的方法包括插值和删除-这里我们将考虑插值\">#</a> 为了处理缺失的数据，典型的方法包括<em>插值</em>和<em>删除</em>， 这里，我们将考虑插值</h5>\n<p>In [3]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class=\"line\">inputs = inputs.fillna(inputs.mean())</span><br><span class=\"line\">print(inputs)</span><br><span class=\"line\">   NumRooms Alley</span><br><span class=\"line\">0       3.0  Pave</span><br><span class=\"line\">1       2.0   NaN</span><br><span class=\"line\">2       4.0   NaN</span><br><span class=\"line\">3       3.0   NaN</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"对于inputs中的类别值或离散值我们将nan视为一个类别\"><a class=\"anchor\" href=\"#对于inputs中的类别值或离散值我们将nan视为一个类别\">#</a> 对于 <code>inputs</code>  中的类别值或离散值，我们将 “NaN” 视为一个类别</h5>\n<p>In [4]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class=\"line\">print(inputs)</span><br></pre></td></tr></table></figure></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   NumRooms  Alley_Pave  Alley_nan</span><br><span class=\"line\">0       3.0           1          0</span><br><span class=\"line\">1       2.0           0          1</span><br><span class=\"line\">2       4.0           0          1</span><br><span class=\"line\">3       3.0           0          1</span><br></pre></td></tr></table></figure></p>\n<p>现在 <code>inputs</code>  和 <code>outputs</code>  中的所有条目都是数值类型，它们可以转换为张量格式</p>\n<p>In [5]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\"></span><br><span class=\"line\">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class=\"line\">X, y</span><br></pre></td></tr></table></figure></p>\n<p>Out[5]:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(tensor([[3., 1., 0.],</span><br><span class=\"line\">         [2., 0., 1.],</span><br><span class=\"line\">         [4., 0., 1.],</span><br><span class=\"line\">         [3., 0., 1.]], dtype=torch.float64),</span><br><span class=\"line\"> tensor([127500, 106000, 178100, 140000]))</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240214012040421.png\" alt=\"image-20240214012040421\" /></p>\n<p>tensor 是数学上的一个概念 array 是计算机一个概念</p>\n<h2 id=\"下载电子书\"><a class=\"anchor\" href=\"#下载电子书\">#</a> 下载电子书</h2>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbac289f345e8d036f2f6e.png\" alt=\"image-20240208194727799\" /></p>\n<h3 id=\"zlibrary\"><a class=\"anchor\" href=\"#zlibrary\">#</a> zlibrary</h3>\n<p>由于 free 经常被免费下架</p>\n<p><strong>最新版网址获取方式：</strong></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">blackbox@zlib.se</span><br></pre></td></tr></table></figure></p>\n<p>向这个邮箱发任意信息，等几分钟就会回复最新版的网址</p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbac4c9f345e8d036f8948.png\" alt=\"image-20240208200237222\" /></p>\n<p><img data-src=\"https://pic.imgdb.cn/item/65cbac679f345e8d036fcbdc.png\" alt=\"image-20240208200405438\" /></p>\n",
            "tags": [
                "深度学习",
                "跟着李沐学深度学习"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/",
            "url": "https://jinjiaojiao.top/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/",
            "title": "PyTorch学习第二章——深度学习入门",
            "date_published": "2024-02-11T10:25:18.000Z",
            "content_html": "<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NjU0Mzc5MQ==\">60 分钟快速入门 PyTorch - 知乎 (zhihu.com)</span></p>\n<h1 id=\"第二章pytorch之60min入门\"><a class=\"anchor\" href=\"#第二章pytorch之60min入门\">#</a> 第二章： <code>PyTorch</code>  之 <code>60min</code>  入门</h1>\n<p><img data-src=\"https://pic1.zhimg.com/80/v2-ef363cc5320400c63cf356c203d39bec_1440w.webp\" alt=\"img\" /></p>\n<h2 id=\"什么是-pytorch\"><a class=\"anchor\" href=\"#什么是-pytorch\">#</a> 什么是  <code>PyTorch</code> ?</h2>\n<p><code>PyTorch</code>  是一个基于 Python 的科学计算包，主要定位两类人群：</p>\n<ul>\n<li><code>NumPy</code>  的替代品，可以利用  <code>GPU</code>  的性能进行计算。</li>\n<li>深度学习研究平台拥有足够的<strong>灵活性</strong>和<strong>速度</strong></li>\n</ul>\n<h2 id=\"开始学习\"><a class=\"anchor\" href=\"#开始学习\">#</a> 开始学习</h2>\n<h3 id=\"tensors-张量\"><a class=\"anchor\" href=\"#tensors-张量\">#</a> Tensors (张量)</h3>\n<p>Tensors 类似于  <code>NumPy</code>  的  <code>ndarrays</code>  ，同时 Tensors 可以使用  <code>GPU</code>  进行计算。</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from __future__ import print_function</span><br><span class=\"line\">import torch</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"torchempty-声明一个未初始化的矩阵\"><a class=\"anchor\" href=\"#torchempty-声明一个未初始化的矩阵\">#</a> <strong>torch.empty()</strong>: 声明一个未初始化的矩阵。</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.empty(5, 3)</span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出:</p>\n<blockquote>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[9.2737e-41, 8.9074e-01, 1.9286e-37],</span><br><span class=\"line\">     [1.7228e-34, 5.7064e+01, 9.2737e-41],</span><br><span class=\"line\">     [2.2803e+02, 1.9288e-37, 1.7228e-34],</span><br><span class=\"line\">     [1.4609e+04, 9.2737e-41, 5.8375e+04],</span><br><span class=\"line\">     [1.9290e-37, 1.7228e-34, 3.7402e+06]])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<h5 id=\"torchrand随机初始化一个矩阵\"><a class=\"anchor\" href=\"#torchrand随机初始化一个矩阵\">#</a> <strong>torch.rand()</strong>：随机初始化一个矩阵</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.rand(5, 3)</span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.6291,  0.2581,  0.6414],</span><br><span class=\"line\">  [ 0.9739,  0.8243,  0.2276],</span><br><span class=\"line\">  [ 0.4184,  0.1815,  0.5131],</span><br><span class=\"line\">  [ 0.5533,  0.5440,  0.0718],</span><br><span class=\"line\">  [ 0.2908,  0.1850,  0.5297]])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<h5 id=\"验证能否运行在gpu\"><a class=\"anchor\" href=\"#验证能否运行在gpu\">#</a> 验证能否运行在 GPU</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.cuda.is_available()</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"torchzeros创建数值皆为-0-的矩阵\"><a class=\"anchor\" href=\"#torchzeros创建数值皆为-0-的矩阵\">#</a> <strong>torch.zeros()</strong>：创建数值皆为 0 的矩阵</h5>\n<p>Construct a matrix filled zeros and of dtype long:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.zeros(5, 3, dtype=torch.long)</span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出:</p>\n<blockquote>\n<p>tensor([[ 0,  0,  0],<br />\n[ 0,  0,  0],<br />\n[ 0,  0,  0],<br />\n[ 0,  0,  0],<br />\n[ 0,  0,  0]])</p>\n</blockquote>\n<h5 id=\"torchtensor直接传递-tensor-数值来创建\"><a class=\"anchor\" href=\"#torchtensor直接传递-tensor-数值来创建\">#</a> <strong>torch.tensor()</strong>：直接传递 tensor 数值来创建</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor 数值是 [5.5 , 3]</span><br><span class=\"line\">x = torch.tensor([5.5, 3])</span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出:</p>\n<blockquote>\n<p>tensor([ 5.5000,  3.0000])</p>\n</blockquote>\n<p>除了上述几种方法，还可以根据已有的 tensor 变量创建新的 tensor 变量，这种做法的好处就是可以保留已有 tensor 的一些属性，包括尺寸大小、数值属性，除非是重新定义这些属性。相应的实现方法如下：</p>\n<h5 id=\"tensornew_onesnew_-方法需要输入尺寸大小\"><a class=\"anchor\" href=\"#tensornew_onesnew_-方法需要输入尺寸大小\">#</a> tensor.new_ones()<em>：new_</em>() 方法需要输入尺寸大小</h5>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 显示定义新的尺寸是 5*3，数值类型是 torch.double</span><br><span class=\"line\">tensor2 = tensor1.new_ones(5, 3, dtype=torch.double)  # new_* 方法需要输入 tensor 大小</span><br><span class=\"line\">print(tensor2)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"torchrandn_likeold_tensor保留相同的尺寸大小\"><a class=\"anchor\" href=\"#torchrandn_likeold_tensor保留相同的尺寸大小\">#</a> <strong>torch.randn_like(old_tensor)</strong>：保留相同的尺寸大小</h5>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 修改数值类型</span><br><span class=\"line\">tensor3 = torch.randn_like(tensor2, dtype=torch.float)</span><br><span class=\"line\">print(&#x27;tensor3: &#x27;, tensor3)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果，这里是根据上个方法声明的  <code>tensor2</code>  变量来声明新的变量，可以看出尺寸大小都是 5*3，但是数值类型是改变了的。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor3:  tensor([[-0.4491, -0.2634, -0.0040],</span><br><span class=\"line\">        [-0.1624,  0.4475, -0.8407],</span><br><span class=\"line\">        [-0.6539, -1.2772,  0.6060],</span><br><span class=\"line\">        [ 0.2304,  0.0879, -0.3876],</span><br><span class=\"line\">        [ 1.2900, -0.7475, -1.8212]])</span><br></pre></td></tr></table></figure></p>\n<p>最后，对 tensors 的尺寸大小获取可以采用  <code>tensor.size()</code>  方法：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(tensor3.size())  </span><br><span class=\"line\"># 输出: torch.Size([5, 3])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"获取它的维度信息\"><a class=\"anchor\" href=\"#获取它的维度信息\">#</a> <strong>获取它的维度信息:</strong></h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(x.size())</span><br></pre></td></tr></table></figure></p>\n<p>输出:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([5, 3])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<p>注意</p>\n<p><strong>注意</strong>：  <code>torch.Size</code>  实际上是<strong>元组 (tuple) 类型，所以支持所有的元组操作</strong>。</p>\n<h3 id=\"操作\"><a class=\"anchor\" href=\"#操作\">#</a> 操作</h3>\n<p>在接下来的例子中，我们将会看到加法操作。</p>\n<h4 id=\"加法\"><a class=\"anchor\" href=\"#加法\">#</a> 加法</h4>\n<h5 id=\"运算符\"><a class=\"anchor\" href=\"#运算符\">#</a> + 运算符</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.rand(5, 3)</span><br><span class=\"line\">print(x + y)</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class=\"line\">     [ 2.3854,  0.0707,  2.1970],</span><br><span class=\"line\">     [-0.3587,  1.2359,  1.8951],</span><br><span class=\"line\">     [-0.1189, -0.1376,  0.4647],</span><br><span class=\"line\">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<h5 id=\"add\"><a class=\"anchor\" href=\"#add\">#</a> add</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(torch.add(x, y))</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class=\"line\">        [ 2.3854,  0.0707,  2.1970],</span><br><span class=\"line\">        [-0.3587,  1.2359,  1.8951],</span><br><span class=\"line\">        [-0.1189, -0.1376,  0.4647],</span><br><span class=\"line\">        [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"result提供一个输出\"><a class=\"anchor\" href=\"#result提供一个输出\">#</a> result 提供一个输出</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">result = torch.empty(5, 3)</span><br><span class=\"line\">torch.add(x, y, out=result) #x+y 结果储存在result中</span><br><span class=\"line\">print(result)</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class=\"line\">     [ 2.3854,  0.0707,  2.1970],</span><br><span class=\"line\">     [-0.3587,  1.2359,  1.8951],</span><br><span class=\"line\">     [-0.1189, -0.1376,  0.4647],</span><br><span class=\"line\">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<h5 id=\"add_-直接修改变量\"><a class=\"anchor\" href=\"#add_-直接修改变量\">#</a> add_ 直接修改变量</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># adds x to y</span><br><span class=\"line\">y.add_(x)</span><br><span class=\"line\">print(y)</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class=\"line\">     [ 2.3854,  0.0707,  2.1970],</span><br><span class=\"line\">     [-0.3587,  1.2359,  1.8951],</span><br><span class=\"line\">     [-0.1189, -0.1376,  0.4647],</span><br><span class=\"line\">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<p><strong>Note：</strong></p>\n<p>注意 任何使张量会发生变化的操作都有一个前缀 '_'。例如： <code>x.copy_(y)</code> ,  <code>x.t_()</code> , 将会改变  <code>x</code> .</p>\n<h4 id=\"对于-tensor-的访问\"><a class=\"anchor\" href=\"#对于-tensor-的访问\">#</a> 对于 Tensor 的访问</h4>\n<p>除了加法运算操作，，和 Numpy 对数组类似，可以使用索引来访问某一维的数据，如下所示：</p>\n<h5 id=\"索引操作\"><a class=\"anchor\" href=\"#索引操作\">#</a> <strong>索引</strong>操作</h5>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 访问 tensor3 第一列数据</span><br><span class=\"line\">print(x[:, 1])</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([ 0.4477, -0.0048,  1.0878, -0.2174,  1.3609])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<h5 id=\"torchview对-tensor-的尺寸修改\"><a class=\"anchor\" href=\"#torchview对-tensor-的尺寸修改\">#</a> torch.view ()：对 Tensor 的尺寸修改</h5>\n<p>如果你想改变一个 tensor 的大小或者形状，你可以使用  <code>torch.view</code> :</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.randn(4, 4)</span><br><span class=\"line\">y = x.view(16)  # 1*16</span><br><span class=\"line\">z = x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class=\"line\">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<p><code>-1</code>  用于 <code>view</code>  方法中作为一个特殊的参数值，表示自动计算该维度的大小。当你重新调整一个张量的形状时， <code>-1</code>  将会被替换为一个值，这个值是根据张量的总元素数和其他维度的大小自动计算出来的，以保证新形状的元素总数与原张量相同。</p>\n<p>总数不变</p>\n<h5 id=\"item\"><a class=\"anchor\" href=\"#item\">#</a> .item()</h5>\n<p>如果你有一个元素 tensor ，<strong>使用 .item () 来获得这个 value</strong> 。如果 tensor 仅有一个元素，可以采用  <code>.item()</code>  来获取类似 Python 中整数类型的数值：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.randn(1)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">print(x.item())</span><br></pre></td></tr></table></figure></p>\n<p>Out:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([ 0.9422])</span><br><span class=\"line\">0.9422121644020081</span><br></pre></td></tr></table></figure></p>\n<p>更多运算操作请看文档</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS90b3JjaC5odG1s\">torch — PyTorch 2.2 documentation</span></p>\n<h3 id=\"和numpy数组的转换\"><a class=\"anchor\" href=\"#和numpy数组的转换\">#</a> 和 Numpy 数组的转换</h3>\n<p>Tensor 和 Numpy 的数组可以相互转换，并且两者转换后共享在 CPU 下的内存空间，即改变其中一个的数值，另一个变量也会随之改变。</p>\n<h3 id=\"tensor-转换为-numpy-数组\"><a class=\"anchor\" href=\"#tensor-转换为-numpy-数组\">#</a> <strong>Tensor 转换为 Numpy 数组</strong></h3>\n<p>实现 Tensor 转换为 Numpy 数组的例子如下所示，调用  <code>tensor.numpy()</code>  可以实现这个转换操作。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.ones(5)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\">print(b)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([1., 1., 1., 1., 1.])</span><br><span class=\"line\">[1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"numpy-数组转换为-tensor\"><a class=\"anchor\" href=\"#numpy-数组转换为-tensor\">#</a> <strong>Numpy 数组转换为 Tensor</strong></h3>\n<p>转换的操作是调用  <code>torch.from_numpy(numpy_array)</code>  方法。例子如下所示：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\">a = np.ones(5)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">np.add(a, 1, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[2. 2. 2. 2. 2.]</span><br><span class=\"line\">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>\n<p>在  <code>CPU</code>  上，除了  <code>CharTensor</code>  外的所有  <code>Tensor</code>  类型变量，都支持和  <code>Numpy</code>  数组的相互转换操作。</p>\n<h3 id=\"cuda-张量\"><a class=\"anchor\" href=\"#cuda-张量\">#</a> <strong>CUDA 张量</strong></h3>\n<p><code>Tensors</code>  可以通过  <code>.to</code>  方法转换到不同的设备上，即 CPU 或者 GPU 上。</p>\n<p>例子：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 当 CUDA 可用的时候，可用运行下方这段代码，采用 torch.device() 方法来改变 tensors 是否在 GPU 上进行计算操作</span><br><span class=\"line\">if torch.cuda.is_available():</span><br><span class=\"line\">    device = torch.device(&quot;cuda&quot;)          # 定义一个 CUDA 设备对象</span><br><span class=\"line\">    y = torch.ones_like(x, device=device)  # 显示创建在 GPU 上的一个 tensor</span><br><span class=\"line\">    x = x.to(device)                       # 也可以采用 .to(&quot;cuda&quot;) </span><br><span class=\"line\">    z = x + y</span><br><span class=\"line\">    print(z)</span><br><span class=\"line\">    print(z.to(&quot;cpu&quot;, torch.double))       # .to() 方法也可以改变数值类型</span><br></pre></td></tr></table></figure></p>\n<p>输出结果，第一个结果就是在 GPU 上的结果，打印变量的时候会带有  <code>device='cuda:0'</code> ，而第二个是在 CPU 上的变量。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([1.4549], device=&#x27;cuda:0&#x27;)</span><br><span class=\"line\">tensor([1.4549], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>\n<p>本小节教程：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovdGVuc29yX3R1dG9yaWFsLmh0bWw=\">https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html</span></p>\n<p>本小节的代码：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS9iYXNpY19wcmFjdGlzZS5pcHluYg==\">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/basic_practise.ipynb</span></p>\n<h3 id=\"autograd\"><a class=\"anchor\" href=\"#autograd\">#</a> <strong>autograd</strong></h3>\n<p>对于 Pytorch 的神经网络来说，非常关键的一个库就是  <code>autograd</code>  ，</p>\n<p>提供了对  <code>Tensors</code>  上所有运算操作的<strong>自动微分功能</strong>，也就是<strong>计算梯度</strong>的功能。</p>\n<p>它属于  <code>define-by-run</code>  类型框架，即反向传播操作的定义是根据代码的运行方式，因此每次迭代都可以是不同的。</p>\n<h3 id=\"张量\"><a class=\"anchor\" href=\"#张量\">#</a> <strong>张量</strong></h3>\n<p><code>torch.Tensor</code>  是 Pytorch 最主要的库，当设置它的属性  <code>.requires_grad=True</code> ，那么就会开始<strong>追踪在该变量上的所有操作</strong>，而完成计算后，可以调用  <code>.backward()</code>  并自动计算所有的梯度，得到的梯度都保存在属性  <code>.grad</code>  中。</p>\n<p>调用  <code>.detach()</code>  方法<strong>分离出计算的历史</strong>，可以停止一个 tensor 变量继续追踪其历史信息 ，同时也防止未来的计算会被追踪。</p>\n<p>使用 <code>with torch.no_grad():</code>  就是告诉 PyTorch：“现在我只想用模型来做一些前向计算，不需要做梯度更新，请暂时不要保存那些用于梯度更新所必需的信息，以节省计算资源和内存”。这样做可以让模型运行得更快，同时消耗更少的资源。</p>\n<h4 id=\"function\"><a class=\"anchor\" href=\"#function\">#</a> Function</h4>\n<p>对于  <code>autograd</code>  的实现，还有一个类也是非常重要 <code>Function</code>  。</p>\n<p><code>Tensor</code>  和  <code>Function</code>  两个类是有关联并建立了一个<strong>非循环的图</strong>，可以编码一个完整的计算记录。每个 tensor 变量都带有属性  <code>.grad_fn</code>  ，该属性引用了创建了这个变量的  <code>Function</code>  （除了由用户创建的 Tensors，它们的  <code>grad_fn=None</code>  )。</p>\n<p>&lt;details&gt;<br />\n&lt;summary&gt;grad_fn&lt;/summary&gt;<br />\n 在深度学习中，模型训练的一个重要步骤是计算损失函数（即模型输出与真实值之间的差距）关于模型参数的梯度（或导数），然后根据这些梯度来更新模型参数，以使损失函数的值减小。这个过程称为梯度下降。PyTorch 通过建立一个计算图来帮助实现这个过程，而这个计算图是由 Tensor 和 Function 这两个类的实例组成的。<br />\nTensor<br />\n 在 PyTorch 中，Tensor 是一个多维数组，用于存储模型的输入数据、参数、输出数据以及计算过程中的各种中间数据。每个 Tensor 都可以跟踪它是如何被创建的 —— 即它是通过什么样的操作从其他 Tensor 转换而来的。Function<br />\n 每个操作，不管是简单的数学运算还是复杂的神经网络层操作，都可以看作是一个 Function。这些 Function 不仅执行计算，还记录了计算的细节，以便于后续进行梯度的反向传播。<br />\n计算图<br />\n当你在 PyTorch 中执行操作时，你实际上是在构建一个计算图。这个图是由节点（Tensor）和边（Function，表示操作）组成的。这个图是向前构建的：从输入 Tensor 开始，通过各种操作，最终到达输出 Tensor。这个过程称为前向传播。<br />\n.grad_fn 属性<br />\n每个 Tensor 都有一个.grad_fn 属性，这个属性是一个指向 Function 的引用，即这个 Tensor 是通过哪个 Function 计算得到的。如果这个 Tensor 是直接由用户创建的（不是通过某些操作得到的），那么它的.grad_fn 就是 None，因为它不是通过计算得到的。<br />\n非循环图<br />\n这个计算图是非循环的，意味着数据流是有方向的，从输入流向输出，不会有任何循环或回路。这使得在图中进行前向传播和反向传播（用于计算梯度）变得简单明了。</p>\n<p>为什么这很重要？<br />\n当进行反向传播以计算梯度时，PyTorch 会沿着这个图从输出向后逐步移动，使用<strong>链式法</strong>则自动计算每个参数的梯度。这个过程完全自动化，用户不需要手动编写梯度计算代码，极大地简化了深度学习模型的训练过程。</p>\n<p>简而言之， <code>Tensor</code>  和 <code>Function</code>  通过<strong>计算图</strong>相互关联，这个图能够追踪整个计算过程，为自动梯度计算（自动微分）提供支持，使得深度学习模型的训练变得更加高效和简单。</p>\n<p>如果要进行求导运算，可以调用一个  <code>Tensor</code>  变量的方法  <code>.backward()</code>  。如果该变量是一个标量，即仅有一个元素，那么不需要传递任何参数给方法  <code>.backward()</code> ，当包含多个元素的时候，就必须指定一个  <code>gradient</code>  参数，表示匹配尺寸大小的 tensor，这部分见第二小节介绍梯度的内容。</p>\n<p>接下来就开始用代码来进一步介绍。</p>\n<p>首先导入必须的库：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure></p>\n<p>开始创建一个 tensor， 并让  <code>requires_grad=True</code>  来<strong>追踪该变量相关的计算操作</strong>：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.ones(2, 2, requires_grad=True)</span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1., 1.],</span><br><span class=\"line\">        [1., 1.]], requires_grad=True)</span><br></pre></td></tr></table></figure></p>\n<p>执行任意计算操作，这里进行简单的加法运算：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = x + 2</span><br><span class=\"line\">print(y)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[3., 3.],</span><br><span class=\"line\">        [3., 3.]], grad_fn=&lt;AddBackward&gt;)</span><br></pre></td></tr></table></figure></p>\n<p><code>y</code>  是一个操作的结果，所以它带有属性  <code>grad_fn</code> ：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(y.grad_fn)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;AddBackward object at 0x00000216D25DCC88&gt;</span><br></pre></td></tr></table></figure></p>\n<p>继续对变量  <code>y</code>  进行操作：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z = y * y * 3</span><br><span class=\"line\">out = z.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">print(&#x27;z=&#x27;, z)</span><br><span class=\"line\">print(&#x27;out=&#x27;, out)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z= tensor([[27., 27.],</span><br><span class=\"line\">        [27., 27.]], grad_fn=&lt;MulBackward&gt;)</span><br><span class=\"line\"></span><br><span class=\"line\">out= tensor(27., grad_fn=&lt;MeanBackward1&gt;)</span><br></pre></td></tr></table></figure></p>\n<p>实际上，一个  <code>Tensor</code>  变量的默认  <code>requires_grad</code>  是  <code>False</code>  ，可以像上述定义一个变量时候指定该属性是  <code>True</code> ，当然也可以定义变量后，调用  <code>.requires_grad_(True)</code>  设置为  <code>True</code>  ，这里带有后缀  <code>_</code>  是会改变变量本身的属性，在上一节介绍加法操作  <code>add_()</code>  说明过</p>\n<p>代码例子：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.randn(2, 2)</span><br><span class=\"line\">a = ((a * 3) / (a - 1))</span><br><span class=\"line\">print(a.requires_grad)</span><br><span class=\"line\">a.requires_grad_(True)</span><br><span class=\"line\">print(a.requires_grad)</span><br><span class=\"line\">b = (a * a).sum()</span><br><span class=\"line\">print(b.grad_fn)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果如下，第一行是为设置  <code>requires_grad</code>  的结果，接着显示调用  <code>.requires_grad_(True)</code> ，输出结果就是  <code>True</code>  。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">False</span><br><span class=\"line\"></span><br><span class=\"line\">True</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;SumBackward0 object at 0x00000216D25ED710&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"梯度\"><a class=\"anchor\" href=\"#梯度\">#</a> <strong>梯度</strong></h3>\n<p>接下来就是开始计算梯度，进行<strong>反向传播</strong>的操作。 <code>out</code>  变量是上一小节中定义的，它是一个标量，因此  <code>out.backward()</code>  相当于  <code>out.backward(torch.tensor(1.))</code>  ，</p>\n<p>代码如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">out.backward()</span><br><span class=\"line\"># 输出梯度 d(out)/dx</span><br><span class=\"line\">print(x.grad)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[4.5000, 4.5000],</span><br><span class=\"line\">        [4.5000, 4.5000]])</span><br></pre></td></tr></table></figure></p>\n<p>结果应该就是得到数值都是 4.5 的矩阵。这里我们用  <code>o</code>  表示  <code>out</code>  变量，那么根据之前的定义会有：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>O</mi><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>z</mi><mi>i</mi></msub><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">O = \\frac{1}{4} \\sum_i z_i,\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.599109em;vertical-align:-1.277669em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">4</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0500050000000003em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span></span></span></span></span></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mn>3</mn><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mn>2</mn><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">z_i = 3(x_i + 2)^2,\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">3</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\">2</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span></span></span></span></span></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><msub><mo fence=\"false\">∣</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>27</mn></mrow><annotation encoding=\"application/x-tex\">z_i \\big|_{x_i=1} = 27\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.3677899999999998em;vertical-align:-0.49980999999999987em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8679800000000001em;\"><span style=\"top:-2.2559899999999997em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.26698em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.86798em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35000999999999993em;\"><span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.051398000000000055em;\"><span style=\"top:-2.30029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.49980999999999987em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord\">7</span></span></span></span></span></p>\n<p>详细来说，初始定义的  <code>x</code>  是一个全为 1 的矩阵，然后加法操作  <code>x+2</code>  得到  <code>y</code>  ，接着  <code>y*y*3</code> ， 得到  <code>z</code>  ，并且此时  <code>z</code>  是一个 2*2 的矩阵，所以整体求平均得到  <code>out</code>  变量应该是除以 4，所以得到上述三条公式。</p>\n<p>因此，计算梯度：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>o</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>3</mn><mn>2</mn></mfrac><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mn>2</mn><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2} (x_i + 2),\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.20744em;vertical-align:-0.8360000000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.3139999999999996em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">o</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8360000000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.00744em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span></span></span></span></span></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>o</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo fence=\"true\">∣</mo></mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>9</mn><mn>2</mn></mfrac><mo>=</mo><mn>4.5</mn></mrow><annotation encoding=\"application/x-tex\">\\left.\\frac{\\partial o}{\\partial x_i}\\right|_{x_i=1} = \\frac{9}{2} = 4.5\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.57979em;vertical-align:-1.0998199999999998em;\"></span><span class=\"minner\"><span class=\"minner\"><span class=\"mopen nulldelimiter\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.3139999999999996em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">o</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8360000000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4799700000000002em;\"><span style=\"top:-1.65598em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.25698em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.85798em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.87897em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-3.47997em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500199999999999em;\"><span></span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:-0.5486119999999999em;\"><span style=\"top:-1.7002800000000005em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0998199999999998em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.00744em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">2</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">9</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span><span class=\"mord\">.</span><span class=\"mord\">5</span></span></span></span></span></p>\n<p>从数学上来说，如果你有一个向量值函数：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo>⃗</mo></mover><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y} = f(\\vec{x})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.714em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.20772em;\"><span class=\"overlay\" style=\"height:0.714em;width:0.471em;\"><svg width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n<p>那么对应的梯度是一个雅克比矩阵 (Jacobian matrix)：</p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210172846221.png\" alt=\"image-20240210172846221\" /></p>\n<p>一般来说， <code>torch.autograd</code>  就是用于计算雅克比向量 (vector-Jacobian) 乘积的工具。这里略过数学公式，直接上代码例子介绍：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.randn(3, requires_grad=True)</span><br><span class=\"line\"></span><br><span class=\"line\">y = x * 2</span><br><span class=\"line\">while y.data.norm() &lt; 1000:</span><br><span class=\"line\">    y = y * 2</span><br><span class=\"line\"></span><br><span class=\"line\">print(y)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"神经网络\"><a class=\"anchor\" href=\"#神经网络\">#</a> 神经网络</h2>\n<p>在 PyTorch 中  <code>torch.nn</code>  专门用于实现神经网络。其中  <code>nn.Module</code>  包含了网络层的搭建，以及一个方法 --  <code>forward(input)</code>  ，并返回网络的输出  <code>output</code>  .</p>\n<p>下面是一个经典的 LeNet 网络，用于对字符进行分类。</p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-06a914f4ee93f25c0d6c924df9b4b4cb_1440w.webp\" alt=\"img\" /></p>\n<p>对于神经网络来说，一个标准的<strong>训练流程</strong>是这样的：</p>\n<ul>\n<li>\n<p>定义一个<strong>多层的神经网络</strong></p>\n</li>\n<li>\n<p>对数据集的<strong>预处理</strong>并准备作为网络的输入</p>\n</li>\n<li>\n<p>将数据<strong>输入到网络</strong></p>\n</li>\n<li>\n<p>计算网络的<strong>损失</strong></p>\n</li>\n<li>\n<p><strong>反向传播</strong>，计算<strong>梯度</strong></p>\n</li>\n<li>\n<p><strong>更新</strong>网络的梯度，一个简单的更新规则是  <code>weight = weight - learning_rate * gradient</code></p>\n</li>\n</ul>\n<h3 id=\"定义网络\"><a class=\"anchor\" href=\"#定义网络\">#</a> <strong>定义网络</strong></h3>\n<p>首先定义一个神经网络，下面是一个 5 层的卷积神经网络，包含两层卷积层和三层全连接层：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net,self).__init__()</span><br><span class=\"line\">         <span class=\"comment\"># 输入图像是单通道，conv1 kenrnel size=5*5，输出通道 6</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span> ,<span class=\"number\">6</span> ,<span class=\"number\">5</span> )</span><br><span class=\"line\">        <span class=\"comment\"># conv2 kernel size=5*5, 输出通道 16</span></span><br><span class=\"line\">        <span class=\"comment\">#全连接层</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span> , <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>,<span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># max-pooling 采用一个 (2,2) 的滑动窗口</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">         <span class=\"comment\"># 核(kernel)大小是方形的话，可仅定义一个数字，如 (2,2) 用 2 即可</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>)</span><br><span class=\"line\">        x = x.view(-<span class=\"number\">1</span>, self.num_flat_features(x))</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">num_flat_features</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 除了 batch 维度外的所有维度</span></span><br><span class=\"line\">        size = x.size()[<span class=\"number\">1</span>:]</span><br><span class=\"line\">        num_features = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</span><br><span class=\"line\">            num_features *= s</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num_features</span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net)</span><br><span class=\"line\">打印网络结构：</span><br><span class=\"line\"></span><br><span class=\"line\">Net(</span><br><span class=\"line\">  (conv1): Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (conv2): Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (fc1): Linear(in_features=<span class=\"number\">400</span>, out_features=<span class=\"number\">120</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  (fc2): Linear(in_features=<span class=\"number\">120</span>, out_features=<span class=\"number\">84</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  (fc3): Linear(in_features=<span class=\"number\">84</span>, out_features=<span class=\"number\">10</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<p>打印网络结构：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Net(</span><br><span class=\"line\">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class=\"line\">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class=\"line\">  (fc1): Linear(in_features=400, out_features=120, bias=True)</span><br><span class=\"line\">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class=\"line\">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<p>这里必须实现  <code>forward</code>  函数，而  <code>backward</code>  函数在采用  <code>autograd</code>  时就自动定义好了，在  <code>forward</code>  方法可以采用任何的张量操作。</p>\n<p><code>net.parameters()</code>  可以返回网络的训练参数，使用例子如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">params = list(net.parameters())</span><br><span class=\"line\">print(&#x27;参数数量: &#x27;, len(params))</span><br><span class=\"line\"># conv1.weight</span><br><span class=\"line\">print(&#x27;第一个参数大小: &#x27;, params[0].size())</span><br></pre></td></tr></table></figure></p>\n<p>输出：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">参数数量:  10</span><br><span class=\"line\">第一个参数大小:  torch.Size([6, 1, 5, 5])</span><br></pre></td></tr></table></figure></p>\n<p>然后简单测试下这个网络，随机生成一个 32*32 的输入：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 随机定义一个变量输入网络</span><br><span class=\"line\">input = torch.randn(1, 1, 32, 32)</span><br><span class=\"line\">out = net(input)</span><br><span class=\"line\">print(out)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.1005,  0.0263,  0.0013, -0.1157, -0.1197, -0.0141,  0.1425, -0.0521,</span><br><span class=\"line\">          0.0689,  0.0220]], grad_fn=&lt;ThAddmmBackward&gt;)</span><br></pre></td></tr></table></figure></p>\n<p>接着反向传播需要先清空梯度缓存，并反向传播随机梯度：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 清空所有参数的梯度缓存，然后计算随机梯度进行反向传播</span><br><span class=\"line\">net.zero_grad()</span><br><span class=\"line\">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure></p>\n<p><strong>注意</strong>：</p>\n<blockquote>\n<p><code>torch.nn</code>  只支持 ** 小批量 (mini-batches)** 数据，也就是输入不能是单个样本，比如对于  <code>nn.Conv2d</code>  接收的输入是一个 4 维张量 -- <code>nSamples * nChannels * Height * Width</code>  。<br />\n所以，如果你输入的是单个样本，<strong>需要采用</strong>  <code>**input.unsqueeze(0)**</code>  <strong>来扩充一个假的 batch 维度，即从 3 维变为 4 维</strong>。</p>\n</blockquote>\n<h3 id=\"损失函数\"><a class=\"anchor\" href=\"#损失函数\">#</a> <strong>损失函数</strong></h3>\n<p>损失函数的输入是  <code>(output, target)</code>  ，即网络输出和真实标签对的数据，然后返回一个数值表示网络输出和真实标签的差距。</p>\n<p>PyTorch 中其实已经定义了不少的<span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy9kb2NzL25uLmh0bWwlMjNsb3NzLWZ1bmN0aW9ucw==\">损失函数</span>，这里仅采用简单的均方误差： <code>nn.MSELoss</code>  ，例子如下：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output = net(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"comment\"># 定义伪标签</span></span><br><span class=\"line\">target = torch.randn(<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"comment\"># 调整大小，使得和 output 一样的 size</span></span><br><span class=\"line\">target = target.view(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)</span><br><span class=\"line\">criterion = nn.MSELoss()</span><br><span class=\"line\"></span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(loss)</span><br></pre></td></tr></table></figure></p>\n<p>输出如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor(0.6524, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure></p>\n<p>这里，整个网络的数据输入到输出经历的计算图如下所示，其实也就是数据从输入层到输出层，计算  <code>loss</code>  的过程。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class=\"line\">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class=\"line\">      -&gt; MSELoss</span><br><span class=\"line\">      -&gt; loss</span><br></pre></td></tr></table></figure></p>\n<p>如果调用  <code>loss.backward()</code>  ，那么整个图都是可微分的，也就是说包括  <code>loss</code>  ，图中的所有张量变量，只要其属性  <code>requires_grad=True</code>  ，那么其梯度  <code>.grad</code>  张量都会随着梯度一直累计。</p>\n<p>用代码来说明：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># MSELoss</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(loss.grad_fn)</span><br><span class=\"line\"><span class=\"comment\"># Linear layer</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(loss.grad_fn.next_functions[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># Relu</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(loss.grad_fn.next_functions[<span class=\"number\">0</span>][<span class=\"number\">0</span>].next_functions[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure></p>\n<p>输出：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;MseLossBackward object at 0x0000019C0C349908&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;ThAddmmBackward object at 0x0000019C0C365A58&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;ExpandBackward object at 0x0000019C0C3659E8&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"反向传播\"><a class=\"anchor\" href=\"#反向传播\">#</a> <strong>反向传播</strong></h3>\n<p>反向传播的实现只需要调用  <code>loss.backward()</code>  即可，当然首先需要清空当前梯度缓存，即 <code>.zero_grad()</code>  方法，否则之前的梯度会累加到当前的梯度，这样会影响权值参数的更新。</p>\n<p>下面是一个简单的例子，以  <code>conv1</code>  层的偏置参数  <code>bias</code>  在反向传播前后的结果为例：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清空所有参数的梯度缓存</span></span><br><span class=\"line\">net.zero_grad()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.conv1.bias.grad)</span><br><span class=\"line\"></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conv1.bias.grad before backward</span><br><span class=\"line\">tensor([0., 0., 0., 0., 0., 0.])</span><br><span class=\"line\"></span><br><span class=\"line\">conv1.bias.grad after backward</span><br><span class=\"line\">tensor([ 0.0069,  0.0021,  0.0090, -0.0060, -0.0008, -0.0073])</span><br></pre></td></tr></table></figure></p>\n<p>了解更多有关  <code>torch.nn</code>  库，可以查看官方文档：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1s\">https://pytorch.org/docs/stable/nn.html</span></p>\n<h3 id=\"更新权重\"><a class=\"anchor\" href=\"#更新权重\">#</a> <strong>更新权重</strong></h3>\n<p>采用随机梯度下降 (Stochastic Gradient Descent, SGD) 方法的最简单的更新权重规则如下：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure></p>\n<p>按照这个规则，代码实现如下所示：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 简单实现权重的更新例子</span><br><span class=\"line\">learning_rate = 0.01</span><br><span class=\"line\">for f in net.parameters():</span><br><span class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure></p>\n<p>但是这只是最简单的规则，深度学习有很多的优化算法，不仅仅是  <code>SGD</code> ，还有  <code>Nesterov-SGD, Adam, RMSProp</code>  等等，为了采用这些不同的方法，这里采用  <code>torch.optim</code>  库，使用例子如下所示：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch.optim as optim</span><br><span class=\"line\"># 创建优化器</span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class=\"line\"></span><br><span class=\"line\"># 在训练过程中执行下列操作</span><br><span class=\"line\">optimizer.zero_grad() # 清空梯度缓存</span><br><span class=\"line\">output = net(input)</span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"># 更新权重</span><br><span class=\"line\">optimizer.step()</span><br></pre></td></tr></table></figure></p>\n<p><strong>注意</strong>，同样需要调用  <code>optimizer.zero_grad()</code>  方法清空梯度缓存。</p>\n<p>本小节教程：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovbmV1cmFsX25ldHdvcmtzX3R1dG9yaWFsLmh0bWw=\">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</span></p>\n<p>本小节的代码：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS9uZXVyYWxfbmV0d29yay5pcHluYg==\">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/neural_network.ipynb</span></p>\n<h3 id=\"训练分类器\"><a class=\"anchor\" href=\"#训练分类器\">#</a> <strong>训练分类器</strong></h3>\n<p>上一节介绍了如何构建神经网络、计算  <code>loss</code>  和更新网络的权值参数，接下来需要做的就是实现一个图片分类器。</p>\n<h4 id=\"训练数据\"><a class=\"anchor\" href=\"#训练数据\">#</a> <strong>训练数据</strong></h4>\n<p>在训练分类器前，当然需要考虑数据的问题。通常在处理如图片、文本、语音或者视频数据的时候，一般都采用标准的 Python 库将其加载并转成 Numpy 数组，然后再转回为 PyTorch 的张量。</p>\n<ul>\n<li>对于图像，可以采用  <code>Pillow, OpenCV</code>  库；</li>\n<li>对于语音，有  <code>scipy</code>  和  <code>librosa</code> ;</li>\n<li>对于文本，可以选择原生 Python 或者 Cython 进行加载数据，或者使用  <code>NLTK</code>  和  <code>SpaCy</code>  。</li>\n</ul>\n<p>PyTorch 对于计算机视觉，特别创建了一个  <code>torchvision</code>  的库，它包含一个数据加载器 (data loader)，可以加载比较常见的数据集，比如  <code>Imagenet, CIFAR10, MNIST</code>  等等，然后还有一个用于图像的数据转换器 (data transformers)，调用的库是  <code>torchvision.datasets</code>  和  <code>torch.utils.data.DataLoader</code>  。</p>\n<p>在本教程中，将采用  <code>CIFAR10</code>  数据集，它包含 10 个类别，分别是飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。数据集中的图片都是  <code>3x32x32</code> 。一些例子如下所示：</p>\n<p><img data-src=\"https://pic2.zhimg.com/80/v2-2dcc41f9079d1abf5883a113c0d1ca31_1440w.webp\" alt=\"img\" /></p>\n<h3 id=\"训练图片分类器\"><a class=\"anchor\" href=\"#训练图片分类器\">#</a> <strong>训练图片分类器</strong></h3>\n<p>训练流程如下：</p>\n<ol>\n<li>通过调用  <code>torchvision</code>  加载和归一化  <code>CIFAR10</code>  训练集和测试集；</li>\n<li>构建一个卷积神经网络；</li>\n<li>定义一个损失函数；</li>\n<li>在训练集上训练网络；</li>\n<li>在测试集上测试网络性能。</li>\n</ol>\n<h4 id=\"加载和归一化-cifar10\"><a class=\"anchor\" href=\"#加载和归一化-cifar10\">#</a> <strong>加载和归一化 CIFAR10</strong></h4>\n<p>首先导入必须的包：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</span><br></pre></td></tr></table></figure></p>\n<p>`</p>\n<p><code>torchvision</code>  的数据集输出的图片都是  <code>PILImage</code>  ，即取值范围是  <code>[0, 1]</code>  ，这里需要做一个转换，变成取值范围是  <code>[-1, 1]</code>  ,</p>\n<p>代码如下所示：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将图片数据从 [0,1] 归一化为 [-1, 1] 的取值范围</span></span><br><span class=\"line\">transform = transforms.Compose(</span><br><span class=\"line\">    [transforms.ToTensor(),</span><br><span class=\"line\">     transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))])</span><br><span class=\"line\"></span><br><span class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                        download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</span><br><span class=\"line\">                                          shuffle=<span class=\"literal\">True</span>, num_workers=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">testset = torchvision.datasets.CIFAR10(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">False</span>,</span><br><span class=\"line\">                                       download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class=\"number\">4</span>,</span><br><span class=\"line\">                                         shuffle=<span class=\"literal\">False</span>, num_workers=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">classes = (<span class=\"string\">&#x27;plane&#x27;</span>, <span class=\"string\">&#x27;car&#x27;</span>, <span class=\"string\">&#x27;bird&#x27;</span>, <span class=\"string\">&#x27;cat&#x27;</span>,</span><br><span class=\"line\">           <span class=\"string\">&#x27;deer&#x27;</span>, <span class=\"string\">&#x27;dog&#x27;</span>, <span class=\"string\">&#x27;frog&#x27;</span>, <span class=\"string\">&#x27;horse&#x27;</span>, <span class=\"string\">&#x27;ship&#x27;</span>, <span class=\"string\">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure></p>\n<p>这里下载好数据后，可以可视化部分训练图片，代码如下：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 展示图片的函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">imshow</span>(<span class=\"params\">img</span>):</span><br><span class=\"line\">    img = img / <span class=\"number\">2</span> + <span class=\"number\">0.5</span>     <span class=\"comment\"># 非归一化</span></span><br><span class=\"line\">    npimg = img.numpy()</span><br><span class=\"line\">    plt.imshow(np.transpose(npimg, (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 随机获取训练集图片</span></span><br><span class=\"line\">dataiter = <span class=\"built_in\">iter</span>(trainloader)</span><br><span class=\"line\">images, labels = dataiter.<span class=\"built_in\">next</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 展示图片</span></span><br><span class=\"line\">imshow(torchvision.utils.make_grid(images))</span><br><span class=\"line\"><span class=\"comment\"># 打印图片类别标签</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27; &#x27;</span>.join(<span class=\"string\">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>)))</span><br></pre></td></tr></table></figure></p>\n<p>展示图片如下所示：</p>\n<p><img data-src=\"https://pic2.zhimg.com/80/v2-736499796b713d873d1f9ae72fbc66f5_1440w.webp\" alt=\"img\" /></p>\n<p>其类别标签为：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">frog plane   dog  ship</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"构建一个卷积神经网络\"><a class=\"anchor\" href=\"#构建一个卷积神经网络\">#</a> <strong>构建一个卷积神经网络</strong></h4>\n<p>这部分内容其实直接采用上一节定义的网络即可，除了修改  <code>conv1</code>  的输入通道，从 1 变为 3，因为这次接收的是 3 通道的彩色图片。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.pool = nn.MaxPool2d(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class=\"line\">        x = x.view(-<span class=\"number\">1</span>, <span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>)</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"定义损失函数和优化器\"><a class=\"anchor\" href=\"#定义损失函数和优化器\">#</a> <strong>定义损失函数和优化器</strong></h4>\n<p>这里采用<strong>类别交叉熵函数</strong>和<strong>带有动量的 SGD 优化方法：</strong></p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"训练网络\"><a class=\"anchor\" href=\"#训练网络\">#</a> <strong>训练网络</strong></h4>\n<p>第四步自然就是开始训练网络，指定需要迭代的 epoch，然后输入数据，指定次数打印当前网络的信息，比如  <code>loss</code>  或者准确率等性能评价标准。</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\">start = time.time()</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    running_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(trainloader, <span class=\"number\">0</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 获取输入数据</span></span><br><span class=\"line\">        inputs, labels = data</span><br><span class=\"line\">        <span class=\"comment\"># 清空梯度缓存</span></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = net(inputs)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 打印统计信息</span></span><br><span class=\"line\">        running_loss += loss.item()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 每 2000 次迭代打印一次信息</span></span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class=\"number\">1</span>, i+<span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</span><br><span class=\"line\">            running_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Finished Training! Total cost time: &#x27;</span>, time.time()-start)</span><br></pre></td></tr></table></figure></p>\n<p>这里定义训练总共 2 个 epoch，训练信息如下，大概耗时为 77s。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1,  2000] loss: 2.226</span><br><span class=\"line\">[1,  4000] loss: 1.897</span><br><span class=\"line\">[1,  6000] loss: 1.725</span><br><span class=\"line\">[1,  8000] loss: 1.617</span><br><span class=\"line\">[1, 10000] loss: 1.524</span><br><span class=\"line\">[1, 12000] loss: 1.489</span><br><span class=\"line\">[2,  2000] loss: 1.407</span><br><span class=\"line\">[2,  4000] loss: 1.376</span><br><span class=\"line\">[2,  6000] loss: 1.354</span><br><span class=\"line\">[2,  8000] loss: 1.347</span><br><span class=\"line\">[2, 10000] loss: 1.324</span><br><span class=\"line\">[2, 12000] loss: 1.311</span><br><span class=\"line\"></span><br><span class=\"line\">Finished Training! Total cost time:  77.24696755409241</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"测试模型性能\"><a class=\"anchor\" href=\"#测试模型性能\">#</a> <strong>测试模型性能</strong></h4>\n<p>训练好一个网络模型后，就需要用测试集进行测试，检验网络模型的泛化能力。对于图像分类任务来说，一般就是用准确率作为评价标准。</p>\n<p>首先，我们先用一个  <code>batch</code>  的图片进行小小测试，这里  <code>batch=4</code>  ，也就是 4 张图片，代码如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataiter = iter(testloader)</span><br><span class=\"line\">images, labels = dataiter.next()</span><br><span class=\"line\"></span><br><span class=\"line\"># 打印图片</span><br><span class=\"line\">imshow(torchvision.utils.make_grid(images))</span><br><span class=\"line\">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure></p>\n<p>图片和标签分别如下所示：</p>\n<p><img data-src=\"https://pic4.zhimg.com/80/v2-ddb522e45f298b8da5ab6d3a48ac470b_1440w.webp\" alt=\"img\" /></p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GroundTruth:    cat  ship  ship plane</span><br></pre></td></tr></table></figure></p>\n<p>然后用这四张图片输入网络，看看网络的预测结果：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 网络输出</span><br><span class=\"line\">outputs = net(images)</span><br><span class=\"line\"></span><br><span class=\"line\"># 预测结果</span><br><span class=\"line\">_, predicted = torch.max(outputs, 1)</span><br><span class=\"line\">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[predicted[j]] for j in range(4)))</span><br></pre></td></tr></table></figure></p>\n<p>输出为：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Predicted:    cat  ship  ship  ship</span><br></pre></td></tr></table></figure></p>\n<p>前面三张图片都预测正确了，第四张图片错误预测飞机为船。</p>\n<p>接着，让我们看看在整个测试集上的准确率可以达到多少吧！</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">correct = 0</span><br><span class=\"line\">total = 0</span><br><span class=\"line\">with torch.no_grad():</span><br><span class=\"line\">    for data in testloader:</span><br><span class=\"line\">        images, labels = data</span><br><span class=\"line\">        outputs = net(images)</span><br><span class=\"line\">        _, predicted = torch.max(outputs.data, 1)</span><br><span class=\"line\">        total += labels.size(0)</span><br><span class=\"line\">        correct += (predicted == labels).sum().item()</span><br><span class=\"line\"></span><br><span class=\"line\">print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % (100 * correct / total))</span><br></pre></td></tr></table></figure></p>\n<p>输出结果如下</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Accuracy of the network on the 10000 test images: 55 %</span><br></pre></td></tr></table></figure></p>\n<p>这里可能准确率并不一定一样，教程中的结果是  <code>51%</code>  ，因为权重初始化问题，可能多少有些浮动，相比随机猜测 10 个类别的准确率 (即 10%)，这个结果是不错的，当然实际上是非常不好，不过我们仅仅采用 5 层网络，而且仅仅作为教程的一个示例代码。</p>\n<p>然后，还可以再进一步，查看每个类别的分类准确率，跟上述代码有所不同的是，计算准确率部分是  <code>c = (predicted == labels).squeeze()</code> ，这段代码其实会根据预测和真实标签是否相等，输出 1 或者 0，表示真或者假，因此在计算当前类别正确预测数</p>\n<p>量时候直接相加，预测正确自然就是加 1，错误就是加 0，也就是没有变化。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class_correct = list(0. for i in range(10))</span><br><span class=\"line\">class_total = list(0. for i in range(10))</span><br><span class=\"line\">with torch.no_grad():</span><br><span class=\"line\">    for data in testloader:</span><br><span class=\"line\">        images, labels = data</span><br><span class=\"line\">        outputs = net(images)</span><br><span class=\"line\">        _, predicted = torch.max(outputs, 1)</span><br><span class=\"line\">        c = (predicted == labels).squeeze()</span><br><span class=\"line\">        for i in range(4):</span><br><span class=\"line\">            label = labels[i]</span><br><span class=\"line\">            class_correct[label] += c[i].item()</span><br><span class=\"line\">            class_total[label] += 1</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">for i in range(10):</span><br><span class=\"line\">    print(&#x27;Accuracy of %5s : %2d %%&#x27; % (classes[i], 100 * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure></p>\n<p>输出结果，可以看到猫、鸟、鹿是错误率前三，即预测最不准确的三个类别，反倒是船和卡车最准确。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Accuracy of plane : 58 %</span><br><span class=\"line\">Accuracy of   car : 59 %</span><br><span class=\"line\">Accuracy of  bird : 40 %</span><br><span class=\"line\">Accuracy of   cat : 33 %</span><br><span class=\"line\">Accuracy of  deer : 39 %</span><br><span class=\"line\">Accuracy of   dog : 60 %</span><br><span class=\"line\">Accuracy of  frog : 54 %</span><br><span class=\"line\">Accuracy of horse : 66 %</span><br><span class=\"line\">Accuracy of  ship : 70 %</span><br><span class=\"line\">Accuracy of truck : 72 %</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"在-gpu-上训练\"><a class=\"anchor\" href=\"#在-gpu-上训练\">#</a> <strong>在 GPU 上训练</strong></h3>\n<p>深度学习自然需要 GPU 来加快训练速度的。所以接下来介绍如果是在 GPU 上训练，应该如何实现。</p>\n<p>首先，需要检查是否有可用的 GPU 来训练，代码如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class=\"line\">print(device)</span><br></pre></td></tr></table></figure></p>\n<p>输出结果如下，这表明你的第一块 GPU 显卡或者唯一的 GPU 显卡是空闲可用状态，否则会打印  <code>cpu</code>  。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cuda:0</span><br></pre></td></tr></table></figure></p>\n<p>既然有可用的 GPU ，接下来就是在 GPU 上进行训练了，其中需要修改的代码如下，分别是需要将网络参数和数据都转移到 GPU 上：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.to(device)</span><br><span class=\"line\">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure></p>\n<p>修改后的训练部分代码：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import time</span><br><span class=\"line\"># 在 GPU 上训练注意需要将网络和数据放到 GPU 上</span><br><span class=\"line\">net.to(device)</span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br><span class=\"line\"></span><br><span class=\"line\">start = time.time()</span><br><span class=\"line\">for epoch in range(2):</span><br><span class=\"line\"></span><br><span class=\"line\">    running_loss = 0.0</span><br><span class=\"line\">    for i, data in enumerate(trainloader, 0):</span><br><span class=\"line\">        # 获取输入数据</span><br><span class=\"line\">        inputs, labels = data</span><br><span class=\"line\">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class=\"line\">        # 清空梯度缓存</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        outputs = net(inputs)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        # 打印统计信息</span><br><span class=\"line\">        running_loss += loss.item()</span><br><span class=\"line\">        if i % 2000 == 1999:</span><br><span class=\"line\">            # 每 2000 次迭代打印一次信息</span><br><span class=\"line\">            print(&#x27;[%d, %5d] loss: %.3f&#x27; % (epoch + 1, i+1, running_loss / 2000))</span><br><span class=\"line\">            running_loss = 0.0</span><br><span class=\"line\">print(&#x27;Finished Training! Total cost time: &#x27;, time.time() - start)</span><br></pre></td></tr></table></figure></p>\n<p>注意，这里调用  <code>net.to(device)</code>  后，需要定义下优化器，即传入的是 CUDA 张量的网络参数。训练结果和之前的类似，而且其实因为这个网络非常小，转移到 GPU 上并不会有多大的速度提升，而且我的训练结果看来反而变慢了，也可能是因为我的笔记本的 GPU 显卡问题。</p>\n<p>如果需要进一步提升速度，可以考虑采用多 GPUs，也就是下一节的内容。</p>\n<p>本小节教程：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovY2lmYXIxMF90dXRvcmlhbC5odG1s\">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</span></p>\n<p>本小节的代码：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS90cmFpbl9jbGFzc2lmaWVyX2V4YW1wbGUuaXB5bmI=\">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/train_classifier_example.ipynb</span></p>\n<h2 id=\"数据并行\"><a class=\"anchor\" href=\"#数据并行\">#</a> <strong>数据并行</strong></h2>\n<p>这部分教程将学习如何使用  <code>DataParallel</code>  来使用多个 GPUs 训练网络。</p>\n<p>首先，在 GPU 上训练模型的做法很简单，如下代码所示，定义一个  <code>device</code>  对象，然后用  <code>.to()</code>  方法将网络模型参数放到指定的 GPU 上。</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(&quot;cuda:0&quot;)</span><br><span class=\"line\">model.to(device)</span><br></pre></td></tr></table></figure></p>\n<p>接着就是将所有的张量变量放到 GPU 上：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure></p>\n<p>注意，这里  <code>my_tensor.to(device)</code>  是返回一个  <code>my_tensor</code>  的新的拷贝对象，而不是直接修改  <code>my_tensor</code>  变量，因此你需要将其赋值给一个新的张量，然后使用这个张量。</p>\n<p>Pytorch 默认只会采用一个 GPU，因此需要使用多个 GPU，需要采用  <code>DataParallel</code>  ，代码如下所示：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure></p>\n<p>这代码也就是本节教程的关键，接下来会继续详细介绍。</p>\n<h3 id=\"导入和参数\"><a class=\"anchor\" href=\"#导入和参数\">#</a> <strong>导入和参数</strong></h3>\n<p>首先导入必须的库以及定义一些参数：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">import torch.nn as nn</span><br><span class=\"line\">from torch.utils.data import Dataset, DataLoader</span><br><span class=\"line\"></span><br><span class=\"line\"># Parameters and DataLoaders</span><br><span class=\"line\">input_size = 5</span><br><span class=\"line\">output_size = 2</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = 30</span><br><span class=\"line\">data_size = 100</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>这里主要定义网络输入大小和输出大小， <code>batch</code>  以及图片的大小，并定义了一个  <code>device</code>  对象。</p>\n<h3 id=\"构建一个假数据集\"><a class=\"anchor\" href=\"#构建一个假数据集\">#</a> <strong>构建一个假数据集</strong></h3>\n<p>接着就是构建一个假的 (随机) 数据集。实现代码如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class RandomDataset(Dataset):</span><br><span class=\"line\"></span><br><span class=\"line\">    def __init__(self, size, length):</span><br><span class=\"line\">        self.len = length</span><br><span class=\"line\">        self.data = torch.randn(length, size)</span><br><span class=\"line\"></span><br><span class=\"line\">    def __getitem__(self, index):</span><br><span class=\"line\">        return self.data[index]</span><br><span class=\"line\"></span><br><span class=\"line\">    def __len__(self):</span><br><span class=\"line\">        return self.len</span><br><span class=\"line\"></span><br><span class=\"line\">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),</span><br><span class=\"line\">                         batch_size=batch_size, shuffle=True)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"简单的模型\"><a class=\"anchor\" href=\"#简单的模型\">#</a> <strong>简单的模型</strong></h3>\n<p>接下来构建一个简单的网络模型，仅仅包含一层全连接层的神经网络，加入  <code>print()</code>  函数用于监控网络输入和输出  <code>tensors</code>  的大小：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Model(nn.Module):</span><br><span class=\"line\">    # Our model</span><br><span class=\"line\"></span><br><span class=\"line\">    def __init__(self, input_size, output_size):</span><br><span class=\"line\">        super(Model, self).__init__()</span><br><span class=\"line\">        self.fc = nn.Linear(input_size, output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, input):</span><br><span class=\"line\">        output = self.fc(input)</span><br><span class=\"line\">        print(&quot;\\tIn Model: input size&quot;, input.size(),</span><br><span class=\"line\">              &quot;output size&quot;, output.size())</span><br><span class=\"line\"></span><br><span class=\"line\">        return output</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"创建模型和数据平行\"><a class=\"anchor\" href=\"#创建模型和数据平行\">#</a> <strong>创建模型和数据平行</strong></h3>\n<p>这是本节的核心部分。首先需要定义一个模型实例，并且检查是否拥有多个 GPUs，如果是就可以将模型包裹在  <code>nn.DataParallel</code>  ，并调用  <code>model.to(device)</code>  。代码如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = Model(input_size, output_size)</span><br><span class=\"line\">if torch.cuda.device_count() &gt; 1:</span><br><span class=\"line\">  print(&quot;Let&#x27;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)</span><br><span class=\"line\">  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span><br><span class=\"line\">  model = nn.DataParallel(model)</span><br><span class=\"line\"></span><br><span class=\"line\">model.to(device)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"运行模型\"><a class=\"anchor\" href=\"#运行模型\">#</a> <strong>运行模型</strong></h3>\n<p>接着就可以运行模型，看看打印的信息：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for data in rand_loader:</span><br><span class=\"line\">    input = data.to(device)</span><br><span class=\"line\">    output = model(input)</span><br><span class=\"line\">    print(&quot;Outside: input size&quot;, input.size(),</span><br><span class=\"line\">          &quot;output_size&quot;, output.size())</span><br></pre></td></tr></table></figure></p>\n<p>输出如下：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"运行结果\"><a class=\"anchor\" href=\"#运行结果\">#</a> <strong>运行结果</strong></h3>\n<p>如果仅仅只有 1 个或者没有 GPU ，那么  <code>batch=30</code>  的时候，模型会得到输入输出的大小都是 30。但如果有多个 GPUs，那么结果如下：</p>\n<h3 id=\"2-gpus\"><a class=\"anchor\" href=\"#2-gpus\">#</a> <strong>2 GPUs</strong></h3>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># on 2 GPUs</span><br><span class=\"line\">Let&#x27;s use 2 GPUs!</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class=\"line\">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-gpus\"><a class=\"anchor\" href=\"#3-gpus\">#</a> <strong>3 GPUs</strong></h3>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Let&#x27;s use 3 GPUs!</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"8-gpus\"><a class=\"anchor\" href=\"#8-gpus\">#</a> <strong>8 GPUs</strong></h3>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Let&#x27;s use 8 GPUs!</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class=\"line\">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> <strong>总结</strong></h3>\n<p><code>DataParallel</code>  会自动分割数据集并发送任务给多个 GPUs 上的多个模型。然后等待每个模型都完成各自的工作后，它又会收集并融合结果，然后返回。</p>\n<p>更详细的数据并行教程：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvZm9ybWVyX3RvcmNoaWVzL3BhcmFsbGVsaXNtX3R1dG9yaWFsLmh0bWw=\">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</span></p>\n<p>本小节教程：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovZGF0YV9wYXJhbGxlbF90dXRvcmlhbC5odG1s\">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</span></p>\n<hr />\n<h3 id=\"小结\"><a class=\"anchor\" href=\"#小结\">#</a> <strong>小结</strong></h3>\n<p>教程从最基础的张量开始介绍，然后介绍了非常重要的自动求梯度的  <code>autograd</code>  ，接着介绍如何构建一个神经网络，如何训练图像分类器，最后简单介绍使用多 GPUs 加快训练速度的方法。</p>\n<p>快速入门教程就介绍完了，接下来你可以选择：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvaW50ZXJtZWRpYXRlL3JlaW5mb3JjZW1lbnRfcV9sZWFybmluZy5odG1s\">训练一个神经网络来玩视频游戏</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvaW1hZ2VuZXQ=\">在 imagenet 上训练 ResNet</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvZGNnYW4=\">采用 GAN 训练一个人脸生成器</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvd29yZF9sYW5ndWFnZV9tb2RlbA==\">采用循环 LSTM 网络训练一个词语级别的语言模型</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXM=\">更多的例子</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHM=\">更多的教程</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9kaXNjdXNzLnB5dG9yY2gub3JnLw==\">在 Forums 社区讨论 PyTorch</span></li>\n</ul>\n<h2 id=\"项目练习手写数字识别练习mnist\"><a class=\"anchor\" href=\"#项目练习手写数字识别练习mnist\">#</a> 项目练习：手写数字识别练习 MNIST</h2>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210153152886.png\" alt=\"image-20240210153152886\" /></p>\n<p>softmax 归一化</p>\n<p>梯度下降法等调参</p>\n<p>一批次一批次的训练：一个批次一个 batch</p>\n<p>神经网络过程是线性的，需要非线性结果</p>\n<p>在每个节点上再套上一个非线性函数 f (), 又称激活函数</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msubsup><mi>x</mi><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>f</mi><mrow><mo fence=\"true\">(</mo><msubsup><mi>a</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi></mrow><mi>k</mi></msubsup><mo>⋅</mo><msubsup><mi>x</mi><mi>i</mi><mi>k</mi></msubsup><mo>+</mo><msubsup><mi>b</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi></mrow><mi>k</mi></msubsup><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_j^{k+1} = \\sum_i f\\left(a_{i,j}^k \\cdot x_i^k + b_{i,j}^k\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.3022109999999998em;vertical-align:-0.403103em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-2.4330050000000005em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.403103em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.327674em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0500050000000003em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.899108em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999998em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.899108em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">)</span></span></span></span></span></span></span></p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210153529250.png\" alt=\"image-20240210153529250\" /></p>\n<p>同时安装四个库</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install numpy torch torchvision matplotlib</span><br></pre></td></tr></table></figure></p>\n<p>手写数字识别练习</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#定义一个神经网络</span><br><span class=\"line\">class Net(torch.nn.Module):</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tdef __init__(self):</span><br><span class=\"line\">\t\t#神经网络主体，包含四个全连接层</span><br><span class=\"line\">\t\tsuper().__init__()</span><br><span class=\"line\">\t\tself.fc1 = torch.nn.Linear(28*28,64) #输入为28*28像素尺寸的图像</span><br><span class=\"line\">\t\tself.fc2 = torch.nn.Linear(64,64)</span><br><span class=\"line\">\t\tself.fc3 = torch.nn.Linear(64,64)</span><br><span class=\"line\">\t\tself.fc4 = torch.nn.Linear(64,10)</span><br><span class=\"line\">\t#中间3层都放了64个节点，输出为10个数字类别</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tdef forward(self,x): #x图像输入</span><br><span class=\"line\">\t\tx = torch.nn.functional.relu(self.fc1(x) )</span><br><span class=\"line\">\t\tx = torch.nn.functional.relu(self.fc2(x) )</span><br><span class=\"line\">\t\tx = torch.nn.functional.relu(self.fc3(x) )</span><br><span class=\"line\">\t\tx = torch.nn.functional.log_softmax(self.fc3(x),dim=1)#提高计算稳定性</span><br><span class=\"line\">\t\treturn x</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t#导入数据</span><br><span class=\"line\">\tdef get_data_loader(is_train):</span><br><span class=\"line\">\t\t#定义数据转换类型</span><br><span class=\"line\">\t\tto_tensor = transforms.Compose([transforms,ToTensor()])</span><br><span class=\"line\">\t\tdata_set = MNIST(&quot;&quot;,is_train,transform=to_tensor,download = True)</span><br><span class=\"line\">\t\treturn DataLoader(data_set,batch_size=15,shuffle=True)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tdef evaluate(test_data,net):</span><br><span class=\"line\">\t\tn_correct = 0</span><br><span class=\"line\">\t\tn_total = 0</span><br><span class=\"line\">\t\twith torch.no_grad(): \t#从测试集中按批次取出数据</span><br><span class=\"line\">\t\t\tfor(x,y) in test_data:</span><br><span class=\"line\">\t\t\t\toutputs = net.forward(x,view(-1,28*28))</span><br><span class=\"line\">\t\t\t\tfor i,output in enumerate(outputs) :</span><br><span class=\"line\">\t\t\t\t\tif torch.argmax(output) == y[i] :</span><br><span class=\"line\">\t\t\t\t\t\tn_correct += 1</span><br><span class=\"line\">\t\t\t\t\tn_total += 1</span><br><span class=\"line\">\t\treturn n_correct / n_total #返回正确率</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tdef main():</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\ttrain_data = get_data_loader(is_train=True)</span><br><span class=\"line\">\t\ttest_data = get_data_loader(is_train=False)</span><br><span class=\"line\">\t\tnet = Net()</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tprint(&quot;initial accuracy:&quot;,evaluate(test_data,net))</span><br><span class=\"line\">\t\t#训练model Pytorch固定写法</span><br><span class=\"line\">\t\toptimizer = torch.optim.Adam(net.parameters(),lr=0.001)</span><br><span class=\"line\">\t\tfor epoch in range(2):</span><br><span class=\"line\">\t\t\tfor(x,y) in train_data:</span><br><span class=\"line\">\t\t\t\tnet.zero_grad() #初始化</span><br><span class=\"line\">\t\t\t\toutput = net.forward(x,view(-1,28*28 )) #正向传播</span><br><span class=\"line\">\t\t\t\tloss = torch.nn.functional.nll_loss(output,y) #计算差值</span><br><span class=\"line\">\t\t\t\tloss.backward() #反向误差传播</span><br><span class=\"line\">\t\t\t\toptimizer.step() #优化网络参数</span><br><span class=\"line\">\t\t\t\t#</span><br><span class=\"line\">\t\t\tprint(&quot;epoch&quot;,epoch,&quot;accuracy:&quot;,evaluate(test_data,net))</span><br><span class=\"line\">\t\t#如果一切正常，训练率会越来越高</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t#训练完成后，随机选取三张图像，显示网络预测结果</span><br><span class=\"line\">        for (n, (x, _ )) in enumerate(test_data):</span><br><span class=\"line\">        \tif n &gt; 3:</span><br><span class=\"line\">        \t\tbreak</span><br><span class=\"line\">        \tpredict = torch.argmax(net.forward(x[0].view(-1,28*28)) )</span><br><span class=\"line\">        \tplt.figure(n)</span><br><span class=\"line\">        \tplt.imshow(x[0].view(28,28))</span><br><span class=\"line\">        \tplt.title(&quot;prediction: &quot;+str(int(predict) ) )</span><br><span class=\"line\">   \t\tplt.show()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tif __name__ == &quot;__main__&quot;:</span><br><span class=\"line\">\t\tmain()</span><br></pre></td></tr></table></figure></p>\n<p>注释版</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">import torch.nn as nn</span><br><span class=\"line\">import torch.nn.functional as F</span><br><span class=\"line\"></span><br><span class=\"line\">class Net(nn.Module):</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        super(Net,self).__init__()</span><br><span class=\"line\">        # 定义第一个卷积层，输入通道为1（单通道图像，如灰度图），输出通道为6，使用5*5的卷积核</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(1, 6, 5)</span><br><span class=\"line\">        # 第二个卷积层，输入通道为6（由于第一个卷积层的输出是6），输出通道为16，同样使用5*5的卷积核</span><br><span class=\"line\">        # 注意：这里的定义缺失了，应该在`__init__`方法中添加self.conv2的定义</span><br><span class=\"line\">        # 全连接层（fc1）的定义，输入特征维度为16*5*5（假设经过两次卷积和池化后的特征图大小），输出特征维度为120</span><br><span class=\"line\">        self.fc1 = nn.Linear(16*5*5, 120)</span><br><span class=\"line\">        # 第二个全连接层，输入特征维度为120，输出特征维度为84</span><br><span class=\"line\">        self.fc2 = nn.Linear(120, 84)</span><br><span class=\"line\">        # 第三个全连接层，输入特征维度为84，输出特征维度为10（假设为分类问题的类别数）</span><br><span class=\"line\">        self.fc3 = nn.Linear(84, 10)</span><br><span class=\"line\">        </span><br><span class=\"line\">    def forward(self, x):</span><br><span class=\"line\">        # 应用第一个卷积层后使用ReLU激活函数，然后进行2x2的最大池化</span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class=\"line\">        # 应用第二个卷积层，同样使用ReLU激活函数和2x2的最大池化</span><br><span class=\"line\">        # 注意：这里需要确保conv2在`__init__`方法中被定义</span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class=\"line\">        # 将多维输入张量展平成一维，准备输入到全连接层</span><br><span class=\"line\">        x = x.view(-1, self.num_flat_features(x))</span><br><span class=\"line\">        # 第一个全连接层后使用ReLU激活函数</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        # 第二个全连接层同样使用ReLU激活函数</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        # 最后一个全连接层输出最终结果，这里不使用激活函数是因为后续可能接softmax进行分类</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        return x</span><br><span class=\"line\"></span><br><span class=\"line\">    def num_flat_features(self, x):</span><br><span class=\"line\">        # 计算除batch维度外的所有维度乘积，即在全连接层之前需要展平的特征数量</span><br><span class=\"line\">        size = x.size()[1:]  # 所有维度除了batch维度</span><br><span class=\"line\">        num_features = 1</span><br><span class=\"line\">        for s in size:</span><br><span class=\"line\">            num_features *= s</span><br><span class=\"line\">        return num_features</span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br><span class=\"line\">print(net)</span><br></pre></td></tr></table></figure></p>\n<p>这段代码定义了一个简单的卷积神经网络，包含两个卷积层和三个全连接层。它演示了在 PyTorch 中如何构建网络、应用卷积、激活函数、池化以及全连接层。注意，代码中确实漏掉了 <code>self.conv2</code>  的定义，这是必须添加的部分以确保网络能够正常工作。</p>\n<p><code>forward</code>  前向传输<br />\n全连接线性运算 self.fc1 (x) 再套上激活函数 x 为图像输入</p>\n<p>第一个参数表示下载目录，&quot;&quot; 空表示当前目录</p>\n<p><code>is_train</code>  用于指定导入训练集还是测试集</p>\n<p><code>batch_size=15</code>  表示一个批次包含 15 张图片</p>\n<p><code>shuffle=True</code>  表示打乱顺序</p>\n<p>返回数据加载器 <code>DataLoader</code></p>\n<p><code>evaluate</code>  函数用来评估神经网络的识别正确率</p>\n<p>从测试集中按批次取出数据，计算神经网路的预测值</p>\n<p>再对批次中的每个结果进行比较，累加正确预测的数量</p>\n<p>nll_loss 对数损失函数</p>\n<p>是 log_softmax 中的对数运算</p>\n<p>epoch 训练伦次，提高数据利用率</p>\n",
            "tags": [
                "PyTorch学习",
                "PyTorch学习"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/",
            "url": "https://jinjiaojiao.top/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/",
            "title": "PyTorch学习第一章——简介与安装",
            "date_published": "2024-02-10T06:15:33.000Z",
            "content_html": "<p>学习参考项目：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL2ZlbmRvdWFpL1B5VG9yY2hEb2NzL3RyZWUvbWFzdGVy\">https://github.com/fendouai/PyTorchDocs/tree/master</span></p>\n<p>处于学习阶段，内容几乎全部上述文档，仅进行归纳整理与加上自己的部分理解，特此声明。</p>\n<h1 id=\"第一章-pytorch之简介与下载\"><a class=\"anchor\" href=\"#第一章-pytorch之简介与下载\">#</a> 第一章 PyTorch 之简介与下载</h1>\n<h2 id=\"pytorch简介\"><a class=\"anchor\" href=\"#pytorch简介\">#</a> Pytorch 简介</h2>\n<p>要介绍 <code>PyTorch</code>  之前，不得不说一下 <code>Torch</code> 。 <code>Torch</code>  是一个有大量机器学习算法支持的科学计算框架，是一个与 <code>Numpy</code>  类似的张量（ <code>Tensor</code> ）<br />\n操作库，其特点是特别灵活，但因其采用了小众的编程语言是 <code>Lua</code> ，所以流行度不高，这也就有了 <code>PyTorch</code>  的出现。所以其实 <code>Torch</code>  是 <code>PyTorch</code>  的前身，它们的底层语言相同，只是使用了不同的 <code>上层包装语言</code></p>\n<p><code>PyTorch</code>  是一个基于 Torch 的 Python<strong> 开源机器学习库</strong>，用于自然语言处理等应用程序。</p>\n<p>它主要由 <code>Facebook</code>  的人工智能小组开发，不仅能够实现强大的 <code>GPU</code>  加速，同时还支持动态神经网络，这一点是现在很多主流框架如 <code>TensorFlow</code>  都不支持的。</p>\n<p><strong> <code>PyTorch</code>  提供了两个高级功能：</strong></p>\n<ul>\n<li>\n<p>具有强大的 <code>GPU</code>  加速的张量计算（如 <code>Numpy</code> ）</p>\n</li>\n<li>\n<p>包含自动求导系统的深度神经网络</p>\n</li>\n</ul>\n<p><code>TensorFlow</code>  和 <code>Caffe</code>  都是<strong>命令式</strong>的编程语言，而且是<strong>静态</strong>的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改变网络的结构，就必须<strong>从头开始</strong>。</p>\n<p>但是对于 <code>PyTorch</code> ，通过<strong>反向求导</strong>技术，可以让你零延迟地任意改变神经网络的行为，而且其实现速度快。正是这一灵活性是 <code>PyTorch</code>  对比 <code>TensorFlow</code>  的最大优势。</p>\n<p>所以，总结一下 PyTorch 的优点：</p>\n<ul>\n<li>支持 GPU</li>\n<li>灵活，支持动态神经网络</li>\n<li>底层代码易于理解</li>\n<li>命令式体验</li>\n<li>自定义扩展</li>\n<li>少量代码就能完成机器学习任务</li>\n</ul>\n<p>缺点：对比 <code>TensorFlow</code> ，</p>\n<ul>\n<li>其全面性处于劣势，目前 PyTorch 还不支持快速傅里叶、沿维翻转张量和检查无穷与非数值张量；</li>\n<li>针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升；</li>\n<li>框架较新，社区没有那么强大，C 库大多数没有文档。</li>\n</ul>\n<h2 id=\"pytorch环境搭建\"><a class=\"anchor\" href=\"#pytorch环境搭建\">#</a>  <code>Pytorch</code>  环境搭建</h2>\n<h3 id=\"1-安装anaconda-35\"><a class=\"anchor\" href=\"#1-安装anaconda-35\">#</a> 1. 安装 Anaconda 3.5</h3>\n<p>Anaconda 是一个用于科学计算的 Python 发行版，支持 Linux、Mac 和 Window 系统，提供了包管理与环境管理的功能，可以很方便地解决 Python 并存、切换，以及各种第三方包安装的问题。</p>\n<h4 id=\"下载\"><a class=\"anchor\" href=\"#下载\">#</a> 下载：</h4>\n<p>可以直接从 <span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY29udGludXVtLmlvL2Rvd25sb2Fkcw==\">Anaconda 官网</span>下载，但因为 Anaconda 的服务器在国外，所以下载速度会很慢，这里推荐使用<span class=\"exturl\" data-url=\"aHR0cHM6Ly9taXJyb3JzLnR1bmEudHNpbmdodWEuZWR1LmNuL2FuYWNvbmRhL2FyY2hpdmUv\">清华的镜像</span>来下载。选择合适你的版本下载，我这里选择<span class=\"exturl\" data-url=\"aHR0cHM6Ly9taXJyb3JzLnR1bmEudHNpbmdodWEuZWR1LmNuL2FuYWNvbmRhL2FyY2hpdmUvQW5hY29uZGEzLTUuMS4wLVdpbmRvd3MteDg2XzY0LmV4ZQ==\"> Anaconda3-5.1.0-Windows-x86_64.exe</span></p>\n<h4 id=\"安装\"><a class=\"anchor\" href=\"#安装\">#</a> 安装：</h4>\n<p>安装完成后，进行 Anaconda 的环境变量配置，打开控制面板 -&gt; 高级系统设置 -&gt; 环境变量 -&gt; 系统变量找到 Path，点击编辑，加入三个文件夹的存储路径（注意三个路径之间需用分号隔开）</p>\n<h3 id=\"2安装pytorch-torchvision\"><a class=\"anchor\" href=\"#2安装pytorch-torchvision\">#</a> 2. 安装 PyTorch &amp; torchvision</h3>\n<h4 id=\"命令获取\"><a class=\"anchor\" href=\"#命令获取\">#</a> 命令获取</h4>\n<p>进入 <span class=\"exturl\" data-url=\"aHR0cHM6Ly9weXRvcmNoLm9yZy8=\">PyTorch 官网</span>，依次选择你电脑的配置（我这里已经下载了 python3.7），这里提供使用 pip 和 conda 两种环境下安装的步骤截图</p>\n<p>(1) 使用 pip：windows+pip+python3.7+None</p>\n<p>(2) 使用 conda：windows+conda+python3.7+None</p>\n<h5 id=\"建议参照官网这里没有拷贝下来\"><a class=\"anchor\" href=\"#建议参照官网这里没有拷贝下来\">#</a> （建议参照官网）这里没有拷贝下来</h5>\n",
            "tags": [
                "PyTorch学习",
                "PyTorch学习"
            ]
        },
        {
            "id": "https://jinjiaojiao.top/2024/02/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/",
            "url": "https://jinjiaojiao.top/2024/02/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/",
            "title": "复盘in2vec",
            "date_published": "2024-02-07T05:26:07.000Z",
            "content_html": "<p>2024-2-7</p>\n<p>复盘：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0NWTEFCLVVuaWJvL2lucjJ2ZWM/dGFiPXJlYWRtZS1vdi1maWxl\">https://github.com/CVLAB-Unibo/inr2vec?tab=readme-ov-file</span></p>\n<h2 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h2>\n<p>用的阿里云服务器</p>\n<p>vscode-&gt; 先下载扩展 remote-ssh</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#ssh username@公有ip</span><br><span class=\"line\"></span><br><span class=\"line\">username 经常是root</span><br><span class=\"line\"></span><br><span class=\"line\">.config 选一个本地的 我就是第一个C:/29785/.config 差不多这个</span><br><span class=\"line\"></span><br><span class=\"line\">Linux</span><br><span class=\"line\"></span><br><span class=\"line\">password:输入你自己的</span><br></pre></td></tr></table></figure></p>\n<p>进入终端</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir cs</span><br><span class=\"line\">cd cs</span><br><span class=\"line\">mkdir in2vec</span><br></pre></td></tr></table></figure></p>\n<p>发现没 conda</p>\n<h2 id=\"服务器安装anaconda\"><a class=\"anchor\" href=\"#服务器安装anaconda\">#</a> 服务器安装 anaconda</h2>\n<h4 id=\"下载anaconda\"><a class=\"anchor\" href=\"#下载anaconda\">#</a> 下载 anaconda</h4>\n<p>后面的链接可以去官网查找自己适合的版本</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.anaconda.com/archive/Anaconda3-5.3.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></p>\n<p>.sh 为 Linux 后缀</p>\n<p>——————————————————</p>\n<p>如果出现</p>\n<blockquote>\n<p>bash: wget: command not found</p>\n</blockquote>\n<p>具体解决办法如下:<br />\nDebian/Ubuntu 系统，需要执行以下命令：</p>\n<blockquote>\n<p>apt-get install -y wget</p>\n</blockquote>\n<p>CentOS 系统则需要输入下面指令:</p>\n<blockquote>\n<p>yum install wget -y</p>\n</blockquote>\n<p>————————————————————</p>\n<h4 id=\"赋权anaconda\"><a class=\"anchor\" href=\"#赋权anaconda\">#</a> 赋权 anaconda</h4>\n<p>接下来我们需要首先赋权再执行安装程序，依次输入下面两句命令:</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chmod +x Anaconda3-5.3.0-Linux-x86_64.sh</span><br><span class=\"line\"></span><br><span class=\"line\">./Anaconda3-5.3.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<p>然后出现下面图所示:</p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/OKcNDvCVY75lrqe.png\" alt=\"image.png\" /></p>\n<h4 id=\"一直点击enter回车键\"><a class=\"anchor\" href=\"#一直点击enter回车键\">#</a> 一直点击 Enter（回车键）</h4>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/mIkfztbra6cKnYT.png\" alt=\"image.png\" /></p>\n<p>此时显示 Anaconda 的信息，并且会出现 More，继续按 Enter，直到如下图所示:</p>\n<h4 id=\"输入-yes\"><a class=\"anchor\" href=\"#输入-yes\">#</a> 输入 yes</h4>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/bEM3dxyNGqgoz4K.png\" alt=\"image.png\" /></p>\n<h4 id=\"继续点击-enter\"><a class=\"anchor\" href=\"#继续点击-enter\">#</a> 继续点击 Enter</h4>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/zZvmdlD3Gcahxqu.png\" alt=\"image.png\" /></p>\n<h4 id=\"输入-yes添加环境变量\"><a class=\"anchor\" href=\"#输入-yes添加环境变量\">#</a> 输入 yes，添加环境变量</h4>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/yBq74es8zYAwDiC.png\" alt=\"image.png\" /></p>\n<p>^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>这里需要注意点的就是如果你直接跳过这部设置环境变量的话：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[no ] &gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>\n<p>那你需要自己到这个文件夹设置你安装 Anaconda 路径</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find -name &quot;anaconda3&quot;</span><br></pre></td></tr></table></figure></p>\n<p>（比如上面显示我的是）</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./anaconda3</span><br></pre></td></tr></table></figure></p>\n<p>单击进去，在最后一行添加：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=./anaconda3/bin:$PATH</span><br></pre></td></tr></table></figure></p>\n<p>这里只是个示例，具体的还是要看你们自己安装的路径。</p>\n<p>然后保存更改，输入下面这句指令：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>—————————————————————</p>\n<h4 id=\"完成安装以及检测是否安装成功\"><a class=\"anchor\" href=\"#完成安装以及检测是否安装成功\">#</a> 完成安装以及检测是否安装成功</h4>\n<p>打开新的终端后，进入自己的文件夹目录下，输入 anaconda -V（注意 a 要小写，V 要大写），conda -V ,</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda -V</span><br><span class=\"line\">或者</span><br><span class=\"line\">conda --version</span><br></pre></td></tr></table></figure></p>\n<p>显示版本信息，若显示则表示安装成功。</p>\n<p><code>conda 4.5.11</code></p>\n<h2 id=\"anaconda安装pytorch\"><a class=\"anchor\" href=\"#anaconda安装pytorch\">#</a> Anaconda 安装<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zby5jc2RuLm5ldC9zby9zZWFyY2g/cT1QeXRvcmNoJmFtcDtzcG09MTAwMS4yMTAxLjMwMDEuNzAyMA==\"> Pytorch</span></h2>\n<h3 id=\"创建虚拟环境\"><a class=\"anchor\" href=\"#创建虚拟环境\">#</a> 创建虚拟环境</h3>\n<p>第一个程序环境名为 pytorch</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n pytorch python=3.7 （pytorch 是我自己取的名字）</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"激活环境\"><a class=\"anchor\" href=\"#激活环境\">#</a> 激活环境</h3>\n<p>使用下面这条命令，激活环境：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate pytorch</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"可能出现的问题\"><a class=\"anchor\" href=\"#可能出现的问题\">#</a> 可能出现的问题</h3>\n<p>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>出现下面所示 1：</p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/yBq74es8zYAwDiC.png\" alt=\"image.png\" /></p>\n<p>问题出现是因为尽管安装了  <code>Anaconda</code>  或  <code>Miniconda</code> ，</p>\n<p>但是你的 shell 环境没有被正确配置以识别  <code>conda</code>  命令。</p>\n<p><strong>解决办法：</strong></p>\n<ol>\n<li>\n<p><strong>启用  <code>conda</code>  命令</strong>:</p>\n<ul>\n<li>\n<p>如果你使用的是 Bash shell 或者类似 Bourne 的 shell，你需要将  <code>conda</code>  的初始化脚本添加到你的 shell 配置文件中（通常是</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>文件），以便  <code>conda</code>  命令可以被识别。根据提供的信息，你可以通过运行以下命令来做到这一点：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;. /root/anaconda3/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>这会将初始化脚本的路径添加到你的</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>文件中，每次启动新的 shell 会话时都会自动执行这个脚本。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>激活  <code>conda</code>  的基础（root）环境</strong>:</p>\n<ul>\n<li>\n<p>为了使用 <code>conda</code>  管理你的环境和包，你需要激活  <code>conda</code>  的基础环境。你可以通过在终端中运行以下命令来做到这一点：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>如果你想在每次打开新的终端会话时自动激活  <code>conda</code>  的基础环境，你可以将上述命令添加到你的</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>文件中：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;conda activate&quot; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure></p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>移除旧的 PATH 设置</strong>:</p>\n<ul>\n<li>\n<p>如果你之前尝试通过直接修改  <code>PATH</code>  环境变量来使用  <code>conda</code> ，你应该从你的</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~/.bashrc</span><br></pre></td></tr></table></figure></p>\n<p>文件中移除这样的行：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PATH=&quot;/root/anaconda3/bin:$PATH&quot;</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p>这是因为直接修改  <code>PATH</code>  不再是推荐的方法，使用  <code>conda</code>  的初始化脚本和  <code>conda activate</code>  命令是更好的选择。</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>完成以上步骤后，关闭并重新打开你的终端，或者运行  <code>source ~/.bashrc</code>  来使改动生效。这样，你的 shell 环境就被正确配置了，应该能够识别  <code>conda</code>  命令了。</p>\n<p><strong>出现下面所示 2:</strong></p>\n<p><code>(pytorch) root@dev-wyf-react:~/wyf#</code></p>\n<p>检测环境是否安装好:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda info --envs</span><br></pre></td></tr></table></figure></p>\n<p>出现下面所示：</p>\n<p><code>base /root/anaconda3</code> <br />\n <code>pytorch * /root/anaconda3/envs/pytorch</code></p>\n<p>然后去选择适合自己的 pytorch 版本，点击下面那个链接:</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9weXRvcmNoLm9yZy8=\">https://pytorch.org/</span></p>\n<p>get started</p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240207012929597.png\" alt=\"image-20240207012929597\" /></p>\n<p>输入到控制台:</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure></p>\n<p>弹出提示，输入 y，即可完成安装，显示 “done”。</p>\n<p>——————————————————————————————</p>\n<h3 id=\"测试安装成功\"><a class=\"anchor\" href=\"#测试安装成功\">#</a> 测试安装成功</h3>\n<p>首先输入：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br></pre></td></tr></table></figure></p>\n<p>然后在输入：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/M9jTXGulqSwhE8d.png\" alt=\"image.png\" /></p>\n<p>如果没有可以<strong>重新下载康康</strong>或者<strong>查看环境</strong></p>\n<h3 id=\"退出之后如何查看自己安装的环境\"><a class=\"anchor\" href=\"#退出之后如何查看自己安装的环境\">#</a> 退出之后如何查看自己安装的环境</h3>\n<p>如果在一台服务器上安装多个环境，一下子可能不记得需要激活哪个环境名称，这时候我们需要使用下面这个命令来查找：</p>\n<blockquote>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda info --envs</span><br></pre></td></tr></table></figure></p>\n</blockquote>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/hkcRHwlpSIx2UJ1.png\" alt=\"image-20240207012544351\" /></p>\n<h2 id=\"复盘项目\"><a class=\"anchor\" href=\"#复盘项目\">#</a> 复盘项目</h2>\n<p>打开</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0NWTEFCLVVuaWJvL2lucjJ2ZWM/dGFiPXJlYWRtZS1vdi1maWxl\">https://github.com/CVLAB-Unibo/inr2vec?tab=readme-ov-file</span></p>\n<h3 id=\"配置git\"><a class=\"anchor\" href=\"#配置git\">#</a> 配置 git</h3>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git --version</span><br></pre></td></tr></table></figure></p>\n<p>如果已经安装过了就可以直接跳到 3.2 了</p>\n<p>也不知道阿里云服务器什么发行版，输入如下命令查看</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /etc/*-release</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/WQzUZjoMIs6tX4L.png\" alt=\"image.png\" /></p>\n<p>根据信息，Alibaba Cloud Linux 3 是一个与 RHEL（Red Hat Enterprise Linux）、Fedora 和 CentOS 兼容的发行版。</p>\n<p>使用与这些发行版相同的包管理器，大多数情况下将是  <code>yum</code>  或者在某些新版本中是  <code>dnf</code> 。</p>\n<p>以下命令来安装 <code>git</code> ：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum update</span><br><span class=\"line\">sudo yum install git</span><br></pre></td></tr></table></figure></p>\n<p>或者，如果系统支持  <code>dnf</code> ：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo dnf update</span><br><span class=\"line\">sudo dnf install git</span><br></pre></td></tr></table></figure></p>\n<p>执行更新（ <code>yum update</code>  或  <code>dnf update</code> ）是一个好习惯，它会确保所有的软件包都是最新的。然后，使用安装命令来安装 <code>git</code> 。</p>\n<h3 id=\"git-clone-代码\"><a class=\"anchor\" href=\"#git-clone-代码\">#</a> git clone 代码</h3>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/CVLAB-Unibo/inr2vec.git</span><br></pre></td></tr></table></figure></p>\n<p>等一会儿就下好啦</p>\n<h3 id=\"创建对应的环境\"><a class=\"anchor\" href=\"#创建对应的环境\">#</a> 创建对应的环境</h3>\n<p>！！！！以下内容仅作知识补充</p>\n<h4 id=\"创建虚拟环境-2\"><a class=\"anchor\" href=\"#创建虚拟环境-2\">#</a> <strong>创建虚拟环境</strong>：</h4>\n<p>​\tAnaconda 创建环境：<br />\n​\t比如，创建 pyhon＝3.7 的版本环境取名叫 <code>inr2vec</code></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n inr2vec python=3.7</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"删除虚拟环境操作谨慎操作\"><a class=\"anchor\" href=\"#删除虚拟环境操作谨慎操作\">#</a> <strong>删除虚拟环境操作</strong>：（谨慎操作）</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda remove -n inr2vec --all</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"激活环境-2\"><a class=\"anchor\" href=\"#激活环境-2\">#</a> <strong>激活环境</strong></h4>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate inr2vec</span><br></pre></td></tr></table></figure></p>\n<p>如果发现进不去，那么</p>\n<p>先:  <code>source activate inr2vec</code></p>\n<p>再:  <code>conda activate inr2vec</code></p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看python版本</span></span><br><span class=\"line\">python --version</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"查看环境下已有的安装包\"><a class=\"anchor\" href=\"#查看环境下已有的安装包\">#</a> 查看环境下已有的安装包：</h4>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda <span class=\"built_in\">list</span></span><br></pre></td></tr></table></figure></p>\n<p>效果如下</p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/y6pf4seiK3kMhlY.png\" alt=\"image.png\" /></p>\n<p>在进入虚拟环境的情况下， <code>安装对应包</code> 直接</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install xxxx 或者 conda install xxxx</span><br></pre></td></tr></table></figure></p>\n<p>如：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install tensorflow</span><br></pre></td></tr></table></figure></p>\n<p><code>注意</code> ：此环境下的安装包在 <code>退出虚拟环境后无法使用</code> 的</p>\n<h4 id=\"退出当前虚拟环境\"><a class=\"anchor\" href=\"#退出当前虚拟环境\">#</a> 退出当前虚拟环境：</h4>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda deactivate</span><br></pre></td></tr></table></figure></p>\n<p>补充：Linux 下查看已有虚拟环境：</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda-env <span class=\"built_in\">list</span></span><br></pre></td></tr></table></figure></p>\n<p>提醒：有时候遇到过几次异常，所以开启完虚拟环境后最好使用命令</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">which python</span><br></pre></td></tr></table></figure></p>\n<p>判断编译器位置最为稳妥（inr2vec 是虚拟环境名），有一个 <code>home/anaconda/envs/nlp/bin/python</code></p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/6YRAH5giOa2KkId.png\" alt=\"image.png\" /></p>\n<p>如果发现没有在  <code>anaconda/envs</code>  的虚拟环境 (inr2vec) 下，则多次使用</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda deactivate</span><br></pre></td></tr></table></figure></p>\n<p>先退出当前环境，然后再重新使用  <code>source activate xxxx</code>  进入环境</p>\n<h4 id=\"重命名环境\"><a class=\"anchor\" href=\"#重命名环境\">#</a> 重命名环境</h4>\n<p>conda 其实没有重命名指令，实现重命名是通过 clone 完成的，分两步：<br />\n①先 clone 一份 new name 的环境<br />\n②删除 old name 的环境</p>\n<p>如，将 <code>inr2vec</code>  重命名成 <code>tf2</code></p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n tf2 --clone inr2vec</span><br></pre></td></tr></table></figure></p>\n<p>删除原环境</p>\n<p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda remove -n inr2vec --<span class=\"built_in\">all</span></span><br></pre></td></tr></table></figure></p>\n<p>————————————</p>\n<h3 id=\"根据readme-继续操作\"><a class=\"anchor\" href=\"#根据readme-继续操作\">#</a> 根据 readme 继续操作</h3>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240207095527514.png\" alt=\"image-20240207095527514\" /></p>\n<p><img data-src=\"C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240207095541074.png\" alt=\"image-20240207095541074\" /></p>\n<h3 id=\"跟着操作细节\"><a class=\"anchor\" href=\"#跟着操作细节\">#</a> 跟着操作细节</h3>\n<p>Create a virtual environment and install the library  <code>pycarus</code> :</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m venv .venv</span><br><span class=\"line\">source .venv/bin/activate</span><br><span class=\"line\">pip install -U pip setuptools</span><br><span class=\"line\">pip install pycarus</span><br></pre></td></tr></table></figure></p>\n<p>前两个没问题 第三个开始发现太慢了</p>\n<p>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>于是决定来个镜像</p>\n<h4 id=\"为本地conda环境配置国内镜像源\"><a class=\"anchor\" href=\"#为本地conda环境配置国内镜像源\">#</a> 为本地 conda 环境配置国内镜像源</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels https://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"为服务器的conda环境配置国内镜像源\"><a class=\"anchor\" href=\"#为服务器的conda环境配置国内镜像源\">#</a> 为服务器的 conda 环境配置国内镜像源</h4>\n<p>添加清华镜像源</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/win-64/</span><br></pre></td></tr></table></figure></p>\n<p><em>【注】使用 http 不是 https，在后面加上 win-64</em></p>\n<h4 id=\"常用操作\"><a class=\"anchor\" href=\"#常用操作\">#</a> 常用操作</h4>\n<p>显示添加的源通道</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --show-sources</span><br></pre></td></tr></table></figure></p>\n<p>移除某一镜像源</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda config --remove channels 源名称或链接 </span><br></pre></td></tr></table></figure></p>\n<p>验证安装</p>\n<p>为了确保正确安装了 PyTorch，我们可以通过运行示例 PyTorch 代码来验证安装。在这里，我们将构造一个随机初始张量</p>\n<p>在 anaconda prompt (miniconda3) 命令行或者 shell 中，输入：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python </span><br></pre></td></tr></table></figure></p>\n<p>然后输入以下代码：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch </span><br><span class=\"line\">x = torch.rand(5, 3) </span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure></p>\n<p>输出应类似于以下内容：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[0.3380, 0.3845, 0.3217],</span><br><span class=\"line\">      [0.8337, 0.9050, 0.2650],</span><br><span class=\"line\">      [0.2979, 0.7141, 0.9069],</span><br><span class=\"line\">      [0.1449, 0.1132, 0.1375],</span><br><span class=\"line\">      [0.4675, 0.3947, 0.1426]])</span><br></pre></td></tr></table></figure></p>\n<p>此外，如果要检查 PyTorch 是否启用了 GPU 和 CUDA，请运行以下命令以返回是否启用了 CUDA 驱动程序：</p>\n<p><figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch  </span><br><span class=\"line\">torch.cuda.is_available()</span><br></pre></td></tr></table></figure></p>\n<p>如果返回了 True， 恭喜您，成功安装了 GPU 版本。</p>\n<p>——————————————————————————————</p>\n<p>Then, try to import  <code>pycarus</code>  to get the command that you can run to install all the needed Pytorch libraries:</p>\n<p><strong>常用的检查有没有 $ pip install -U pip setuptools 装好包的步骤：</strong></p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ python3</span><br><span class=\"line\">&gt;&gt;&gt; import pycarus</span><br><span class=\"line\">...</span><br><span class=\"line\">ModuleNotFoundError: PyTorch is not installed. Install it by running: source /XXX/.venv/lib/python3.8/site-packages/pycarus/install_torch.sh</span><br></pre></td></tr></table></figure></p>\n<p>exit（）退出 python 环境</p>\n<p>不记得有没有安装好</p>\n<p><code>$ pip install -U pip setuptools</code> <br />\n <code>$ pip install pycarus</code></p>\n<p>重新执行啦</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -U pip setuptools</span><br></pre></td></tr></table></figure></p>\n<p>————————————————————————</p>\n<p>出现这种情况</p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/d7BQk6IHzu8DwAP.png\" alt=\"image.png\" /></p>\n<p>成功安装</p>\n<p>waring 是因为</p>\n<blockquote>\n<p>在 Linux 中， <code>root</code>  是具有最高权限的用户账户，可以访问和修改系统的任何部分。以 <code>root</code>  用户身份运行 <code>pip</code>  可能会导致权限问题，尤其是当系统的包管理器试图管理相同的 Python 包时。这可能会导致系统中的包版本冲突或损坏。</p>\n</blockquote>\n<p>解决方法（无虚拟环境条件）</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m venv myenv        # 创建一个名为myenv的虚拟环境</span><br><span class=\"line\">source myenv/bin/activate    # 激活虚拟环境</span><br><span class=\"line\">pip install package_name     # 在虚拟环境中安装包</span><br></pre></td></tr></table></figure></p>\n<p>——————————————————</p>\n<h3 id=\"尾声磁盘空间不足\"><a class=\"anchor\" href=\"#尾声磁盘空间不足\">#</a> 尾声：磁盘空间不足</h3>\n<p>安装包后</p>\n<p><code>ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device</code></p>\n<p>磁盘空间不足的错误（ <code>No space left on device</code> ）</p>\n<p><strong>删除不必要的文件或数据</strong>：首先，检查您的磁盘空间使用情况，查找并删除不再需要的大文件或目录。可以使用如 <code>du</code>  和 <code>df</code>  命令来帮助您找出使用最多磁盘空间的目录。</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df -h          # 查看每个挂载点的磁盘使用情况</span><br><span class=\"line\">du -sh /*      # 查看根目录下每个文件夹的大小</span><br></pre></td></tr></table></figure></p>\n<p>如果发现特定目录特别大，可以深入查看该目录下的内容：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">du -sh /path/to/directory/*   # 替换路径以检查特定目录</span><br></pre></td></tr></table></figure></p>\n<p>当找到不需要的大文件或目录时，使用 <code>rm</code>  命令删除它们：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /path/to/directory/large_file   # 替换路径和文件名以删除大文件</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/8IZJhUogzwb4jA5.png\" alt=\"image.png\" /></p>\n<ol>\n<li>\n<p><strong>清理缓存</strong>：某些系统和应用程序会创建缓存文件，这些文件可以删除以释放空间。例如，您可以清理包管理器的缓存：</p>\n<ul>\n<li>\n<p>如果您的系统基于 RPM（如 Alibaba Cloud Linux），您可以使用以下命令清理缓存：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum clean all  # 或者 sudo dnf clean all</span><br></pre></td></tr></table></figure></p>\n<p>这将删除缓存的软件包和软件仓库元数据。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>删除旧的或无用的虚拟环境</strong>：如果您创建了虚拟环境并且不再需要它们，可以直接删除包含虚拟环境的目录。假设您的虚拟环境名为 <code>myenv</code> ，可以使用以下命令：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf myenv</span><br></pre></td></tr></table></figure></p>\n<p>请确保只删除不再需要的环境，因为这个操作是不可逆的。</p>\n</li>\n<li>\n<p><strong>卸载不需要的软件包</strong>：如果安装了不再需要的软件包，可以将它们卸载以释放空间。</p>\n<p>查找并卸载不再需要的软件包：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum remove package_name   # 替换package_name为您想要卸载的包名</span><br></pre></td></tr></table></figure></p>\n</li>\n<li>\n<p><strong>检查并清理日志文件</strong>：系统日志文件有时会变得很大。检查 <code>/var/log</code>  目录并删除旧的或不必要的日志文件。对于日志文件的清理，建议谨慎行事，因为某些日志文件对于系统运维和故障排查很重要。</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo du -sh /var/log/*     # 查看日志文件的大小</span><br><span class=\"line\">sudo rm /var/log/old_log_file   # 删除旧的或大的日志文件，替换old_log_file为实际文件名</span><br></pre></td></tr></table></figure></p>\n<p><strong>注意</strong>：在删除任何日志文件之前，请确保这些文件不是正在使用的或对系统运行至关重要的。</p>\n</li>\n<li>\n<p><strong>检查用户文件</strong>：检查您的家目录下的文件，并移除不再需要的内容：</p>\n</li>\n</ol>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~                    # 切换到家目录</span><br><span class=\"line\">du -sh *               # 查看家目录下文件和目录的大小</span><br><span class=\"line\">rm -rf unnecessary_folder_or_file   # 删除不需要的文件或目录</span><br></pre></td></tr></table></figure></p>\n<p><strong>警告</strong>： <code>rm -rf</code>  命令非常强大，它会在没有任何提示的情况下删除文件和目录。在使用之前，请确保您正在删除正确的文件或目录。如果不确定，最好备份或者单独删除文件而非整个目录。</p>\n<p>但要请格外小心，不要删除不了解的系统文件，这可能会导致系统不稳定或不可用。如果您不确定某个文件或目录的用途，请在删除前进行查询或备份。</p>\n<h4 id=\"最后查看服务器情况\"><a class=\"anchor\" href=\"#最后查看服务器情况\">#</a> 最后查看服务器情况</h4>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">free -m</span><br></pre></td></tr></table></figure></p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/wZbEkGv5XIfuOep.png\" alt=\"image.png\" /></p>\n<blockquote>\n<p>Mem 行（单位均为 M）：<br />\n* total：内存总数<br />\n * used：已使用内存数<br />\n * free：空闲内存数<br />\n * shared：当前废弃不用<br />\n * buffers：缓存内存数（Buffer）<br />\n* cached：缓存内舒数（Page）</p>\n<p>(-/+ buffers/cache) 行：</p>\n<ul>\n<li>（-buffers/cache）: 真正使用的内存数，指的是第一部分的 used - buffers - cached</li>\n<li>（+buffers/cache）: 可用的内存数，指的是第一部分的 free + buffers + cached</li>\n</ul>\n<p>Swap 行指交换分区。</p>\n</blockquote>\n<p>实际上不要看 free 少就觉得内存不足了，buffers 和 cached 都是可以在使用内存时拿来用的，应该以 (-/+ buffers/cache) 行的 free 和 used 来看。只要没发现 swap 的使用，就不用太担心，如果 swap 用了很多，那就要考虑增加物理内存了。</p>\n<h4 id=\"查看cpu使用情况\"><a class=\"anchor\" href=\"#查看cpu使用情况\">#</a> 查看 CPU 使用情况</h4>\n<p>使用命令：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">top</span><br></pre></td></tr></table></figure></p>\n<p>大致结果类似下图</p>\n<p>上方文字部分的红框为总的 CPU 占用百分率，下方的表格是每个进程的 CPU 占用率，在表格第一行可以看到红框中占用率超过了 150%，这是因为服务器是多核 CPU，而该进程使用了多核。</p>\n<h4 id=\"查看显卡使用情况\"><a class=\"anchor\" href=\"#查看显卡使用情况\">#</a> 查看显卡使用情况</h4>\n<p>使用命令：</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nvidia-smi</span><br></pre></td></tr></table></figure></p>\n<p>大致结果类似下图：</p>\n<p><img data-src=\"https://s2.loli.net/2024/02/07/kBApnN1zP2gU4M5.png\" alt=\"image.png\" /></p>\n<p>表格中会显示显卡的一些信息，第一行是版本信息，第二行是标题栏，第三行就是具体的显卡信息了，如果有多个显卡，会有多行，每一行的信息值对应标题栏对应位置的信息。</p>\n<p><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* GPU：编号</span><br><span class=\"line\">* Fan：风扇转速，在0到100%之间变动，这里是42%</span><br><span class=\"line\">* Name：显卡名，这里是TITAN X</span><br><span class=\"line\">* Temp：显卡温度，这里是69摄氏度</span><br><span class=\"line\">* Perf：性能状态，从P0到P12，P0性能最大，P12最小</span><br><span class=\"line\">* Persistence-M：持续模式的状态开关，该模式耗能大，但是启动新GPU应用时比较快，这里是off</span><br><span class=\"line\">* Pwr：能耗</span><br><span class=\"line\">* Bus-Id：涉及GPU总线的东西</span><br><span class=\"line\">* Disp.A：表示GPU的显示是否初始化</span><br><span class=\"line\">* Memory-Usage：现存使用率，这里已经快满了</span><br><span class=\"line\">* GPU-Util：GPU利用率</span><br><span class=\"line\">* Compute M.：计算模式</span><br></pre></td></tr></table></figure></p>\n<p>需要注意的一点是显存占用率和 GPU 占用率是两个不一样的东西，类似于内存和 CPU，两个指标的占用率不一定是互相对应的。</p>\n<p>在下面就是每个进程使用的 GPU 情况了。</p>\n<p>———————————————</p>\n<p>由于项目复盘起来直接撑爆内存条，所以下一篇学习一些 clip 和 ulip</p>\n<p>参考文档</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZjIwMTcvYXJ0aWNsZS9kZXRhaWxzLzExODY3Njc2NQ==\">https://blog.csdn.net/wyf2017/article/details/118676765</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYnJpdGhUb1NwcmluZy9wLzEzNDk0OTY2Lmh0bWw=\">https://www.cnblogs.com/brithToSpring/p/13494966.html</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80MzQzNTY5NDc=\">https://zhuanlan.zhihu.com/p/434356947</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Nsb3Vkb3hfL2FydGljbGUvZGV0YWlscy83ODY1MTYzNw==\">https://blog.csdn.net/Cloudox_/article/details/78651637</span></p>\n",
            "tags": [
                "AI复盘"
            ]
        }
    ]
}