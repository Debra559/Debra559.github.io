<!-- build time:Thu Feb 29 2024 14:46:22 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="金娇娇" href="https://jinjiaojiao.top/rss.xml"><link rel="alternate" type="application/atom+xml" title="金娇娇" href="https://jinjiaojiao.top/atom.xml"><link rel="alternate" type="application/json" title="金娇娇" href="https://jinjiaojiao.top/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="PyTorch学习"><link rel="canonical" href="https://jinjiaojiao.top/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"><title>PyTorch学习第二章——深度学习入门 - PyTorch学习 | 金娇娇 = 去留无意，漫随天外云卷云舒</title><meta name="generator" content="Hexo 7.1.1"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">PyTorch学习第二章——深度学习入门</h1><div class="meta"><span class="item" title="创建时间：2024-02-11 18:25:18"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-02-11T18:25:18+08:00">2024-02-11</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>31k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>28 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">金娇娇</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclfdu6exj20zk0m87hw.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gipew28b65j20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclh5u05ej20zk0m87df.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giph4lm9i7j20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicm07ih54j20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 PyTorch学习"><span itemprop="name">PyTorch学习</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jinjiaojiao.top/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Debra"><meta itemprop="description" content="去留无意，漫随天外云卷云舒, 金同学的个人博客"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="金娇娇"></span><div class="body md" itemprop="articleBody"><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NjU0Mzc5MQ==">60 分钟快速入门 PyTorch - 知乎 (zhihu.com)</span></p><h1 id="第二章pytorch之60min入门"><a class="anchor" href="#第二章pytorch之60min入门">#</a> 第二章： <code>PyTorch</code> 之 <code>60min</code> 入门</h1><p><img data-src="https://pic1.zhimg.com/80/v2-ef363cc5320400c63cf356c203d39bec_1440w.webp" alt="img"></p><h2 id="什么是-pytorch"><a class="anchor" href="#什么是-pytorch">#</a> 什么是 <code>PyTorch</code> ?</h2><p><code>PyTorch</code> 是一个基于 Python 的科学计算包，主要定位两类人群：</p><ul><li><code>NumPy</code> 的替代品，可以利用 <code>GPU</code> 的性能进行计算。</li><li>深度学习研究平台拥有足够的<strong>灵活性</strong>和<strong>速度</strong></li></ul><h2 id="开始学习"><a class="anchor" href="#开始学习">#</a> 开始学习</h2><h3 id="tensors-张量"><a class="anchor" href="#tensors-张量">#</a> Tensors (张量)</h3><p>Tensors 类似于 <code>NumPy</code> 的 <code>ndarrays</code> ，同时 Tensors 可以使用 <code>GPU</code> 进行计算。</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure><p></p><h5 id="torchempty-声明一个未初始化的矩阵"><a class="anchor" href="#torchempty-声明一个未初始化的矩阵">#</a> <strong>torch.empty()</strong>: 声明一个未初始化的矩阵。</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p></p><p>输出:</p><blockquote><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[9.2737e-41, 8.9074e-01, 1.9286e-37],</span><br><span class="line">     [1.7228e-34, 5.7064e+01, 9.2737e-41],</span><br><span class="line">     [2.2803e+02, 1.9288e-37, 1.7228e-34],</span><br><span class="line">     [1.4609e+04, 9.2737e-41, 5.8375e+04],</span><br><span class="line">     [1.9290e-37, 1.7228e-34, 3.7402e+06]])</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="torchrand随机初始化一个矩阵"><a class="anchor" href="#torchrand随机初始化一个矩阵">#</a> <strong>torch.rand()</strong>：随机初始化一个矩阵</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p></p><p>输出:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.6291,  0.2581,  0.6414],</span><br><span class="line">  [ 0.9739,  0.8243,  0.2276],</span><br><span class="line">  [ 0.4184,  0.1815,  0.5131],</span><br><span class="line">  [ 0.5533,  0.5440,  0.0718],</span><br><span class="line">  [ 0.2908,  0.1850,  0.5297]])</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="验证能否运行在gpu"><a class="anchor" href="#验证能否运行在gpu">#</a> 验证能否运行在 GPU</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><p></p><h5 id="torchzeros创建数值皆为-0-的矩阵"><a class="anchor" href="#torchzeros创建数值皆为-0-的矩阵">#</a> <strong>torch.zeros()</strong>：创建数值皆为 0 的矩阵</h5><p>Construct a matrix filled zeros and of dtype long:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5, 3, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p></p><p>输出:</p><blockquote><p>tensor([[ 0, 0, 0],<br>[ 0, 0, 0],<br>[ 0, 0, 0],<br>[ 0, 0, 0],<br>[ 0, 0, 0]])</p></blockquote><h5 id="torchtensor直接传递-tensor-数值来创建"><a class="anchor" href="#torchtensor直接传递-tensor-数值来创建">#</a> <strong>torch.tensor()</strong>：直接传递 tensor 数值来创建</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor 数值是 [5.5 , 3]</span><br><span class="line">x = torch.tensor([5.5, 3])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p></p><p>输出:</p><blockquote><p>tensor([ 5.5000, 3.0000])</p></blockquote><p>除了上述几种方法，还可以根据已有的 tensor 变量创建新的 tensor 变量，这种做法的好处就是可以保留已有 tensor 的一些属性，包括尺寸大小、数值属性，除非是重新定义这些属性。相应的实现方法如下：</p><h5 id="tensornew_onesnew_-方法需要输入尺寸大小"><a class="anchor" href="#tensornew_onesnew_-方法需要输入尺寸大小">#</a> tensor.new_ones()<em>：new_</em>() 方法需要输入尺寸大小</h5><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 显示定义新的尺寸是 5*3，数值类型是 torch.double</span><br><span class="line">tensor2 = tensor1.new_ones(5, 3, dtype=torch.double)  # new_* 方法需要输入 tensor 大小</span><br><span class="line">print(tensor2)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p></p><h5 id="torchrandn_likeold_tensor保留相同的尺寸大小"><a class="anchor" href="#torchrandn_likeold_tensor保留相同的尺寸大小">#</a> <strong>torch.randn_like(old_tensor)</strong>：保留相同的尺寸大小</h5><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 修改数值类型</span><br><span class="line">tensor3 = torch.randn_like(tensor2, dtype=torch.float)</span><br><span class="line">print(&#x27;tensor3: &#x27;, tensor3)</span><br></pre></td></tr></table></figure><p></p><p>输出结果，这里是根据上个方法声明的 <code>tensor2</code> 变量来声明新的变量，可以看出尺寸大小都是 5*3，但是数值类型是改变了的。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor3:  tensor([[-0.4491, -0.2634, -0.0040],</span><br><span class="line">        [-0.1624,  0.4475, -0.8407],</span><br><span class="line">        [-0.6539, -1.2772,  0.6060],</span><br><span class="line">        [ 0.2304,  0.0879, -0.3876],</span><br><span class="line">        [ 1.2900, -0.7475, -1.8212]])</span><br></pre></td></tr></table></figure><p></p><p>最后，对 tensors 的尺寸大小获取可以采用 <code>tensor.size()</code> 方法：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(tensor3.size())  </span><br><span class="line"># 输出: torch.Size([5, 3])</span><br></pre></td></tr></table></figure><p></p><h5 id="获取它的维度信息"><a class="anchor" href="#获取它的维度信息">#</a> <strong>获取它的维度信息:</strong></h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><p></p><p>输出:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure><p></p></blockquote><p>注意</p><p><strong>注意</strong>： <code>torch.Size</code> 实际上是<strong>元组 (tuple) 类型，所以支持所有的元组操作</strong>。</p><h3 id="操作"><a class="anchor" href="#操作">#</a> 操作</h3><p>在接下来的例子中，我们将会看到加法操作。</p><h4 id="加法"><a class="anchor" href="#加法">#</a> 加法</h4><h5 id="运算符"><a class="anchor" href="#运算符">#</a> + 运算符</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class="line">     [ 2.3854,  0.0707,  2.1970],</span><br><span class="line">     [-0.3587,  1.2359,  1.8951],</span><br><span class="line">     [-0.1189, -0.1376,  0.4647],</span><br><span class="line">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="add"><a class="anchor" href="#add">#</a> add</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class="line">        [ 2.3854,  0.0707,  2.1970],</span><br><span class="line">        [-0.3587,  1.2359,  1.8951],</span><br><span class="line">        [-0.1189, -0.1376,  0.4647],</span><br><span class="line">        [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure><p></p><h5 id="result提供一个输出"><a class="anchor" href="#result提供一个输出">#</a> result 提供一个输出</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(5, 3)</span><br><span class="line">torch.add(x, y, out=result) #x+y 结果储存在result中</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class="line">     [ 2.3854,  0.0707,  2.1970],</span><br><span class="line">     [-0.3587,  1.2359,  1.8951],</span><br><span class="line">     [-0.1189, -0.1376,  0.4647],</span><br><span class="line">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="add_-直接修改变量"><a class="anchor" href="#add_-直接修改变量">#</a> add_ 直接修改变量</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># adds x to y</span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1859,  1.3970,  0.5236],</span><br><span class="line">     [ 2.3854,  0.0707,  2.1970],</span><br><span class="line">     [-0.3587,  1.2359,  1.8951],</span><br><span class="line">     [-0.1189, -0.1376,  0.4647],</span><br><span class="line">     [-1.8968,  2.0164,  0.1092]])</span><br></pre></td></tr></table></figure><p></p></blockquote><p><strong>Note：</strong></p><p>注意 任何使张量会发生变化的操作都有一个前缀 '_'。例如： <code>x.copy_(y)</code> , <code>x.t_()</code> , 将会改变 <code>x</code> .</p><h4 id="对于-tensor-的访问"><a class="anchor" href="#对于-tensor-的访问">#</a> 对于 Tensor 的访问</h4><p>除了加法运算操作，，和 Numpy 对数组类似，可以使用索引来访问某一维的数据，如下所示：</p><h5 id="索引操作"><a class="anchor" href="#索引操作">#</a> <strong>索引</strong>操作</h5><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 访问 tensor3 第一列数据</span><br><span class="line">print(x[:, 1])</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.4477, -0.0048,  1.0878, -0.2174,  1.3609])</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="torchview对-tensor-的尺寸修改"><a class="anchor" href="#torchview对-tensor-的尺寸修改">#</a> torch.view ()：对 Tensor 的尺寸修改</h5><p>如果你想改变一个 tensor 的大小或者形状，你可以使用 <code>torch.view</code> :</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(4, 4)</span><br><span class="line">y = x.view(16)  # 1*16</span><br><span class="line">z = x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><blockquote><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span><br></pre></td></tr></table></figure><p></p></blockquote><p><code>-1</code> 用于 <code>view</code> 方法中作为一个特殊的参数值，表示自动计算该维度的大小。当你重新调整一个张量的形状时， <code>-1</code> 将会被替换为一个值，这个值是根据张量的总元素数和其他维度的大小自动计算出来的，以保证新形状的元素总数与原张量相同。</p><p>总数不变</p><h5 id="item"><a class="anchor" href="#item">#</a> .item()</h5><p>如果你有一个元素 tensor ，<strong>使用 .item () 来获得这个 value</strong> 。如果 tensor 仅有一个元素，可以采用 <code>.item()</code> 来获取类似 Python 中整数类型的数值：</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(1)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><p></p><p>Out:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.9422])</span><br><span class="line">0.9422121644020081</span><br></pre></td></tr></table></figure><p></p><p>更多运算操作请看文档</p><p><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS90b3JjaC5odG1s">torch — PyTorch 2.2 documentation</span></p><h3 id="和numpy数组的转换"><a class="anchor" href="#和numpy数组的转换">#</a> 和 Numpy 数组的转换</h3><p>Tensor 和 Numpy 的数组可以相互转换，并且两者转换后共享在 CPU 下的内存空间，即改变其中一个的数值，另一个变量也会随之改变。</p><h3 id="tensor-转换为-numpy-数组"><a class="anchor" href="#tensor-转换为-numpy-数组">#</a> <strong>Tensor 转换为 Numpy 数组</strong></h3><p>实现 Tensor 转换为 Numpy 数组的例子如下所示，调用 <code>tensor.numpy()</code> 可以实现这个转换操作。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(5)</span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1., 1.])</span><br><span class="line">[1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure><p></p><h3 id="numpy-数组转换为-tensor"><a class="anchor" href="#numpy-数组转换为-tensor">#</a> <strong>Numpy 数组转换为 Tensor</strong></h3><p>转换的操作是调用 <code>torch.from_numpy(numpy_array)</code> 方法。例子如下所示：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2. 2. 2. 2. 2.]</span><br><span class="line">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p></p><p>在 <code>CPU</code> 上，除了 <code>CharTensor</code> 外的所有 <code>Tensor</code> 类型变量，都支持和 <code>Numpy</code> 数组的相互转换操作。</p><h3 id="cuda-张量"><a class="anchor" href="#cuda-张量">#</a> <strong>CUDA 张量</strong></h3><p><code>Tensors</code> 可以通过 <code>.to</code> 方法转换到不同的设备上，即 CPU 或者 GPU 上。</p><p>例子：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 当 CUDA 可用的时候，可用运行下方这段代码，采用 torch.device() 方法来改变 tensors 是否在 GPU 上进行计算操作</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    device = torch.device(&quot;cuda&quot;)          # 定义一个 CUDA 设备对象</span><br><span class="line">    y = torch.ones_like(x, device=device)  # 显示创建在 GPU 上的一个 tensor</span><br><span class="line">    x = x.to(device)                       # 也可以采用 .to(&quot;cuda&quot;) </span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(&quot;cpu&quot;, torch.double))       # .to() 方法也可以改变数值类型</span><br></pre></td></tr></table></figure><p></p><p>输出结果，第一个结果就是在 GPU 上的结果，打印变量的时候会带有 <code>device='cuda:0'</code> ，而第二个是在 CPU 上的变量。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([1.4549], device=&#x27;cuda:0&#x27;)</span><br><span class="line">tensor([1.4549], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p></p><p>本小节教程：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovdGVuc29yX3R1dG9yaWFsLmh0bWw=">https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html</span></p><p>本小节的代码：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS9iYXNpY19wcmFjdGlzZS5pcHluYg==">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/basic_practise.ipynb</span></p><h3 id="autograd"><a class="anchor" href="#autograd">#</a> <strong>autograd</strong></h3><p>对于 Pytorch 的神经网络来说，非常关键的一个库就是 <code>autograd</code> ，</p><p>提供了对 <code>Tensors</code> 上所有运算操作的<strong>自动微分功能</strong>，也就是<strong>计算梯度</strong>的功能。</p><p>它属于 <code>define-by-run</code> 类型框架，即反向传播操作的定义是根据代码的运行方式，因此每次迭代都可以是不同的。</p><h3 id="张量"><a class="anchor" href="#张量">#</a> <strong>张量</strong></h3><p><code>torch.Tensor</code> 是 Pytorch 最主要的库，当设置它的属性 <code>.requires_grad=True</code> ，那么就会开始<strong>追踪在该变量上的所有操作</strong>，而完成计算后，可以调用 <code>.backward()</code> 并自动计算所有的梯度，得到的梯度都保存在属性 <code>.grad</code> 中。</p><p>调用 <code>.detach()</code> 方法<strong>分离出计算的历史</strong>，可以停止一个 tensor 变量继续追踪其历史信息 ，同时也防止未来的计算会被追踪。</p><p>使用 <code>with torch.no_grad():</code> 就是告诉 PyTorch：“现在我只想用模型来做一些前向计算，不需要做梯度更新，请暂时不要保存那些用于梯度更新所必需的信息，以节省计算资源和内存”。这样做可以让模型运行得更快，同时消耗更少的资源。</p><h4 id="function"><a class="anchor" href="#function">#</a> Function</h4><p>对于 <code>autograd</code> 的实现，还有一个类也是非常重要 <code>Function</code> 。</p><p><code>Tensor</code> 和 <code>Function</code> 两个类是有关联并建立了一个<strong>非循环的图</strong>，可以编码一个完整的计算记录。每个 tensor 变量都带有属性 <code>.grad_fn</code> ，该属性引用了创建了这个变量的 <code>Function</code> （除了由用户创建的 Tensors，它们的 <code>grad_fn=None</code> )。</p><p>&lt;details&gt;<br>&lt;summary&gt;grad_fn&lt;/summary&gt;<br>在深度学习中，模型训练的一个重要步骤是计算损失函数（即模型输出与真实值之间的差距）关于模型参数的梯度（或导数），然后根据这些梯度来更新模型参数，以使损失函数的值减小。这个过程称为梯度下降。PyTorch 通过建立一个计算图来帮助实现这个过程，而这个计算图是由 Tensor 和 Function 这两个类的实例组成的。<br>Tensor<br>在 PyTorch 中，Tensor 是一个多维数组，用于存储模型的输入数据、参数、输出数据以及计算过程中的各种中间数据。每个 Tensor 都可以跟踪它是如何被创建的 —— 即它是通过什么样的操作从其他 Tensor 转换而来的。Function<br>每个操作，不管是简单的数学运算还是复杂的神经网络层操作，都可以看作是一个 Function。这些 Function 不仅执行计算，还记录了计算的细节，以便于后续进行梯度的反向传播。<br>计算图<br>当你在 PyTorch 中执行操作时，你实际上是在构建一个计算图。这个图是由节点（Tensor）和边（Function，表示操作）组成的。这个图是向前构建的：从输入 Tensor 开始，通过各种操作，最终到达输出 Tensor。这个过程称为前向传播。<br>.grad_fn 属性<br>每个 Tensor 都有一个.grad_fn 属性，这个属性是一个指向 Function 的引用，即这个 Tensor 是通过哪个 Function 计算得到的。如果这个 Tensor 是直接由用户创建的（不是通过某些操作得到的），那么它的.grad_fn 就是 None，因为它不是通过计算得到的。<br>非循环图<br>这个计算图是非循环的，意味着数据流是有方向的，从输入流向输出，不会有任何循环或回路。这使得在图中进行前向传播和反向传播（用于计算梯度）变得简单明了。</p><p>为什么这很重要？<br>当进行反向传播以计算梯度时，PyTorch 会沿着这个图从输出向后逐步移动，使用<strong>链式法</strong>则自动计算每个参数的梯度。这个过程完全自动化，用户不需要手动编写梯度计算代码，极大地简化了深度学习模型的训练过程。</p><p>简而言之， <code>Tensor</code> 和 <code>Function</code> 通过<strong>计算图</strong>相互关联，这个图能够追踪整个计算过程，为自动梯度计算（自动微分）提供支持，使得深度学习模型的训练变得更加高效和简单。</p><p>如果要进行求导运算，可以调用一个 <code>Tensor</code> 变量的方法 <code>.backward()</code> 。如果该变量是一个标量，即仅有一个元素，那么不需要传递任何参数给方法 <code>.backward()</code> ，当包含多个元素的时候，就必须指定一个 <code>gradient</code> 参数，表示匹配尺寸大小的 tensor，这部分见第二小节介绍梯度的内容。</p><p>接下来就开始用代码来进一步介绍。</p><p>首先导入必须的库：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure><p></p><p>开始创建一个 tensor， 并让 <code>requires_grad=True</code> 来<strong>追踪该变量相关的计算操作</strong>：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]], requires_grad=True)</span><br></pre></td></tr></table></figure><p></p><p>执行任意计算操作，这里进行简单的加法运算：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[3., 3.],</span><br><span class="line">        [3., 3.]], grad_fn=&lt;AddBackward&gt;)</span><br></pre></td></tr></table></figure><p></p><p><code>y</code> 是一个操作的结果，所以它带有属性 <code>grad_fn</code> ：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;AddBackward object at 0x00000216D25DCC88&gt;</span><br></pre></td></tr></table></figure><p></p><p>继续对变量 <code>y</code> 进行操作：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(&#x27;z=&#x27;, z)</span><br><span class="line">print(&#x27;out=&#x27;, out)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z= tensor([[27., 27.],</span><br><span class="line">        [27., 27.]], grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">out= tensor(27., grad_fn=&lt;MeanBackward1&gt;)</span><br></pre></td></tr></table></figure><p></p><p>实际上，一个 <code>Tensor</code> 变量的默认 <code>requires_grad</code> 是 <code>False</code> ，可以像上述定义一个变量时候指定该属性是 <code>True</code> ，当然也可以定义变量后，调用 <code>.requires_grad_(True)</code> 设置为 <code>True</code> ，这里带有后缀 <code>_</code> 是会改变变量本身的属性，在上一节介绍加法操作 <code>add_()</code> 说明过</p><p>代码例子：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(2, 2)</span><br><span class="line">a = ((a * 3) / (a - 1))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(True)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure><p></p><p>输出结果如下，第一行是为设置 <code>requires_grad</code> 的结果，接着显示调用 <code>.requires_grad_(True)</code> ，输出结果就是 <code>True</code> 。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line"></span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&lt;SumBackward0 object at 0x00000216D25ED710&gt;</span><br></pre></td></tr></table></figure><p></p><h3 id="梯度"><a class="anchor" href="#梯度">#</a> <strong>梯度</strong></h3><p>接下来就是开始计算梯度，进行<strong>反向传播</strong>的操作。 <code>out</code> 变量是上一小节中定义的，它是一个标量，因此 <code>out.backward()</code> 相当于 <code>out.backward(torch.tensor(1.))</code> ，</p><p>代码如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"># 输出梯度 d(out)/dx</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[4.5000, 4.5000],</span><br><span class="line">        [4.5000, 4.5000]])</span><br></pre></td></tr></table></figure><p></p><p>结果应该就是得到数值都是 4.5 的矩阵。这里我们用 <code>o</code> 表示 <code>out</code> 变量，那么根据之前的定义会有：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">O = \frac{1}{4} \sum_i z_i,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.599109em;vertical-align:-1.277669em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.04398em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mn>3</mn><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mn>2</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">z_i = 3(x_i + 2)^2,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.04398em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">3</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-.25em"></span><span class="mord">2</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8641079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><msub><mo fence="false">∣</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>27</mn></mrow><annotation encoding="application/x-tex">z_i \big|_{x_i=1} = 27</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3677899999999998em;vertical-align:-.49980999999999987em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.04398em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.051398000000000055em"><span style="top:-2.30029em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.49980999999999987em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">7</span></span></span></span></span></p><p>详细来说，初始定义的 <code>x</code> 是一个全为 1 的矩阵，然后加法操作 <code>x+2</code> 得到 <code>y</code> ，接着 <code>y*y*3</code> ， 得到 <code>z</code> ，并且此时 <code>z</code> 是一个 2*2 的矩阵，所以整体求平均得到 <code>out</code> 变量应该是除以 4，所以得到上述三条公式。</p><p>因此，计算梯度：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>o</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>3</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mn>2</mn><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\frac{\partial o}{\partial x_i} = \frac{3}{2} (x_i + 2),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-.8360000000000001em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.3139999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8360000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">2</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>o</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo fence="true">∣</mo></mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>9</mn><mn>2</mn></mfrac><mo>=</mo><mn>4.5</mn></mrow><annotation encoding="application/x-tex">\left.\frac{\partial o}{\partial x_i}\right|_{x_i=1} = \frac{9}{2} = 4.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.57979em;vertical-align:-1.0998199999999998em"></span><span class="minner"><span class="minner"><span class="mopen nulldelimiter"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.3139999999999996em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8360000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4799700000000002em"><span style="top:-1.65598em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-2.25698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-2.85798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-2.87897em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span><span style="top:-3.47997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.9500199999999999em"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:-.5486119999999999em"><span style="top:-1.7002800000000005em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3280857142857143em"><span style="top:-2.357em;margin-left:0;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.143em"><span></span></span></span></span></span></span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0998199999999998em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span><span class="mord">.</span><span class="mord">5</span></span></span></span></span></p><p>从数学上来说，如果你有一个向量值函数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><mi>x</mi><mo>⃗</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = f(\vec{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.20772em"><span class="overlay" style="height:.714em;width:.471em"><svg width="0.471em" height="0.714em" style="width:.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>那么对应的梯度是一个雅克比矩阵 (Jacobian matrix)：</p><p><img data-src="C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210172846221.png" alt="image-20240210172846221"></p><p>一般来说， <code>torch.autograd</code> 就是用于计算雅克比向量 (vector-Jacobian) 乘积的工具。这里略过数学公式，直接上代码例子介绍：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3, requires_grad=True)</span><br><span class="line"></span><br><span class="line">y = x * 2</span><br><span class="line">while y.data.norm() &lt; 1000:</span><br><span class="line">    y = y * 2</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p></p><h2 id="神经网络"><a class="anchor" href="#神经网络">#</a> 神经网络</h2><p>在 PyTorch 中 <code>torch.nn</code> 专门用于实现神经网络。其中 <code>nn.Module</code> 包含了网络层的搭建，以及一个方法 -- <code>forward(input)</code> ，并返回网络的输出 <code>output</code> .</p><p>下面是一个经典的 LeNet 网络，用于对字符进行分类。</p><p><img data-src="https://pic4.zhimg.com/80/v2-06a914f4ee93f25c0d6c924df9b4b4cb_1440w.webp" alt="img"></p><p>对于神经网络来说，一个标准的<strong>训练流程</strong>是这样的：</p><ul><li><p>定义一个<strong>多层的神经网络</strong></p></li><li><p>对数据集的<strong>预处理</strong>并准备作为网络的输入</p></li><li><p>将数据<strong>输入到网络</strong></p></li><li><p>计算网络的<strong>损失</strong></p></li><li><p><strong>反向传播</strong>，计算<strong>梯度</strong></p></li><li><p><strong>更新</strong>网络的梯度，一个简单的更新规则是 <code>weight = weight - learning_rate * gradient</code></p></li></ul><h3 id="定义网络"><a class="anchor" href="#定义网络">#</a> <strong>定义网络</strong></h3><p>首先定义一个神经网络，下面是一个 5 层的卷积神经网络，包含两层卷积层和三层全连接层：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">         <span class="comment"># 输入图像是单通道，conv1 kenrnel size=5*5，输出通道 6</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span> ,<span class="number">6</span> ,<span class="number">5</span> )</span><br><span class="line">        <span class="comment"># conv2 kernel size=5*5, 输出通道 16</span></span><br><span class="line">        <span class="comment">#全连接层</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span> , <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>,<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment"># max-pooling 采用一个 (2,2) 的滑动窗口</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">         <span class="comment"># 核(kernel)大小是方形的话，可仅定义一个数字，如 (2,2) 用 2 即可</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 除了 batch 维度外的所有维度</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">打印网络结构：</span><br><span class="line"></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>打印网络结构：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (fc1): Linear(in_features=400, out_features=120, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>这里必须实现 <code>forward</code> 函数，而 <code>backward</code> 函数在采用 <code>autograd</code> 时就自动定义好了，在 <code>forward</code> 方法可以采用任何的张量操作。</p><p><code>net.parameters()</code> 可以返回网络的训练参数，使用例子如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(&#x27;参数数量: &#x27;, len(params))</span><br><span class="line"># conv1.weight</span><br><span class="line">print(&#x27;第一个参数大小: &#x27;, params[0].size())</span><br></pre></td></tr></table></figure><p></p><p>输出：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">参数数量:  10</span><br><span class="line">第一个参数大小:  torch.Size([6, 1, 5, 5])</span><br></pre></td></tr></table></figure><p></p><p>然后简单测试下这个网络，随机生成一个 32*32 的输入：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 随机定义一个变量输入网络</span><br><span class="line">input = torch.randn(1, 1, 32, 32)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.1005,  0.0263,  0.0013, -0.1157, -0.1197, -0.0141,  0.1425, -0.0521,</span><br><span class="line">          0.0689,  0.0220]], grad_fn=&lt;ThAddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p></p><p>接着反向传播需要先清空梯度缓存，并反向传播随机梯度：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 清空所有参数的梯度缓存，然后计算随机梯度进行反向传播</span><br><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure><p></p><p><strong>注意</strong>：</p><blockquote><p><code>torch.nn</code> 只支持 ** 小批量 (mini-batches)** 数据，也就是输入不能是单个样本，比如对于 <code>nn.Conv2d</code> 接收的输入是一个 4 维张量 -- <code>nSamples * nChannels * Height * Width</code> 。<br>所以，如果你输入的是单个样本，<strong>需要采用</strong> <code>**input.unsqueeze(0)**</code> <strong>来扩充一个假的 batch 维度，即从 3 维变为 4 维</strong>。</p></blockquote><h3 id="损失函数"><a class="anchor" href="#损失函数">#</a> <strong>损失函数</strong></h3><p>损失函数的输入是 <code>(output, target)</code> ，即网络输出和真实标签对的数据，然后返回一个数值表示网络输出和真实标签的差距。</p><p>PyTorch 中其实已经定义了不少的<span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy9kb2NzL25uLmh0bWwlMjNsb3NzLWZ1bmN0aW9ucw==">损失函数</span>，这里仅采用简单的均方误差： <code>nn.MSELoss</code> ，例子如下：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 定义伪标签</span></span><br><span class="line">target = torch.randn(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 调整大小，使得和 output 一样的 size</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><p></p><p>输出如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.6524, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure><p></p><p>这里，整个网络的数据输入到输出经历的计算图如下所示，其实也就是数据从输入层到输出层，计算 <code>loss</code> 的过程。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure><p></p><p>如果调用 <code>loss.backward()</code> ，那么整个图都是可微分的，也就是说包括 <code>loss</code> ，图中的所有张量变量，只要其属性 <code>requires_grad=True</code> ，那么其梯度 <code>.grad</code> 张量都会随着梯度一直累计。</p><p>用代码来说明：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn)</span><br><span class="line"><span class="comment"># Linear layer</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Relu</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p></p><p>输出：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;MseLossBackward object at 0x0000019C0C349908&gt;</span><br><span class="line"></span><br><span class="line">&lt;ThAddmmBackward object at 0x0000019C0C365A58&gt;</span><br><span class="line"></span><br><span class="line">&lt;ExpandBackward object at 0x0000019C0C3659E8&gt;</span><br></pre></td></tr></table></figure><p></p><h3 id="反向传播"><a class="anchor" href="#反向传播">#</a> <strong>反向传播</strong></h3><p>反向传播的实现只需要调用 <code>loss.backward()</code> 即可，当然首先需要清空当前梯度缓存，即 <code>.zero_grad()</code> 方法，否则之前的梯度会累加到当前的梯度，这样会影响权值参数的更新。</p><p>下面是一个简单的例子，以 <code>conv1</code> 层的偏置参数 <code>bias</code> 在反向传播前后的结果为例：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空所有参数的梯度缓存</span></span><br><span class="line">net.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line">tensor([0., 0., 0., 0., 0., 0.])</span><br><span class="line"></span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([ 0.0069,  0.0021,  0.0090, -0.0060, -0.0008, -0.0073])</span><br></pre></td></tr></table></figure><p></p><p>了解更多有关 <code>torch.nn</code> 库，可以查看官方文档：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1s">https://pytorch.org/docs/stable/nn.html</span></p><h3 id="更新权重"><a class="anchor" href="#更新权重">#</a> <strong>更新权重</strong></h3><p>采用随机梯度下降 (Stochastic Gradient Descent, SGD) 方法的最简单的更新权重规则如下：</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure><p></p><p>按照这个规则，代码实现如下所示：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 简单实现权重的更新例子</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p></p><p>但是这只是最简单的规则，深度学习有很多的优化算法，不仅仅是 <code>SGD</code> ，还有 <code>Nesterov-SGD, Adam, RMSProp</code> 等等，为了采用这些不同的方法，这里采用 <code>torch.optim</code> 库，使用例子如下所示：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"># 创建优化器</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># 在训练过程中执行下列操作</span><br><span class="line">optimizer.zero_grad() # 清空梯度缓存</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line"># 更新权重</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p></p><p><strong>注意</strong>，同样需要调用 <code>optimizer.zero_grad()</code> 方法清空梯度缓存。</p><p>本小节教程：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovbmV1cmFsX25ldHdvcmtzX3R1dG9yaWFsLmh0bWw=">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</span></p><p>本小节的代码：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS9uZXVyYWxfbmV0d29yay5pcHluYg==">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/neural_network.ipynb</span></p><h3 id="训练分类器"><a class="anchor" href="#训练分类器">#</a> <strong>训练分类器</strong></h3><p>上一节介绍了如何构建神经网络、计算 <code>loss</code> 和更新网络的权值参数，接下来需要做的就是实现一个图片分类器。</p><h4 id="训练数据"><a class="anchor" href="#训练数据">#</a> <strong>训练数据</strong></h4><p>在训练分类器前，当然需要考虑数据的问题。通常在处理如图片、文本、语音或者视频数据的时候，一般都采用标准的 Python 库将其加载并转成 Numpy 数组，然后再转回为 PyTorch 的张量。</p><ul><li>对于图像，可以采用 <code>Pillow, OpenCV</code> 库；</li><li>对于语音，有 <code>scipy</code> 和 <code>librosa</code> ;</li><li>对于文本，可以选择原生 Python 或者 Cython 进行加载数据，或者使用 <code>NLTK</code> 和 <code>SpaCy</code> 。</li></ul><p>PyTorch 对于计算机视觉，特别创建了一个 <code>torchvision</code> 的库，它包含一个数据加载器 (data loader)，可以加载比较常见的数据集，比如 <code>Imagenet, CIFAR10, MNIST</code> 等等，然后还有一个用于图像的数据转换器 (data transformers)，调用的库是 <code>torchvision.datasets</code> 和 <code>torch.utils.data.DataLoader</code> 。</p><p>在本教程中，将采用 <code>CIFAR10</code> 数据集，它包含 10 个类别，分别是飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。数据集中的图片都是 <code>3x32x32</code> 。一些例子如下所示：</p><p><img data-src="https://pic2.zhimg.com/80/v2-2dcc41f9079d1abf5883a113c0d1ca31_1440w.webp" alt="img"></p><h3 id="训练图片分类器"><a class="anchor" href="#训练图片分类器">#</a> <strong>训练图片分类器</strong></h3><p>训练流程如下：</p><ol><li>通过调用 <code>torchvision</code> 加载和归一化 <code>CIFAR10</code> 训练集和测试集；</li><li>构建一个卷积神经网络；</li><li>定义一个损失函数；</li><li>在训练集上训练网络；</li><li>在测试集上测试网络性能。</li></ol><h4 id="加载和归一化-cifar10"><a class="anchor" href="#加载和归一化-cifar10">#</a> <strong>加载和归一化 CIFAR10</strong></h4><p>首先导入必须的包：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure><p></p><p>`</p><p><code>torchvision</code> 的数据集输出的图片都是 <code>PILImage</code> ，即取值范围是 <code>[0, 1]</code> ，这里需要做一个转换，变成取值范围是 <code>[-1, 1]</code> ,</p><p>代码如下所示：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将图片数据从 [0,1] 归一化为 [-1, 1] 的取值范围</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><p>这里下载好数据后，可以可视化部分训练图片，代码如下：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># 非归一化</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机获取训练集图片</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># 打印图片类别标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p></p><p>展示图片如下所示：</p><p><img data-src="https://pic2.zhimg.com/80/v2-736499796b713d873d1f9ae72fbc66f5_1440w.webp" alt="img"></p><p>其类别标签为：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frog plane   dog  ship</span><br></pre></td></tr></table></figure><p></p><h4 id="构建一个卷积神经网络"><a class="anchor" href="#构建一个卷积神经网络">#</a> <strong>构建一个卷积神经网络</strong></h4><p>这部分内容其实直接采用上一节定义的网络即可，除了修改 <code>conv1</code> 的输入通道，从 1 变为 3，因为这次接收的是 3 通道的彩色图片。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><p></p><h4 id="定义损失函数和优化器"><a class="anchor" href="#定义损失函数和优化器">#</a> <strong>定义损失函数和优化器</strong></h4><p>这里采用<strong>类别交叉熵函数</strong>和<strong>带有动量的 SGD 优化方法：</strong></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="训练网络"><a class="anchor" href="#训练网络">#</a> <strong>训练网络</strong></h4><p>第四步自然就是开始训练网络，指定需要迭代的 epoch，然后输入数据，指定次数打印当前网络的信息，比如 <code>loss</code> 或者准确率等性能评价标准。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取输入数据</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line">        <span class="comment"># 清空梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印统计信息</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">            <span class="comment"># 每 2000 次迭代打印一次信息</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, i+<span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training! Total cost time: &#x27;</span>, time.time()-start)</span><br></pre></td></tr></table></figure><p></p><p>这里定义训练总共 2 个 epoch，训练信息如下，大概耗时为 77s。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[1,  2000] loss: 2.226</span><br><span class="line">[1,  4000] loss: 1.897</span><br><span class="line">[1,  6000] loss: 1.725</span><br><span class="line">[1,  8000] loss: 1.617</span><br><span class="line">[1, 10000] loss: 1.524</span><br><span class="line">[1, 12000] loss: 1.489</span><br><span class="line">[2,  2000] loss: 1.407</span><br><span class="line">[2,  4000] loss: 1.376</span><br><span class="line">[2,  6000] loss: 1.354</span><br><span class="line">[2,  8000] loss: 1.347</span><br><span class="line">[2, 10000] loss: 1.324</span><br><span class="line">[2, 12000] loss: 1.311</span><br><span class="line"></span><br><span class="line">Finished Training! Total cost time:  77.24696755409241</span><br></pre></td></tr></table></figure><p></p><h4 id="测试模型性能"><a class="anchor" href="#测试模型性能">#</a> <strong>测试模型性能</strong></h4><p>训练好一个网络模型后，就需要用测试集进行测试，检验网络模型的泛化能力。对于图像分类任务来说，一般就是用准确率作为评价标准。</p><p>首先，我们先用一个 <code>batch</code> 的图片进行小小测试，这里 <code>batch=4</code> ，也就是 4 张图片，代码如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"># 打印图片</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[labels[j]] for j in range(4)))</span><br></pre></td></tr></table></figure><p></p><p>图片和标签分别如下所示：</p><p><img data-src="https://pic4.zhimg.com/80/v2-ddb522e45f298b8da5ab6d3a48ac470b_1440w.webp" alt="img"></p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GroundTruth:    cat  ship  ship plane</span><br></pre></td></tr></table></figure><p></p><p>然后用这四张图片输入网络，看看网络的预测结果：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 网络输出</span><br><span class="line">outputs = net(images)</span><br><span class="line"></span><br><span class="line"># 预测结果</span><br><span class="line">_, predicted = torch.max(outputs, 1)</span><br><span class="line">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(&#x27;%5s&#x27; % classes[predicted[j]] for j in range(4)))</span><br></pre></td></tr></table></figure><p></p><p>输出为：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted:    cat  ship  ship  ship</span><br></pre></td></tr></table></figure><p></p><p>前面三张图片都预测正确了，第四张图片错误预测飞机为船。</p><p>接着，让我们看看在整个测试集上的准确率可以达到多少吧！</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % (100 * correct / total))</span><br></pre></td></tr></table></figure><p></p><p>输出结果如下</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the network on the 10000 test images: 55 %</span><br></pre></td></tr></table></figure><p></p><p>这里可能准确率并不一定一样，教程中的结果是 <code>51%</code> ，因为权重初始化问题，可能多少有些浮动，相比随机猜测 10 个类别的准确率 (即 10%)，这个结果是不错的，当然实际上是非常不好，不过我们仅仅采用 5 层网络，而且仅仅作为教程的一个示例代码。</p><p>然后，还可以再进一步，查看每个类别的分类准确率，跟上述代码有所不同的是，计算准确率部分是 <code>c = (predicted == labels).squeeze()</code> ，这段代码其实会根据预测和真实标签是否相等，输出 1 或者 0，表示真或者假，因此在计算当前类别正确预测数</p><p>量时候直接相加，预测正确自然就是加 1，错误就是加 0，也就是没有变化。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(0. for i in range(10))</span><br><span class="line">class_total = list(0. for i in range(10))</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, 1)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        for i in range(4):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    print(&#x27;Accuracy of %5s : %2d %%&#x27; % (classes[i], 100 * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><p></p><p>输出结果，可以看到猫、鸟、鹿是错误率前三，即预测最不准确的三个类别，反倒是船和卡车最准确。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of plane : 58 %</span><br><span class="line">Accuracy of   car : 59 %</span><br><span class="line">Accuracy of  bird : 40 %</span><br><span class="line">Accuracy of   cat : 33 %</span><br><span class="line">Accuracy of  deer : 39 %</span><br><span class="line">Accuracy of   dog : 60 %</span><br><span class="line">Accuracy of  frog : 54 %</span><br><span class="line">Accuracy of horse : 66 %</span><br><span class="line">Accuracy of  ship : 70 %</span><br><span class="line">Accuracy of truck : 72 %</span><br></pre></td></tr></table></figure><p></p><h3 id="在-gpu-上训练"><a class="anchor" href="#在-gpu-上训练">#</a> <strong>在 GPU 上训练</strong></h3><p>深度学习自然需要 GPU 来加快训练速度的。所以接下来介绍如果是在 GPU 上训练，应该如何实现。</p><p>首先，需要检查是否有可用的 GPU 来训练，代码如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure><p></p><p>输出结果如下，这表明你的第一块 GPU 显卡或者唯一的 GPU 显卡是空闲可用状态，否则会打印 <code>cpu</code> 。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cuda:0</span><br></pre></td></tr></table></figure><p></p><p>既然有可用的 GPU ，接下来就是在 GPU 上进行训练了，其中需要修改的代码如下，分别是需要将网络参数和数据都转移到 GPU 上：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure><p></p><p>修改后的训练部分代码：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line"># 在 GPU 上训练注意需要将网络和数据放到 GPU 上</span><br><span class="line">net.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">for epoch in range(2):</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # 获取输入数据</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">        # 清空梯度缓存</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # 打印统计信息</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999:</span><br><span class="line">            # 每 2000 次迭代打印一次信息</span><br><span class="line">            print(&#x27;[%d, %5d] loss: %.3f&#x27; % (epoch + 1, i+1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line">print(&#x27;Finished Training! Total cost time: &#x27;, time.time() - start)</span><br></pre></td></tr></table></figure><p></p><p>注意，这里调用 <code>net.to(device)</code> 后，需要定义下优化器，即传入的是 CUDA 张量的网络参数。训练结果和之前的类似，而且其实因为这个网络非常小，转移到 GPU 上并不会有多大的速度提升，而且我的训练结果看来反而变慢了，也可能是因为我的笔记本的 GPU 显卡问题。</p><p>如果需要进一步提升速度，可以考虑采用多 GPUs，也就是下一节的内容。</p><p>本小节教程：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovY2lmYXIxMF90dXRvcmlhbC5odG1s">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html</span></p><p>本小节的代码：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL2NjYzAxMy9EZWVwTGVhcm5pbmdfTm90ZXMvYmxvYi9tYXN0ZXIvUHl0b3JjaC9wcmFjdGlzZS90cmFpbl9jbGFzc2lmaWVyX2V4YW1wbGUuaXB5bmI=">https://github.com/ccc013/DeepLearning_Notes/blob/master/Pytorch/practise/train_classifier_example.ipynb</span></p><h2 id="数据并行"><a class="anchor" href="#数据并行">#</a> <strong>数据并行</strong></h2><p>这部分教程将学习如何使用 <code>DataParallel</code> 来使用多个 GPUs 训练网络。</p><p>首先，在 GPU 上训练模型的做法很简单，如下代码所示，定义一个 <code>device</code> 对象，然后用 <code>.to()</code> 方法将网络模型参数放到指定的 GPU 上。</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda:0&quot;)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p></p><p>接着就是将所有的张量变量放到 GPU 上：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure><p></p><p>注意，这里 <code>my_tensor.to(device)</code> 是返回一个 <code>my_tensor</code> 的新的拷贝对象，而不是直接修改 <code>my_tensor</code> 变量，因此你需要将其赋值给一个新的张量，然后使用这个张量。</p><p>Pytorch 默认只会采用一个 GPU，因此需要使用多个 GPU，需要采用 <code>DataParallel</code> ，代码如下所示：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p></p><p>这代码也就是本节教程的关键，接下来会继续详细介绍。</p><h3 id="导入和参数"><a class="anchor" href="#导入和参数">#</a> <strong>导入和参数</strong></h3><p>首先导入必须的库以及定义一些参数：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"># Parameters and DataLoaders</span><br><span class="line">input_size = 5</span><br><span class="line">output_size = 2</span><br><span class="line"></span><br><span class="line">batch_size = 30</span><br><span class="line">data_size = 100</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure><p></p><p>这里主要定义网络输入大小和输出大小， <code>batch</code> 以及图片的大小，并定义了一个 <code>device</code> 对象。</p><h3 id="构建一个假数据集"><a class="anchor" href="#构建一个假数据集">#</a> <strong>构建一个假数据集</strong></h3><p>接着就是构建一个假的 (随机) 数据集。实现代码如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class RandomDataset(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, size, length):</span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        return self.data[index]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.len</span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),</span><br><span class="line">                         batch_size=batch_size, shuffle=True)</span><br></pre></td></tr></table></figure><p></p><h3 id="简单的模型"><a class="anchor" href="#简单的模型">#</a> <strong>简单的模型</strong></h3><p>接下来构建一个简单的网络模型，仅仅包含一层全连接层的神经网络，加入 <code>print()</code> 函数用于监控网络输入和输出 <code>tensors</code> 的大小：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    # Our model</span><br><span class="line"></span><br><span class="line">    def __init__(self, input_size, output_size):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(&quot;\tIn Model: input size&quot;, input.size(),</span><br><span class="line">              &quot;output size&quot;, output.size())</span><br><span class="line"></span><br><span class="line">        return output</span><br></pre></td></tr></table></figure><p></p><h3 id="创建模型和数据平行"><a class="anchor" href="#创建模型和数据平行">#</a> <strong>创建模型和数据平行</strong></h3><p>这是本节的核心部分。首先需要定义一个模型实例，并且检查是否拥有多个 GPUs，如果是就可以将模型包裹在 <code>nn.DataParallel</code> ，并调用 <code>model.to(device)</code> 。代码如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line">if torch.cuda.device_count() &gt; 1:</span><br><span class="line">  print(&quot;Let&#x27;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)</span><br><span class="line">  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span><br><span class="line">  model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p></p><h3 id="运行模型"><a class="anchor" href="#运行模型">#</a> <strong>运行模型</strong></h3><p>接着就可以运行模型，看看打印的信息：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for data in rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(&quot;Outside: input size&quot;, input.size(),</span><br><span class="line">          &quot;output_size&quot;, output.size())</span><br></pre></td></tr></table></figure><p></p><p>输出如下：</p><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p></p><h3 id="运行结果"><a class="anchor" href="#运行结果">#</a> <strong>运行结果</strong></h3><p>如果仅仅只有 1 个或者没有 GPU ，那么 <code>batch=30</code> 的时候，模型会得到输入输出的大小都是 30。但如果有多个 GPUs，那么结果如下：</p><h3 id="2-gpus"><a class="anchor" href="#2-gpus">#</a> <strong>2 GPUs</strong></h3><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># on 2 GPUs</span><br><span class="line">Let&#x27;s use 2 GPUs!</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p></p><h3 id="3-gpus"><a class="anchor" href="#3-gpus">#</a> <strong>3 GPUs</strong></h3><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Let&#x27;s use 3 GPUs!</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p></p><h3 id="8-gpus"><a class="anchor" href="#8-gpus">#</a> <strong>8 GPUs</strong></h3><p></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Let&#x27;s use 8 GPUs!</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span><br><span class="line">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span><br></pre></td></tr></table></figure><p></p><h3 id="总结"><a class="anchor" href="#总结">#</a> <strong>总结</strong></h3><p><code>DataParallel</code> 会自动分割数据集并发送任务给多个 GPUs 上的多个模型。然后等待每个模型都完成各自的工作后，它又会收集并融合结果，然后返回。</p><p>更详细的数据并行教程：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvZm9ybWVyX3RvcmNoaWVzL3BhcmFsbGVsaXNtX3R1dG9yaWFsLmh0bWw=">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</span></p><p>本小节教程：</p><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovZGF0YV9wYXJhbGxlbF90dXRvcmlhbC5odG1s">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</span></p><hr><h3 id="小结"><a class="anchor" href="#小结">#</a> <strong>小结</strong></h3><p>教程从最基础的张量开始介绍，然后介绍了非常重要的自动求梯度的 <code>autograd</code> ，接着介绍如何构建一个神经网络，如何训练图像分类器，最后简单介绍使用多 GPUs 加快训练速度的方法。</p><p>快速入门教程就介绍完了，接下来你可以选择：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHMvaW50ZXJtZWRpYXRlL3JlaW5mb3JjZW1lbnRfcV9sZWFybmluZy5odG1s">训练一个神经网络来玩视频游戏</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvaW1hZ2VuZXQ=">在 imagenet 上训练 ResNet</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvZGNnYW4=">采用 GAN 训练一个人脸生成器</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXMvdHJlZS9tYXN0ZXIvd29yZF9sYW5ndWFnZV9tb2RlbA==">采用循环 LSTM 网络训练一个词语级别的语言模型</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9naXRodWIuY29tL3B5dG9yY2gvZXhhbXBsZXM=">更多的例子</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9weXRvcmNoLm9yZy90dXRvcmlhbHM=">更多的教程</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnpoaWh1LmNvbS8/dGFyZ2V0PWh0dHBzJTNBLy9kaXNjdXNzLnB5dG9yY2gub3JnLw==">在 Forums 社区讨论 PyTorch</span></li></ul><h2 id="项目练习手写数字识别练习mnist"><a class="anchor" href="#项目练习手写数字识别练习mnist">#</a> 项目练习：手写数字识别练习 MNIST</h2><p><img data-src="C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210153152886.png" alt="image-20240210153152886"></p><p>softmax 归一化</p><p>梯度下降法等调参</p><p>一批次一批次的训练：一个批次一个 batch</p><p>神经网络过程是线性的，需要非线性结果</p><p>在每个节点上再套上一个非线性函数 f (), 又称激活函数</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>x</mi><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>f</mi><mrow><mo fence="true">(</mo><msubsup><mi>a</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>k</mi></msubsup><mo>⋅</mo><msubsup><mi>x</mi><mi>i</mi><mi>k</mi></msubsup><mo>+</mo><msubsup><mi>b</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>k</mi></msubsup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_j^{k+1} = \sum_i f\left(a_{i,j}^k \cdot x_i^k + b_{i,j}^k\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3022109999999998em;vertical-align:-.403103em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8991079999999999em"><span style="top:-2.4330050000000005em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.403103em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.327674em;vertical-align:-1.277669em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.899108em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.383108em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8991079999999998em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.899108em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.383108em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p><p><img data-src="C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240210153529250.png" alt="image-20240210153529250"></p><p>同时安装四个库</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy torch torchvision matplotlib</span><br></pre></td></tr></table></figure><p></p><p>手写数字识别练习</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">#定义一个神经网络</span><br><span class="line">class Net(torch.nn.Module):</span><br><span class="line">	</span><br><span class="line">	def __init__(self):</span><br><span class="line">		#神经网络主体，包含四个全连接层</span><br><span class="line">		super().__init__()</span><br><span class="line">		self.fc1 = torch.nn.Linear(28*28,64) #输入为28*28像素尺寸的图像</span><br><span class="line">		self.fc2 = torch.nn.Linear(64,64)</span><br><span class="line">		self.fc3 = torch.nn.Linear(64,64)</span><br><span class="line">		self.fc4 = torch.nn.Linear(64,10)</span><br><span class="line">	#中间3层都放了64个节点，输出为10个数字类别</span><br><span class="line">	</span><br><span class="line">	def forward(self,x): #x图像输入</span><br><span class="line">		x = torch.nn.functional.relu(self.fc1(x) )</span><br><span class="line">		x = torch.nn.functional.relu(self.fc2(x) )</span><br><span class="line">		x = torch.nn.functional.relu(self.fc3(x) )</span><br><span class="line">		x = torch.nn.functional.log_softmax(self.fc3(x),dim=1)#提高计算稳定性</span><br><span class="line">		return x</span><br><span class="line">	</span><br><span class="line">	#导入数据</span><br><span class="line">	def get_data_loader(is_train):</span><br><span class="line">		#定义数据转换类型</span><br><span class="line">		to_tensor = transforms.Compose([transforms,ToTensor()])</span><br><span class="line">		data_set = MNIST(&quot;&quot;,is_train,transform=to_tensor,download = True)</span><br><span class="line">		return DataLoader(data_set,batch_size=15,shuffle=True)</span><br><span class="line">	</span><br><span class="line">	def evaluate(test_data,net):</span><br><span class="line">		n_correct = 0</span><br><span class="line">		n_total = 0</span><br><span class="line">		with torch.no_grad(): 	#从测试集中按批次取出数据</span><br><span class="line">			for(x,y) in test_data:</span><br><span class="line">				outputs = net.forward(x,view(-1,28*28))</span><br><span class="line">				for i,output in enumerate(outputs) :</span><br><span class="line">					if torch.argmax(output) == y[i] :</span><br><span class="line">						n_correct += 1</span><br><span class="line">					n_total += 1</span><br><span class="line">		return n_correct / n_total #返回正确率</span><br><span class="line">	</span><br><span class="line">	def main():</span><br><span class="line">		</span><br><span class="line">		train_data = get_data_loader(is_train=True)</span><br><span class="line">		test_data = get_data_loader(is_train=False)</span><br><span class="line">		net = Net()</span><br><span class="line">		</span><br><span class="line">		print(&quot;initial accuracy:&quot;,evaluate(test_data,net))</span><br><span class="line">		#训练model Pytorch固定写法</span><br><span class="line">		optimizer = torch.optim.Adam(net.parameters(),lr=0.001)</span><br><span class="line">		for epoch in range(2):</span><br><span class="line">			for(x,y) in train_data:</span><br><span class="line">				net.zero_grad() #初始化</span><br><span class="line">				output = net.forward(x,view(-1,28*28 )) #正向传播</span><br><span class="line">				loss = torch.nn.functional.nll_loss(output,y) #计算差值</span><br><span class="line">				loss.backward() #反向误差传播</span><br><span class="line">				optimizer.step() #优化网络参数</span><br><span class="line">				#</span><br><span class="line">			print(&quot;epoch&quot;,epoch,&quot;accuracy:&quot;,evaluate(test_data,net))</span><br><span class="line">		#如果一切正常，训练率会越来越高</span><br><span class="line">		</span><br><span class="line">		#训练完成后，随机选取三张图像，显示网络预测结果</span><br><span class="line">        for (n, (x, _ )) in enumerate(test_data):</span><br><span class="line">        	if n &gt; 3:</span><br><span class="line">        		break</span><br><span class="line">        	predict = torch.argmax(net.forward(x[0].view(-1,28*28)) )</span><br><span class="line">        	plt.figure(n)</span><br><span class="line">        	plt.imshow(x[0].view(28,28))</span><br><span class="line">        	plt.title(&quot;prediction: &quot;+str(int(predict) ) )</span><br><span class="line">   		plt.show()</span><br><span class="line">	</span><br><span class="line">	if __name__ == &quot;__main__&quot;:</span><br><span class="line">		main()</span><br></pre></td></tr></table></figure><p></p><p>注释版</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        # 定义第一个卷积层，输入通道为1（单通道图像，如灰度图），输出通道为6，使用5*5的卷积核</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5)</span><br><span class="line">        # 第二个卷积层，输入通道为6（由于第一个卷积层的输出是6），输出通道为16，同样使用5*5的卷积核</span><br><span class="line">        # 注意：这里的定义缺失了，应该在`__init__`方法中添加self.conv2的定义</span><br><span class="line">        # 全连接层（fc1）的定义，输入特征维度为16*5*5（假设经过两次卷积和池化后的特征图大小），输出特征维度为120</span><br><span class="line">        self.fc1 = nn.Linear(16*5*5, 120)</span><br><span class="line">        # 第二个全连接层，输入特征维度为120，输出特征维度为84</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        # 第三个全连接层，输入特征维度为84，输出特征维度为10（假设为分类问题的类别数）</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # 应用第一个卷积层后使用ReLU激活函数，然后进行2x2的最大池化</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        # 应用第二个卷积层，同样使用ReLU激活函数和2x2的最大池化</span><br><span class="line">        # 注意：这里需要确保conv2在`__init__`方法中被定义</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</span><br><span class="line">        # 将多维输入张量展平成一维，准备输入到全连接层</span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line">        # 第一个全连接层后使用ReLU激活函数</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        # 第二个全连接层同样使用ReLU激活函数</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        # 最后一个全连接层输出最终结果，这里不使用激活函数是因为后续可能接softmax进行分类</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        # 计算除batch维度外的所有维度乘积，即在全连接层之前需要展平的特征数量</span><br><span class="line">        size = x.size()[1:]  # 所有维度除了batch维度</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p></p><p>这段代码定义了一个简单的卷积神经网络，包含两个卷积层和三个全连接层。它演示了在 PyTorch 中如何构建网络、应用卷积、激活函数、池化以及全连接层。注意，代码中确实漏掉了 <code>self.conv2</code> 的定义，这是必须添加的部分以确保网络能够正常工作。</p><p><code>forward</code> 前向传输<br>全连接线性运算 self.fc1 (x) 再套上激活函数 x 为图像输入</p><p>第一个参数表示下载目录，&quot;&quot; 空表示当前目录</p><p><code>is_train</code> 用于指定导入训练集还是测试集</p><p><code>batch_size=15</code> 表示一个批次包含 15 张图片</p><p><code>shuffle=True</code> 表示打乱顺序</p><p>返回数据加载器 <code>DataLoader</code></p><p><code>evaluate</code> 函数用来评估神经网络的识别正确率</p><p>从测试集中按批次取出数据，计算神经网路的预测值</p><p>再对批次中的每个结果进行比较，累加正确预测的数量</p><p>nll_loss 对数损失函数</p><p>是 log_softmax 中的对数运算</p><p>epoch 训练伦次，提高数据利用率</p><div class="tags"><a href="/tags/PyTorch%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> PyTorch学习</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-02-24 13:58:37" itemprop="dateModified" datetime="2024-02-24T13:58:37+08:00">2024-02-24</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Debra 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Debra 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Debra <i class="ic i-at"><em>@</em></i>金娇娇</li><li class="link"><strong>本文链接：</strong> <a href="https://jinjiaojiao.top/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" title="PyTorch学习第二章——深度学习入门">https://jinjiaojiao.top/2024/02/11/PyTorch学习第二章——深度学习入门/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipet4bz0yj20zk0m8e81.jpg" title="PyTorch学习第一章——简介与安装"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> PyTorch学习</span><h3>PyTorch学习第一章——简介与安装</h3></a></div><div class="item right"><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicljitigmj20zk0m87fp.jpg" title="学习d2l深度学习day1——介绍与数据处理"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 深度学习</span><h3>学习d2l深度学习day1——介绍与数据处理</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0pytorch%E4%B9%8B60min%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">第二章： PyTorch 之 60min 入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-pytorch"><span class="toc-number">1.1.</span> <span class="toc-text">什么是 PyTorch ?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text">开始学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensors-%E5%BC%A0%E9%87%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">Tensors (张量)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torchempty-%E5%A3%B0%E6%98%8E%E4%B8%80%E4%B8%AA%E6%9C%AA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E7%9F%A9%E9%98%B5"><span class="toc-number">1.2.1.0.1.</span> <span class="toc-text">torch.empty(): 声明一个未初始化的矩阵。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torchrand%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5"><span class="toc-number">1.2.1.0.2.</span> <span class="toc-text">torch.rand()：随机初始化一个矩阵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E8%83%BD%E5%90%A6%E8%BF%90%E8%A1%8C%E5%9C%A8gpu"><span class="toc-number">1.2.1.0.3.</span> <span class="toc-text">验证能否运行在 GPU</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torchzeros%E5%88%9B%E5%BB%BA%E6%95%B0%E5%80%BC%E7%9A%86%E4%B8%BA-0-%E7%9A%84%E7%9F%A9%E9%98%B5"><span class="toc-number">1.2.1.0.4.</span> <span class="toc-text">torch.zeros()：创建数值皆为 0 的矩阵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torchtensor%E7%9B%B4%E6%8E%A5%E4%BC%A0%E9%80%92-tensor-%E6%95%B0%E5%80%BC%E6%9D%A5%E5%88%9B%E5%BB%BA"><span class="toc-number">1.2.1.0.5.</span> <span class="toc-text">torch.tensor()：直接传递 tensor 数值来创建</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tensornew_onesnew_-%E6%96%B9%E6%B3%95%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E5%B0%BA%E5%AF%B8%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.2.1.0.6.</span> <span class="toc-text">tensor.new_ones()：new_() 方法需要输入尺寸大小</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torchrandn_likeold_tensor%E4%BF%9D%E7%95%99%E7%9B%B8%E5%90%8C%E7%9A%84%E5%B0%BA%E5%AF%B8%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.2.1.0.7.</span> <span class="toc-text">torch.randn_like(old_tensor)：保留相同的尺寸大小</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%AE%83%E7%9A%84%E7%BB%B4%E5%BA%A6%E4%BF%A1%E6%81%AF"><span class="toc-number">1.2.1.0.8.</span> <span class="toc-text">获取它的维度信息:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.2.</span> <span class="toc-text">操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">加法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-number">1.2.2.1.1.</span> <span class="toc-text">+ 运算符</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#add"><span class="toc-number">1.2.2.1.2.</span> <span class="toc-text">add</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#result%E6%8F%90%E4%BE%9B%E4%B8%80%E4%B8%AA%E8%BE%93%E5%87%BA"><span class="toc-number">1.2.2.1.3.</span> <span class="toc-text">result 提供一个输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#add_-%E7%9B%B4%E6%8E%A5%E4%BF%AE%E6%94%B9%E5%8F%98%E9%87%8F"><span class="toc-number">1.2.2.1.4.</span> <span class="toc-text">add_ 直接修改变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E-tensor-%E7%9A%84%E8%AE%BF%E9%97%AE"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">对于 Tensor 的访问</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.2.2.1.</span> <span class="toc-text">索引操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torchview%E5%AF%B9-tensor-%E7%9A%84%E5%B0%BA%E5%AF%B8%E4%BF%AE%E6%94%B9"><span class="toc-number">1.2.2.2.2.</span> <span class="toc-text">torch.view ()：对 Tensor 的尺寸修改</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#item"><span class="toc-number">1.2.2.2.3.</span> <span class="toc-text">.item()</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%92%8Cnumpy%E6%95%B0%E7%BB%84%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.2.3.</span> <span class="toc-text">和 Numpy 数组的转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor-%E8%BD%AC%E6%8D%A2%E4%B8%BA-numpy-%E6%95%B0%E7%BB%84"><span class="toc-number">1.2.4.</span> <span class="toc-text">Tensor 转换为 Numpy 数组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy-%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2%E4%B8%BA-tensor"><span class="toc-number">1.2.5.</span> <span class="toc-text">Numpy 数组转换为 Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cuda-%E5%BC%A0%E9%87%8F"><span class="toc-number">1.2.6.</span> <span class="toc-text">CUDA 张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#autograd"><span class="toc-number">1.2.7.</span> <span class="toc-text">autograd</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.2.8.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#function"><span class="toc-number">1.2.8.1.</span> <span class="toc-text">Function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.2.9.</span> <span class="toc-text">梯度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.1.</span> <span class="toc-text">定义网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.3.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E6%9D%83%E9%87%8D"><span class="toc-number">1.3.4.</span> <span class="toc-text">更新权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.3.5.</span> <span class="toc-text">训练分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">训练数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.3.6.</span> <span class="toc-text">训练图片分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96-cifar10"><span class="toc-number">1.3.6.1.</span> <span class="toc-text">加载和归一化 CIFAR10</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.6.2.</span> <span class="toc-text">构建一个卷积神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.3.6.3.</span> <span class="toc-text">定义损失函数和优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.6.4.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD"><span class="toc-number">1.3.6.5.</span> <span class="toc-text">测试模型性能</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.7.</span> <span class="toc-text">在 GPU 上训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-number">1.4.</span> <span class="toc-text">数据并行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.1.</span> <span class="toc-text">导入和参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%81%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.2.</span> <span class="toc-text">构建一个假数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">简单的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E5%B9%B3%E8%A1%8C"><span class="toc-number">1.4.4.</span> <span class="toc-text">创建模型和数据平行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.5.</span> <span class="toc-text">运行模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.4.6.</span> <span class="toc-text">运行结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-gpus"><span class="toc-number">1.4.7.</span> <span class="toc-text">2 GPUs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-gpus"><span class="toc-number">1.4.8.</span> <span class="toc-text">3 GPUs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-gpus"><span class="toc-number">1.4.9.</span> <span class="toc-text">8 GPUs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.10.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.4.11.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E7%BB%83%E4%B9%A0mnist"><span class="toc-number">1.5.</span> <span class="toc-text">项目练习：手写数字识别练习 MNIST</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/" rel="bookmark" title="PyTorch学习第一章——简介与安装">PyTorch学习第一章——简介与安装</a></li><li class="active"><a href="/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" rel="bookmark" title="PyTorch学习第二章——深度学习入门">PyTorch学习第二章——深度学习入门</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Debra" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Debra</p><div class="description" itemprop="description">金同学的个人博客</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">8</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">4</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">3</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0RlYnJhNTU5" title="https:&#x2F;&#x2F;github.com&#x2F;Debra559"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/" title="分类于 PyTorch学习">PyTorch学习</a></div><span><a href="/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/" title="PyTorch学习第一章——简介与安装">PyTorch学习第一章——简介与安装</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/AI%E5%A4%8D%E7%9B%98/" title="分类于 AI复盘">AI复盘</a></div><span><a href="/2024/02/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" title="复盘in2vec">复盘in2vec</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/PyTorch%E5%AD%A6%E4%B9%A0/" title="分类于 PyTorch学习">PyTorch学习</a></div><span><a href="/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" title="PyTorch学习第二章——深度学习入门">PyTorch学习第二章——深度学习入门</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/24/%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" title="分类于 产品经理">产品经理</a></div><span><a href="/2024/02/24/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9/" title="产品经理工作内容">产品经理工作内容</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 深度学习">深度学习</a></div><span><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="学习d2l深度学习day1——介绍与数据处理">学习d2l深度学习day1——介绍与数据处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" title="分类于 产品经理">产品经理</a></div><span><a href="/2024/02/24/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" title="需求分析">需求分析</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 深度学习">深度学习</a></div><span><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/" title="学习d2l深度学习day2——回归">学习d2l深度学习day2——回归</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Debra @ 金娇娇</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">56k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">51 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2024/02/11/PyTorch学习第二章——深度学习入门/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->