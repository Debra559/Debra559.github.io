<!-- build time:Wed Feb 14 2024 22:29:47 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="金娇娇" href="https://jinjiaojiao.top/rss.xml"><link rel="alternate" type="application/atom+xml" title="金娇娇" href="https://jinjiaojiao.top/atom.xml"><link rel="alternate" type="application/json" title="金娇娇" href="https://jinjiaojiao.top/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"><title>学习d2l深度学习day1——介绍与数据处理 | 金娇娇 = 去留无意，漫随天外云卷云舒</title><meta name="generator" content="Hexo 7.1.1"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">学习d2l深度学习day1——介绍与数据处理</h1><div class="meta"><span class="item" title="创建时间：2024-02-14 01:39:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-02-14T01:39:00+08:00">2024-02-14</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>6.2k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>6 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">金娇娇</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1giclhtuo6nj20zk0m8ttm.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gicli9lfebj20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gipeun65urj20zk0m81ii.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gipetlbztpj20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gicitf0kl1j20zk0m87fe.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1giclj9410cj20zk0m8h12.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Debra"><meta itemprop="description" content="去留无意，漫随天外云卷云舒, 金同学的个人博客"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="金娇娇"></span><div class="body md" itemprop="articleBody"><p><strong><em>深入理解深度学习的方法</em>：</strong></p><p>亲自实现，从 0 开始编写可实现运行的程序，一边看源码，一边思考</p><p><strong>课程信息：</strong></p><p><span class="exturl" data-url="aHR0cHM6Ly9jb3Vyc2VzLmQybC5haS96aC12Mi8=">课程安排 - 动手学深度学习课程 (d2l.ai)</span></p><p>认识一下 <code>Colab</code></p><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81Mjc2NjMxNjM=">Colab 使用教程（超级详细版）及 Colab Pro/Pro + 评测 - 知乎 (zhihu.com)</span></p><h1 id="深度学习应用"><a class="anchor" href="#深度学习应用">#</a> 深度学习应用</h1><h3 id="图像分类"><a class="anchor" href="#图像分类">#</a> 图像分类</h3><p><span class="exturl" data-url="aHR0cDovL3d3dy5pbWFnZS1uZXQub3JnLw==">http://www.image-net.org/</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9xei5jb20vMTAzNDk3Mi90aGUtZGF0YS10aGF0LWNoYW5nZWQtdGhlLWRpcmVjdGlvbi1vZi1haS1yZXNlYXJjaC1hbmQtcG9zc2libHktdGhlLXdvcmxk">ImageNet: the data that spawned the current AI boom (qz.com)</span></p><h3 id="物体检测和分割"><a class="anchor" href="#物体检测和分割">#</a> 物体检测和分割</h3><p><img data-src="https://pic.imgdb.cn/item/65cbabb49f345e8d036e0632.png" alt="image-20240213204425211"></p><p>分割指的是某个像素点属于哪个物体</p><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hdHRlcnBvcnQvTWFza19SQ05O">matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow (github.com)</span></p><h3 id="样式迁移"><a class="anchor" href="#样式迁移">#</a> 样式迁移</h3><p>风格变换</p><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3poYW5naGFuZzE5ODkvTVhOZXQtR2x1b24tU3R5bGUtVHJhbnNmZXIv">zhanghang1989/MXNet-Gluon-Style-Transfer: Neural Style and MSG-Net (github.com)</span></p><h3 id="人脸合成"><a class="anchor" href="#人脸合成">#</a> 人脸合成</h3><p><code>Karras et al, ICLR 2018</code></p><h3 id="文字生成图片"><a class="anchor" href="#文字生成图片">#</a> 文字生成图片</h3><p><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWkuY29tL2Jsb2cvZGFsbC1lLw==">https://openai.com/blog/dall-e/</span></p><h3 id="完整的故事"><a class="anchor" href="#完整的故事">#</a> 完整的故事</h3><p>领域专家（实现产品应用）</p><p>数据科学家（data-&gt;model）</p><p>AI 专家（提升模型精度和性能）</p><h2 id="安装"><a class="anchor" href="#安装">#</a> 安装</h2><h3 id="步骤"><a class="anchor" href="#步骤">#</a> 步骤</h3><h4 id="登录"><a class="anchor" href="#登录">#</a> 登录</h4><p>仅参考，李沐老师亚马逊平台 ubuntu 系统</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh ubuntu@100.20.65.33 </span><br></pre></td></tr></table></figure><p></p><h4 id="升级服务器系统"><a class="anchor" href="#升级服务器系统">#</a> 升级服务器系统</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure><p></p><h4 id="装一些gcc这类编译开发环境"><a class="anchor" href="#装一些gcc这类编译开发环境">#</a> 装一些 GCC 这类编译开发环境</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install build-essential</span><br></pre></td></tr></table></figure><p></p><h4 id="安装python"><a class="anchor" href="#安装python">#</a> 安装 python</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install python3.8</span><br></pre></td></tr></table></figure><p></p><h4 id="安装miniconda"><a class="anchor" href="#安装miniconda">#</a> 安装 miniconda</h4><p>打开官网</p><p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmFuYWNvbmRhLmNvbS9mcmVlL21pbmljb25kYS8=">Miniconda — Anaconda documentation</span></p><p>复制所需要的下载连接</p><p>这里 Linux 安装到服务器</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p></p><h4 id="启动"><a class="anchor" href="#启动">#</a> 启动</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p></p><p>一直回车到 yes</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash</span><br></pre></td></tr></table></figure><p></p><p>进入 conda 环境：刚开始最基础的 base 环境</p><h4 id="创建一个新环境"><a class="anchor" href="#创建一个新环境">#</a> 创建一个新环境</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n lm</span><br></pre></td></tr></table></figure><p></p><h4 id="激活"><a class="anchor" href="#激活">#</a> 激活</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate lm</span><br></pre></td></tr></table></figure><p></p><h4 id="安装记事本"><a class="anchor" href="#安装记事本">#</a> 安装记事本</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter d2l torch torchvision</span><br></pre></td></tr></table></figure><p></p><p>（国内慢提前安装个源）</p><p><img data-src="https://pic.imgdb.cn/item/65cbabe29f345e8d036e7eb7.png" alt="image-20240213211231841"></p><h4 id="复制链接"><a class="anchor" href="#复制链接">#</a> 复制链接</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://zh-v2.d2l.ai/d2l-zh.zip</span><br></pre></td></tr></table></figure><p></p><h4 id="安装zip"><a class="anchor" href="#安装zip">#</a> 安装 zip</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install zip</span><br></pre></td></tr></table></figure><p></p><h4 id="查看"><a class="anchor" href="#查看">#</a> 查看</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br></pre></td></tr></table></figure><p></p><h4 id="解压文件"><a class="anchor" href="#解压文件">#</a> 解压文件</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip d2l-zh.zip</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/d2l-ai/d2l-zh-pytorch-slides.git</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><p></p><p><strong>本地 prompt 操作！！</strong></p><p>需要把远程机器的端口映射运行在本地</p><p>m 神操作</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -L8888:localhost:8888 ubuntu@100.20.65.33</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -L8888:localhost:8888 thjin@yuanshen.moe</span><br></pre></td></tr></table></figure><p></p><p>再 vscode 点击 8888 进入</p><p><img data-src="C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240213213900135.png" alt="image-20240213213900135"></p><p>这个场景</p><p><img data-src="https://pic.imgdb.cn/item/65cbabfa9f345e8d036eba41.png" alt="image-20240213214121778"></p><p>继续 prompt</p><p><strong>别忘了启动对应的环境</strong></p><h4 id="下载插件"><a class="anchor" href="#下载插件">#</a> 下载插件</h4><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install rise</span><br></pre></td></tr></table></figure><p></p><p>下载之后 jupyter 就可以直接用了</p><h3 id="补充"><a class="anchor" href="#补充">#</a> 补充</h3><ul><li><p>删除单个文件（所有系统）：</p><p>Code</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1rm filename.ext</span><br></pre></td></tr></table></figure><p></p><p>或（Windows）</p><p>Code</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1del filename.ext</span><br></pre></td></tr></table></figure><p></p></li><li><p>删除空文件夹（所有系统）：</p><p>Code</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1rmdir foldername</span><br></pre></td></tr></table></figure><p></p><p>或（Windows）</p><p>Code</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1rmdir /S /Q foldername  # 使用/S/Q参数强制删除非空目录及其内容</span><br></pre></td></tr></table></figure><p></p></li><li><p>删除非空文件夹及其中的所有内容（所有系统）：</p><p>Code</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1rm -rf foldername</span><br></pre></td></tr></table></figure><p></p><p>注意：在使用 <code>rm -rf</code> 时要格外小心，因为它会立即、不可逆地删除指定的文件夹及其包含的所有内容。</p></li></ul><p><img data-src="https://pic.imgdb.cn/item/65cbac129f345e8d036ef549.png" alt="image-20240213202207539"></p><h3 id="数据操作"><a class="anchor" href="#数据操作">#</a> 数据操作</h3><p><span class="exturl" data-url="aHR0cHM6Ly9jb3Vyc2VzLmQybC5haS96aC12Mi8=">课程安排 - 动手学深度学习课程 (d2l.ai)</span></p><h5 id="导入"><a class="anchor" href="#导入">#</a> 导入</h5><p>首先，我们导入 <code>torch</code> 。请注意，虽然它被称为 PyTorch，但我们应该导入 <code>torch</code> 而不是 <code>pytorch</code></p><p>In [1]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure><p></p><h5 id="张量表示由一个数值组成的数组这个数组可能有多个维度"><a class="anchor" href="#张量表示由一个数值组成的数组这个数组可能有多个维度">#</a> <strong>张量表示由一个数值组成的数组，这个数组可能有多个维度</strong></h5><p>In [2]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(12)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p></p><p>Out[2]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br></pre></td></tr></table></figure><p></p><h5 id="可以通过张量的-shape-属性来访问张量的形状-和张量中元素的总数"><a class="anchor" href="#可以通过张量的-shape-属性来访问张量的形状-和张量中元素的总数">#</a> <strong>可以通过张量的 <code>shape</code> 属性来访问张量的<em>形状</em> 和张量中元素的总数</strong></h5><p>In [3]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p></p><p>Out[3]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([12])</span><br></pre></td></tr></table></figure><p></p><p>In [4]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.numel()  #numel张量大小</span><br></pre></td></tr></table></figure><p></p><p>Out[4]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12</span><br></pre></td></tr></table></figure><p></p><h5 id="要改变一个张量的形状而不改变元素数量和元素值可以调用-reshape-函数"><a class="anchor" href="#要改变一个张量的形状而不改变元素数量和元素值可以调用-reshape-函数">#</a> <strong>要改变一个张量的形状而不改变元素数量和元素值，可以调用 <code>reshape</code> 函数</strong></h5><p>In [5]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape(3, 4)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p></p><p>Out[5]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><p></p><h5 id="使用全0-全1-其他常量或者从特定分布中随机采样的数字"><a class="anchor" href="#使用全0-全1-其他常量或者从特定分布中随机采样的数字">#</a> <strong>使用全 0、全 1、其他常量或者从特定分布中随机采样的数字</strong></h5><p>In [6]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((2, 3, 4))</span><br></pre></td></tr></table></figure><p></p><p>Out[6]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]]])</span><br></pre></td></tr></table></figure><p></p><p>In [7]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones((2, 3, 4))</span><br></pre></td></tr></table></figure><p></p><p>Out[7]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]]])</span><br></pre></td></tr></table></figure><p></p><p>In [8]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(3, 4)</span><br></pre></td></tr></table></figure><p></p><p>Out[8]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2104,  1.4439, -1.3455, -0.8273],</span><br><span class="line">        [ 0.8009,  0.3585, -0.2690,  1.6183],</span><br><span class="line">        [-0.4611,  1.5744, -0.4882, -0.5317]])</span><br></pre></td></tr></table></figure><p></p><h5 id="通过提供包含数值的-python-列表或嵌套列表来为所需张量中的每个元素赋予确定值"><a class="anchor" href="#通过提供包含数值的-python-列表或嵌套列表来为所需张量中的每个元素赋予确定值">#</a> <strong>通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值</strong></h5><p>In [9]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class="line">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]).shape</span><br></pre></td></tr></table></figure><p></p><p>Out[9]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[2, 1, 4, 3],</span><br><span class="line">        [1, 2, 3, 4],</span><br><span class="line">        [4, 3, 2, 1]])</span><br><span class="line">torch.Size([1,3,4])</span><br></pre></td></tr></table></figure><p></p><p><strong>常见的标准算术运算符（ <code>+</code> 、 <code>-</code> 、 <code>*</code> 、 <code>/</code> 和 <code>**</code> ）都可以被升级为按元素运算</strong></p><p>In [10]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([1.0, 2, 4, 8])</span><br><span class="line">y = torch.tensor([2, 2, 2, 2])</span><br><span class="line">x + y, x - y, x * y, x / y, x**y</span><br></pre></td></tr></table></figure><p></p><p>Out[10]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 3.,  4.,  6., 10.]),</span><br><span class="line"> tensor([-1.,  0.,  2.,  6.]),</span><br><span class="line"> tensor([ 2.,  4.,  8., 16.]),</span><br><span class="line"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span><br><span class="line"> tensor([ 1.,  4., 16., 64.]))</span><br></pre></td></tr></table></figure><p></p><h5 id="按按元素方式应用更多的计算"><a class="anchor" href="#按按元素方式应用更多的计算">#</a> 按按元素方式应用更多的计算</h5><p>In [11]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x) #每个数的指数</span><br></pre></td></tr></table></figure><p></p><p>Out[11]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span><br></pre></td></tr></table></figure><p></p><h5 id="我们也可以把多个张量-连结concatenate-在一起"><a class="anchor" href="#我们也可以把多个张量-连结concatenate-在一起">#</a> 我们也可以把多个张量 <em>连结</em>（concatenate） 在一起</h5><p>In [12]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(12, dtype=torch.float32).reshape((3, 4))</span><br><span class="line">Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</span><br><span class="line">torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)</span><br><span class="line"># dim=0 按行拼接  dim=1 按列拼接</span><br></pre></td></tr></table></figure><p></p><p>Out[12]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [ 2.,  1.,  4.,  3.],</span><br><span class="line">         [ 1.,  2.,  3.,  4.],</span><br><span class="line">         [ 4.,  3.,  2.,  1.]]),</span><br><span class="line"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="line">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span><br></pre></td></tr></table></figure><p></p><h5 id="数据预处理通过-逻辑运算符-构建二元张量"><a class="anchor" href="#数据预处理通过-逻辑运算符-构建二元张量">#</a> 数据预处理通过 <em>逻辑运算符</em> 构建二元张量</h5><p>In [13]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure><p></p><p>Out[13]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[False,  True, False,  True],</span><br><span class="line">        [False, False, False, False],</span><br><span class="line">        [False, False, False, False]])</span><br></pre></td></tr></table></figure><p></p><h5 id="求和"><a class="anchor" href="#求和">#</a> 求和</h5><p>对张量中的所有元素进行求和会产生一个只有一个元素的张量</p><p>In [14]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.sum()</span><br></pre></td></tr></table></figure><p></p><p>Out[14]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(66.)</span><br></pre></td></tr></table></figure><p></p><h5 id="广播体制"><a class="anchor" href="#广播体制">#</a> 广播体制</h5><p>即使形状不同，我们仍然可以通过调用 <em>广播机制</em> （broadcasting mechanism） 来执行按元素操作</p><p>In [15]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(3).reshape((3, 1)) #3行1列</span><br><span class="line">b = torch.arange(2).reshape((1, 2))</span><br><span class="line">a, b</span><br></pre></td></tr></table></figure><p></p><p>Out[15]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0],</span><br><span class="line">         [1],</span><br><span class="line">         [2]]),</span><br><span class="line"> tensor([[0, 1]]))</span><br></pre></td></tr></table></figure><p></p><p>In [16]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b  #广播机制 需要均有1维</span><br></pre></td></tr></table></figure><p></p><p>Out[16]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1],</span><br><span class="line">        [1, 2],</span><br><span class="line">        [2, 3]])</span><br></pre></td></tr></table></figure><p></p><h5 id="可以用-1-选择最后一个元素可以用-13-选择第二个和第三个元素"><a class="anchor" href="#可以用-1-选择最后一个元素可以用-13-选择第二个和第三个元素">#</a> 可以用 <code>[-1]</code> 选择最后一个元素，可以用 <code>[1:3]</code> 选择第二个和第三个元素</h5><p>In [17]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[-1] #最后一行</span><br><span class="line">, X[1:3]</span><br></pre></td></tr></table></figure><p></p><p>Out[17]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 8.,  9., 10., 11.]),</span><br><span class="line"> tensor([[ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.]]))</span><br></pre></td></tr></table></figure><p></p><h5 id="除读取外我们还可以通过指定索引来将元素写入矩阵"><a class="anchor" href="#除读取外我们还可以通过指定索引来将元素写入矩阵">#</a> 除读取外，我们还可以通过指定索引来将元素写入矩阵</h5><p>In [18]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X[1, 2] = 9</span><br><span class="line">#相当于 x[0:1,0:2]</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p></p><p>Out[18]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  9.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure><p></p><h5 id="为多个元素赋值相同的值我们只需要索引所有元素然后为它们赋值"><a class="anchor" href="#为多个元素赋值相同的值我们只需要索引所有元素然后为它们赋值">#</a> 为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</h5><p>In [19]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[0:2, :] = 12</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p></p><p>Out[19]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[12., 12., 12., 12.],</span><br><span class="line">        [12., 12., 12., 12.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure><p></p><h5 id="内存问题"><a class="anchor" href="#内存问题">#</a> 内存问题</h5><p>运行一些操作可能会导致为新结果分配内存</p><p>python 引用语义</p><p>id 相当于 c 中的指针</p><p>In [20]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(Y)</span><br><span class="line">Y = Y + X #产生了一个新的Y</span><br><span class="line">id(Y) == before</span><br></pre></td></tr></table></figure><p></p><p>Out[20]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p></p><h5 id="执行原地操作"><a class="anchor" href="#执行原地操作">#</a> 执行原地操作</h5><p>In [21]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line">print(&#x27;id(Z):&#x27;, id(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line">print(&#x27;id(Z):&#x27;, id(Z))</span><br><span class="line">id(Z): 140452400950336</span><br><span class="line">id(Z): 140452400950336</span><br></pre></td></tr></table></figure><p></p><h5 id="如果在后续计算中没有重复使用-x我们也可以使用-x-x-y-或-x-y-来减少操作的内存开销"><a class="anchor" href="#如果在后续计算中没有重复使用-x我们也可以使用-x-x-y-或-x-y-来减少操作的内存开销">#</a> 如果在后续计算中没有重复使用 <code>X</code> ，我们也可以使用 <code>X[:] = X + Y</code> 或 <code>X += Y</code> 来减少操作的内存开销</h5><p>In [22]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure><p></p><p>Out[22]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><p></p><h5 id="转换为-numpy-张量"><a class="anchor" href="#转换为-numpy-张量">#</a> 转换为 <code>NumPy</code> 张量</h5><p>In [23]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line">type(A), type(B)</span><br></pre></td></tr></table></figure><p></p><p>Out[23]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure><p></p><p>numpy 外部库，需要 import</p><h5 id="将大小为1的张量转换为-python-标量"><a class="anchor" href="#将大小为1的张量转换为-python-标量">#</a> 将大小为 1 的张量转换为 Python 标量</h5><p>In [24]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([3.5])</span><br><span class="line">a, a.item(), float(a), int(a)</span><br></pre></td></tr></table></figure><p></p><p>Out[24]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3.5000]), 3.5, 3.5, 3)</span><br></pre></td></tr></table></figure><p></p><h2 id="数据预处理"><a class="anchor" href="#数据预处理">#</a> 数据预处理</h2><h5 id="创建一个人工数据集并存储在csv逗号分隔值文件"><a class="anchor" href="#创建一个人工数据集并存储在csv逗号分隔值文件">#</a> 创建一个人工数据集，并存储在 csv（逗号分隔值）文件</h5><p>In [1]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)</span><br><span class="line">data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)</span><br><span class="line">with open(data_file, &#x27;w&#x27;) as f:</span><br><span class="line">    f.write(&#x27;NumRooms,Alley,Price\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,Pave,127500\n&#x27;)</span><br><span class="line">    f.write(&#x27;2,NA,106000\n&#x27;)</span><br><span class="line">    f.write(&#x27;4,NA,178100\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,NA,140000\n&#x27;)</span><br></pre></td></tr></table></figure><p></p><h5 id="从创建的csv文件中加载原始数据集"><a class="anchor" href="#从创建的csv文件中加载原始数据集">#</a> 从创建的 csv 文件中加载原始数据集</h5><p>In [2]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">print(data)</span><br><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br></pre></td></tr></table></figure><p></p><p>直接输出 data 会好看一些</p><p>一般 csv 与 pandas 一起</p><h5 id="为了处理缺失的数据典型的方法包括插值和删除-这里我们将考虑插值"><a class="anchor" href="#为了处理缺失的数据典型的方法包括插值和删除-这里我们将考虑插值">#</a> 为了处理缺失的数据，典型的方法包括<em>插值</em>和<em>删除</em>， 这里，我们将考虑插值</h5><p>In [3]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line">print(inputs)</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       3.0  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       3.0   NaN</span><br></pre></td></tr></table></figure><p></p><h5 id="对于inputs中的类别值或离散值我们将nan视为一个类别"><a class="anchor" href="#对于inputs中的类别值或离散值我们将nan视为一个类别">#</a> 对于 <code>inputs</code> 中的类别值或离散值，我们将 “NaN” 视为一个类别</h5><p>In [4]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">0       3.0           1          0</span><br><span class="line">1       2.0           0          1</span><br><span class="line">2       4.0           0          1</span><br><span class="line">3       3.0           0          1</span><br></pre></td></tr></table></figure><p></p><p>现在 <code>inputs</code> 和 <code>outputs</code> 中的所有条目都是数值类型，它们可以转换为张量格式</p><p>In [5]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><p></p><p>Out[5]:</p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[3., 1., 0.],</span><br><span class="line">         [2., 0., 1.],</span><br><span class="line">         [4., 0., 1.],</span><br><span class="line">         [3., 0., 1.]], dtype=torch.float64),</span><br><span class="line"> tensor([127500, 106000, 178100, 140000]))</span><br></pre></td></tr></table></figure><p></p><p><img data-src="C:%5CUsers%5C29758%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240214012040421.png" alt="image-20240214012040421"></p><p>tensor 是数学上的一个概念 array 是计算机一个概念</p><h2 id="下载电子书"><a class="anchor" href="#下载电子书">#</a> 下载电子书</h2><p><img data-src="https://pic.imgdb.cn/item/65cbac289f345e8d036f2f6e.png" alt="image-20240208194727799"></p><h3 id="zlibrary"><a class="anchor" href="#zlibrary">#</a> zlibrary</h3><p>由于 free 经常被免费下架</p><p><strong>最新版网址获取方式：</strong></p><p></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blackbox@zlib.se</span><br></pre></td></tr></table></figure><p></p><p>向这个邮箱发任意信息，等几分钟就会回复最新版的网址</p><p><img data-src="https://pic.imgdb.cn/item/65cbac4c9f345e8d036f8948.png" alt="image-20240208200237222"></p><p><img data-src="https://pic.imgdb.cn/item/65cbac679f345e8d036fcbdc.png" alt="image-20240208200405438"></p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-02-14 01:56:33" itemprop="dateModified" datetime="2024-02-14T01:56:33+08:00">2024-02-14</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Debra 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Debra 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Debra <i class="ic i-at"><em>@</em></i>金娇娇</li><li class="link"><strong>本文链接：</strong> <a href="https://jinjiaojiao.top/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="学习d2l深度学习day1——介绍与数据处理">https://jinjiaojiao.top/2024/02/14/学习d2l深度学习day1——介绍与数据处理/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva2.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclh5u05ej20zk0m87df.jpg" title="PyTorch学习第二章——深度学习入门"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i></span><h3>PyTorch学习第二章——深度学习入门</h3></a></div><div class="item right"><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva2.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclffsa1cj20zk0m811l.jpg" title="学习d2l深度学习day2——回归"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i></span><h3>学习d2l深度学习day2——回归</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">深度学习应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.0.1.</span> <span class="toc-text">图像分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2"><span class="toc-number">1.0.2.</span> <span class="toc-text">物体检测和分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB"><span class="toc-number">1.0.3.</span> <span class="toc-text">样式迁移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E8%84%B8%E5%90%88%E6%88%90"><span class="toc-number">1.0.4.</span> <span class="toc-text">人脸合成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87"><span class="toc-number">1.0.5.</span> <span class="toc-text">文字生成图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%85%E4%BA%8B"><span class="toc-number">1.0.6.</span> <span class="toc-text">完整的故事</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.</span> <span class="toc-text">安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.1.1.</span> <span class="toc-text">步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%99%BB%E5%BD%95"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">登录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%87%E7%BA%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">升级服务器系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A3%85%E4%B8%80%E4%BA%9Bgcc%E8%BF%99%E7%B1%BB%E7%BC%96%E8%AF%91%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">装一些 GCC 这类编译开发环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85python"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">安装 python</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85miniconda"><span class="toc-number">1.1.1.5.</span> <span class="toc-text">安装 miniconda</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">1.1.1.6.</span> <span class="toc-text">启动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%96%B0%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.1.7.</span> <span class="toc-text">创建一个新环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB"><span class="toc-number">1.1.1.8.</span> <span class="toc-text">激活</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E8%AE%B0%E4%BA%8B%E6%9C%AC"><span class="toc-number">1.1.1.9.</span> <span class="toc-text">安装记事本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E9%93%BE%E6%8E%A5"><span class="toc-number">1.1.1.10.</span> <span class="toc-text">复制链接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85zip"><span class="toc-number">1.1.1.11.</span> <span class="toc-text">安装 zip</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B"><span class="toc-number">1.1.1.12.</span> <span class="toc-text">查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6"><span class="toc-number">1.1.1.13.</span> <span class="toc-text">解压文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%8F%92%E4%BB%B6"><span class="toc-number">1.1.1.14.</span> <span class="toc-text">下载插件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">1.1.2.</span> <span class="toc-text">补充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.3.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5"><span class="toc-number">1.1.3.0.1.</span> <span class="toc-text">导入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E7%94%B1%E4%B8%80%E4%B8%AA%E6%95%B0%E5%80%BC%E7%BB%84%E6%88%90%E7%9A%84%E6%95%B0%E7%BB%84%E8%BF%99%E4%B8%AA%E6%95%B0%E7%BB%84%E5%8F%AF%E8%83%BD%E6%9C%89%E5%A4%9A%E4%B8%AA%E7%BB%B4%E5%BA%A6"><span class="toc-number">1.1.3.0.2.</span> <span class="toc-text">张量表示由一个数值组成的数组，这个数组可能有多个维度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E5%BC%A0%E9%87%8F%E7%9A%84-shape-%E5%B1%9E%E6%80%A7%E6%9D%A5%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6-%E5%92%8C%E5%BC%A0%E9%87%8F%E4%B8%AD%E5%85%83%E7%B4%A0%E7%9A%84%E6%80%BB%E6%95%B0"><span class="toc-number">1.1.3.0.3.</span> <span class="toc-text">可以通过张量的 shape 属性来访问张量的形状 和张量中元素的总数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A6%81%E6%94%B9%E5%8F%98%E4%B8%80%E4%B8%AA%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6%E8%80%8C%E4%B8%8D%E6%94%B9%E5%8F%98%E5%85%83%E7%B4%A0%E6%95%B0%E9%87%8F%E5%92%8C%E5%85%83%E7%B4%A0%E5%80%BC%E5%8F%AF%E4%BB%A5%E8%B0%83%E7%94%A8-reshape-%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.3.0.4.</span> <span class="toc-text">要改变一个张量的形状而不改变元素数量和元素值，可以调用 reshape 函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%85%A80-%E5%85%A81-%E5%85%B6%E4%BB%96%E5%B8%B8%E9%87%8F%E6%88%96%E8%80%85%E4%BB%8E%E7%89%B9%E5%AE%9A%E5%88%86%E5%B8%83%E4%B8%AD%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E7%9A%84%E6%95%B0%E5%AD%97"><span class="toc-number">1.1.3.0.5.</span> <span class="toc-text">使用全 0、全 1、其他常量或者从特定分布中随机采样的数字</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%8F%90%E4%BE%9B%E5%8C%85%E5%90%AB%E6%95%B0%E5%80%BC%E7%9A%84-python-%E5%88%97%E8%A1%A8%E6%88%96%E5%B5%8C%E5%A5%97%E5%88%97%E8%A1%A8%E6%9D%A5%E4%B8%BA%E6%89%80%E9%9C%80%E5%BC%A0%E9%87%8F%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%85%83%E7%B4%A0%E8%B5%8B%E4%BA%88%E7%A1%AE%E5%AE%9A%E5%80%BC"><span class="toc-number">1.1.3.0.6.</span> <span class="toc-text">通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8C%89%E6%8C%89%E5%85%83%E7%B4%A0%E6%96%B9%E5%BC%8F%E5%BA%94%E7%94%A8%E6%9B%B4%E5%A4%9A%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">1.1.3.0.7.</span> <span class="toc-text">按按元素方式应用更多的计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%8A%8A%E5%A4%9A%E4%B8%AA%E5%BC%A0%E9%87%8F-%E8%BF%9E%E7%BB%93concatenate-%E5%9C%A8%E4%B8%80%E8%B5%B7"><span class="toc-number">1.1.3.0.8.</span> <span class="toc-text">我们也可以把多个张量 连结（concatenate） 在一起</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%80%9A%E8%BF%87-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6-%E6%9E%84%E5%BB%BA%E4%BA%8C%E5%85%83%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.3.0.9.</span> <span class="toc-text">数据预处理通过 逻辑运算符 构建二元张量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%82%E5%92%8C"><span class="toc-number">1.1.3.0.10.</span> <span class="toc-text">求和</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E4%BD%93%E5%88%B6"><span class="toc-number">1.1.3.0.11.</span> <span class="toc-text">广播体制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E7%94%A8-1-%E9%80%89%E6%8B%A9%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E5%8F%AF%E4%BB%A5%E7%94%A8-13-%E9%80%89%E6%8B%A9%E7%AC%AC%E4%BA%8C%E4%B8%AA%E5%92%8C%E7%AC%AC%E4%B8%89%E4%B8%AA%E5%85%83%E7%B4%A0"><span class="toc-number">1.1.3.0.12.</span> <span class="toc-text">可以用 [-1] 选择最后一个元素，可以用 [1:3] 选择第二个和第三个元素</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%99%A4%E8%AF%BB%E5%8F%96%E5%A4%96%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E6%8C%87%E5%AE%9A%E7%B4%A2%E5%BC%95%E6%9D%A5%E5%B0%86%E5%85%83%E7%B4%A0%E5%86%99%E5%85%A5%E7%9F%A9%E9%98%B5"><span class="toc-number">1.1.3.0.13.</span> <span class="toc-text">除读取外，我们还可以通过指定索引来将元素写入矩阵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E5%A4%9A%E4%B8%AA%E5%85%83%E7%B4%A0%E8%B5%8B%E5%80%BC%E7%9B%B8%E5%90%8C%E7%9A%84%E5%80%BC%E6%88%91%E4%BB%AC%E5%8F%AA%E9%9C%80%E8%A6%81%E7%B4%A2%E5%BC%95%E6%89%80%E6%9C%89%E5%85%83%E7%B4%A0%E7%84%B6%E5%90%8E%E4%B8%BA%E5%AE%83%E4%BB%AC%E8%B5%8B%E5%80%BC"><span class="toc-number">1.1.3.0.14.</span> <span class="toc-text">为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.3.0.15.</span> <span class="toc-text">内存问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.3.0.16.</span> <span class="toc-text">执行原地操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E5%9C%A8%E5%90%8E%E7%BB%AD%E8%AE%A1%E7%AE%97%E4%B8%AD%E6%B2%A1%E6%9C%89%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8-x%E6%88%91%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8-x-x-y-%E6%88%96-x-y-%E6%9D%A5%E5%87%8F%E5%B0%91%E6%93%8D%E4%BD%9C%E7%9A%84%E5%86%85%E5%AD%98%E5%BC%80%E9%94%80"><span class="toc-number">1.1.3.0.17.</span> <span class="toc-text">如果在后续计算中没有重复使用 X ，我们也可以使用 X[:] &#x3D; X + Y 或 X +&#x3D; Y 来减少操作的内存开销</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E4%B8%BA-numpy-%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.3.0.18.</span> <span class="toc-text">转换为 NumPy 张量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%86%E5%A4%A7%E5%B0%8F%E4%B8%BA1%E7%9A%84%E5%BC%A0%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA-python-%E6%A0%87%E9%87%8F"><span class="toc-number">1.1.3.0.19.</span> <span class="toc-text">将大小为 1 的张量转换为 Python 标量</span></a></li></ol></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E5%AD%98%E5%82%A8%E5%9C%A8csv%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.0.0.1.</span> <span class="toc-text">创建一个人工数据集，并存储在 csv（逗号分隔值）文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E5%88%9B%E5%BB%BA%E7%9A%84csv%E6%96%87%E4%BB%B6%E4%B8%AD%E5%8A%A0%E8%BD%BD%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.0.0.2.</span> <span class="toc-text">从创建的 csv 文件中加载原始数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BA%86%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%B8%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%E5%8C%85%E6%8B%AC%E6%8F%92%E5%80%BC%E5%92%8C%E5%88%A0%E9%99%A4-%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E5%B0%86%E8%80%83%E8%99%91%E6%8F%92%E5%80%BC"><span class="toc-number">1.2.0.0.3.</span> <span class="toc-text">为了处理缺失的数据，典型的方法包括插值和删除， 这里，我们将考虑插值</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8Einputs%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%88%AB%E5%80%BC%E6%88%96%E7%A6%BB%E6%95%A3%E5%80%BC%E6%88%91%E4%BB%AC%E5%B0%86nan%E8%A7%86%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%B1%BB%E5%88%AB"><span class="toc-number">1.2.0.0.4.</span> <span class="toc-text">对于 inputs 中的类别值或离散值，我们将 “NaN” 视为一个类别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E7%94%B5%E5%AD%90%E4%B9%A6"><span class="toc-number">1.3.</span> <span class="toc-text">下载电子书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#zlibrary"><span class="toc-number">1.3.1.</span> <span class="toc-text">zlibrary</span></a></li></ol></li></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Debra" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Debra</p><div class="description" itemprop="description">金同学的个人博客</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">6</span> <span class="name">文章</span></a></div><div class="item tags"><a href="/tags/"><span class="count">1</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0RlYnJhNTU5" title="https:&#x2F;&#x2F;github.com&#x2F;Debra559"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/07/CS/course-1/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" title="复盘in2vec">复盘in2vec</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day1%E2%80%94%E2%80%94%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="学习d2l深度学习day1——介绍与数据处理">学习d2l深度学习day1——介绍与数据处理</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/14/%E5%AD%A6%E4%B9%A0d2l%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0day2%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/" title="学习d2l深度学习day2——回归">学习d2l深度学习day2——回归</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/11/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" title="PyTorch学习第二章——深度学习入门">PyTorch学习第二章——深度学习入门</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" title="复盘in2vec">复盘in2vec</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/02/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/" title="PyTorch学习第一章——简介与安装">PyTorch学习第一章——简介与安装</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Debra @ 金娇娇</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">55k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">50 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2024/02/14/学习d2l深度学习day1——介绍与数据处理/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->